{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import all_function as af\n",
    "#pd.read_excel('database/Biohub authors.xlsx')\n",
    "\n",
    "import re\n",
    "import os\n",
    "import unicodedata\n",
    "# from datetime import date,time\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import datacompy\n",
    "from os.path import exists as file_exists\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q: how to drop duplicates then combine 'biohub author column?'  df3&df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "m=list(df.columns)\n",
    "m.remove('biohub author')\n",
    "\n",
    "\n",
    "df=df.groupby(m)['biohub author'].apply('; '.join).reset_index()      \n",
    "#df2.groupby('pmid', as_index=False).agg(sum)\n",
    "#df=df.drop_duplicates(subset='pmid', keep=\"last\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "start=(datetime.date.today() - datetime.timedelta(days=5)).strftime('%Y-%m-%d')\n",
    "end=(datetime.date.today() -\n",
    "    datetime.timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "Keyword='biohub'\n",
    "df5=af.Bibliometrics_Collect(start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 用来重置base dataset的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BioMedrxiv: Fetching search results 1 to 10...\n",
      "BioMedrxiv: Fetching search results 11 to 20...\n",
      "BioMedrxiv: Fetching search results 21 to 30...\n",
      "BioMedrxiv: Fetching search results 31 to 40...\n",
      "BioMedrxiv: Fetching search results 41 to 50...\n",
      "BioMedrxiv: Fetching search results 51 to 60...\n",
      "BioMedrxiv: Fetching search results 61 to 70...\n",
      "BioMedrxiv: Fetching search results 71 to 80...\n",
      "BioMedrxiv: Fetching search results 81 to 90...\n",
      "BioMedrxiv: Fetching search results 91 to 100...\n",
      "BioMedrxiv: Fetching search results 101 to 110...\n",
      "BioMedrxiv: Fetching search results 111 to 120...\n",
      "BioMedrxiv: Fetching search results 121 to 130...\n",
      "BioMedrxiv: Fetching search results 131 to 140...\n",
      "BioMedrxiv: Fetching search results 141 to 150...\n",
      "BioMedrxiv: Fetching search results 151 to 160...\n",
      "BioMedrxiv: Fetching search results 161 to 170...\n",
      "BioMedrxiv: Fetching search results 171 to 180...\n",
      "BioMedrxiv: Fetching search results 181 to 190...\n",
      "BioMedrxiv: Fetched 188 records in 628.9 seconds.\n",
      "Arxiv: Fetching search results 1 to 9...\n",
      "Arxiv: Fetching search results 11 to 19...\n",
      "Arxiv: Fetched 19 records in 18.6 seconds.\n",
      "Pubmed: Fetched 232 records in 147.8 seconds.\n"
     ]
    }
   ],
   "source": [
    "# start='2022-09-01'\n",
    "# end='2022-09-10'\n",
    "# af.Pubmed_search2(start,end)\n",
    "\n",
    "# start_date='2022-11-1'\n",
    "# end_date='2022-11-18'\n",
    "\n",
    "\n",
    "import importlib\n",
    "import all_function as af #import the module here, so that it can be reloaded.\n",
    "importlib.reload(af)\n",
    "\n",
    "start_date='2022-6-1'\n",
    "#start_date='2022-5-21'\n",
    "end_date='2022-11-26'\n",
    "\n",
    "start=start_date\n",
    "end=end_date\n",
    "keyword='biohub'\n",
    "Keyword='biohub'\n",
    "\n",
    "# base=af.Bibliometrics_Collect(start,end)\n",
    "# base.fillna('', inplace=True)\n",
    "# base.to_csv('database/basedb.csv', encoding='utf-8-sig',index=False)\n",
    "\n",
    "#d1=af.BioMedrxiv_Search2(start,end,keyword)\n",
    "# d1\n",
    "\n",
    "\n",
    "df1 = af.BioMedrxiv_Search2(start_date=start, end_date=end, keyword=Keyword)\n",
    "df2 = af.Arxiv_Search(start_date=start, keyword=Keyword)\n",
    "df3 = af.Pubmed_search2(start_date=start, end_date=end,TERM='(zuckerb* AND biohub) OR \"cz biohub\" OR \"czi biohub\"',save_AuthorInfo=True)\n",
    "df4 = af.Pubmed_search_author(start_date=end,end_date=end)\n",
    "df = pd.concat([df1, df2, df3,df4])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('database/basedb_ori.csv', encoding='utf-8-sig')\n",
    "\n",
    "df1 = af.BioMedrxiv_Search2(start_date=start, end_date=end, keyword=Keyword)\n",
    "df2 = af.Arxiv_Search(start_date=start, keyword=Keyword)\n",
    "df3 = af.Pubmed_search2(start_date=start, end_date=end,TERM='(zuckerb* AND biohub) OR \"cz biohub\" OR \"czi biohub\"',save_AuthorInfo=True)\n",
    "df4 = af.Pubmed_search_author(start_date=end,end_date=end)\n",
    "df = pd.concat([df1, df2, df3,df4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2022-11-26'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>journal</th>\n",
       "      <th>version</th>\n",
       "      <th>doi</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>authors2</th>\n",
       "      <th>corresponding author</th>\n",
       "      <th>corresponding author institution</th>\n",
       "      <th>version number</th>\n",
       "      <th>type</th>\n",
       "      <th>...</th>\n",
       "      <th>biohub author</th>\n",
       "      <th>pdf url</th>\n",
       "      <th>publish date</th>\n",
       "      <th>pmid</th>\n",
       "      <th>keyword</th>\n",
       "      <th>COIS</th>\n",
       "      <th>confirm preprint doi</th>\n",
       "      <th>record change number</th>\n",
       "      <th>epost date2</th>\n",
       "      <th>publish date2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>arxiv</td>\n",
       "      <td>arXiv:2206.01271v2</td>\n",
       "      <td>10.48550/arXiv.2206.01271</td>\n",
       "      <td>https://arxiv.org/abs/2206.01271</td>\n",
       "      <td>Wildebeest Herds on Rolling Hills: Flocking on...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://arxiv.org/pdf/2206.01271</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-06-02</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Diagnostics (Basel, Switzerland)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.3390/diagnostics12112805</td>\n",
       "      <td>https://pubmed.ncbi.nlm.nih.gov/36428863</td>\n",
       "      <td>Efficient Tracing of the SARS-CoV-2 Omicron Va...</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022/11/27</td>\n",
       "      <td>36428863</td>\n",
       "      <td>NGS; SARS-CoV-2; epidemiology; molecular assay...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>2022-11-15</td>\n",
       "      <td>2022-11-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pathogens (Basel, Switzerland)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.3390/pathogens11111345</td>\n",
       "      <td>https://pubmed.ncbi.nlm.nih.gov/36422597</td>\n",
       "      <td>Luna Virus and Helminths in Wild</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Manu Vanaerschot; Cristina Tato</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022/11/25</td>\n",
       "      <td>36422597</td>\n",
       "      <td>Luna virus; Mastomys natalensis; Zambia; foetu...</td>\n",
       "      <td>The authors declare no conflict of interest.</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>2022-11-14</td>\n",
       "      <td>2022-11-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Seminars in immunopathology</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.1007/s00281-022-00972-2</td>\n",
       "      <td>https://pubmed.ncbi.nlm.nih.gov/36414692</td>\n",
       "      <td>Single-cell RNA-seq methods to interrogate vir...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Purvesh Khatri; Catherine A Blish</td>\n",
       "      <td>Department of Medicine, Division of Infectious...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Catherine A Blish</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022/11/23</td>\n",
       "      <td>36414692</td>\n",
       "      <td>Antiviral immunity; Single-cell RNA sequencing...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>2022-11-21</td>\n",
       "      <td>2022-11-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Biology open</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.1242/bio.059695</td>\n",
       "      <td>https://pubmed.ncbi.nlm.nih.gov/36409314</td>\n",
       "      <td>A genome-wide CRISPR screen implicates plasma ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>James A Olzmann</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022/11/22</td>\n",
       "      <td>36409314</td>\n",
       "      <td>CRISPR; Ceramide; Lipid; Lipotoxicity; Membran...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>2022-11-21</td>\n",
       "      <td>2022-11-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>Nature reviews. Cardiology</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.1038/s41569-021-00665-7</td>\n",
       "      <td>https://pubmed.ncbi.nlm.nih.gov/35027697</td>\n",
       "      <td>Current and novel biomarkers of thrombotic ris...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Diana A Gorog</td>\n",
       "      <td>National Heart and Lung Institute, Imperial Co...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Taia T Wang</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022/1/15</td>\n",
       "      <td>35027697</td>\n",
       "      <td></td>\n",
       "      <td>P.A.G. has received consulting fees and/or hon...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>2022-01-13</td>\n",
       "      <td>2022-01-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>The Journal of infectious diseases</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.1093/infdis/jiab635</td>\n",
       "      <td>https://pubmed.ncbi.nlm.nih.gov/34979030</td>\n",
       "      <td>SARS-CoV-2 Variant Exposures Elicit Antibody R...</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Anthea M Mitchell; Sabrina A Mann; Joseph DeRisi</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022/1/4</td>\n",
       "      <td>34979030</td>\n",
       "      <td>B.1.1.529 (omicron); B.1.617.2 (delta); COVID-...</td>\n",
       "      <td></td>\n",
       "      <td>10.1101/2021.09.08.21263095</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2022-01-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>Infection control and hospital epidemiology</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.1017/ice.2021.391</td>\n",
       "      <td>https://pubmed.ncbi.nlm.nih.gov/34486503</td>\n",
       "      <td>Increased rates of secondary bacterial infecti...</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Charles R Langelier</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021/9/7</td>\n",
       "      <td>34486503</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>2021-09-06</td>\n",
       "      <td>2021-09-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>Plant molecular biology</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.1007/s11103-021-01139-7</td>\n",
       "      <td>https://pubmed.ncbi.nlm.nih.gov/33847871</td>\n",
       "      <td>Correction to: Current status and impending pr...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Rebecca S Bart</td>\n",
       "      <td>Donald Danforth Plant Science Center (DDPSC), ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021/4/14</td>\n",
       "      <td>33847871</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2021-04-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>Plant molecular biology</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.1007/s11103-020-01104-w</td>\n",
       "      <td>https://pubmed.ncbi.nlm.nih.gov/33604743</td>\n",
       "      <td>Current status and impending progress for cass...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Rebecca S Bart</td>\n",
       "      <td>Donald Danforth Plant Science Center (DDPSC), ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021/2/20</td>\n",
       "      <td>33604743</td>\n",
       "      <td>Cassava; Crop improvement; Genomics; Heterozyg...</td>\n",
       "      <td>The authors declare no conflicts of interest.</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>2021-02-18</td>\n",
       "      <td>2021-02-20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>233 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         journal             version  \\\n",
       "18                                         arxiv  arXiv:2206.01271v2   \n",
       "0               Diagnostics (Basel, Switzerland)                 NaN   \n",
       "1                 Pathogens (Basel, Switzerland)                 NaN   \n",
       "2                    Seminars in immunopathology                 NaN   \n",
       "3                                   Biology open                 NaN   \n",
       "..                                           ...                 ...   \n",
       "227                   Nature reviews. Cardiology                 NaN   \n",
       "228           The Journal of infectious diseases                 NaN   \n",
       "229  Infection control and hospital epidemiology                 NaN   \n",
       "230                      Plant molecular biology                 NaN   \n",
       "231                      Plant molecular biology                 NaN   \n",
       "\n",
       "                             doi                                       url  \\\n",
       "18     10.48550/arXiv.2206.01271          https://arxiv.org/abs/2206.01271   \n",
       "0    10.3390/diagnostics12112805  https://pubmed.ncbi.nlm.nih.gov/36428863   \n",
       "1      10.3390/pathogens11111345  https://pubmed.ncbi.nlm.nih.gov/36422597   \n",
       "2     10.1007/s00281-022-00972-2  https://pubmed.ncbi.nlm.nih.gov/36414692   \n",
       "3             10.1242/bio.059695  https://pubmed.ncbi.nlm.nih.gov/36409314   \n",
       "..                           ...                                       ...   \n",
       "227   10.1038/s41569-021-00665-7  https://pubmed.ncbi.nlm.nih.gov/35027697   \n",
       "228       10.1093/infdis/jiab635  https://pubmed.ncbi.nlm.nih.gov/34979030   \n",
       "229         10.1017/ice.2021.391  https://pubmed.ncbi.nlm.nih.gov/34486503   \n",
       "230   10.1007/s11103-021-01139-7  https://pubmed.ncbi.nlm.nih.gov/33847871   \n",
       "231   10.1007/s11103-020-01104-w  https://pubmed.ncbi.nlm.nih.gov/33604743   \n",
       "\n",
       "                                                 title authors2  \\\n",
       "18   Wildebeest Herds on Rolling Hills: Flocking on...      NaN   \n",
       "0    Efficient Tracing of the SARS-CoV-2 Omicron Va...      NaN   \n",
       "1                    Luna Virus and Helminths in Wild       NaN   \n",
       "2    Single-cell RNA-seq methods to interrogate vir...      NaN   \n",
       "3    A genome-wide CRISPR screen implicates plasma ...      NaN   \n",
       "..                                                 ...      ...   \n",
       "227  Current and novel biomarkers of thrombotic ris...      NaN   \n",
       "228  SARS-CoV-2 Variant Exposures Elicit Antibody R...      NaN   \n",
       "229  Increased rates of secondary bacterial infecti...      NaN   \n",
       "230  Correction to: Current status and impending pr...      NaN   \n",
       "231  Current status and impending progress for cass...      NaN   \n",
       "\n",
       "                  corresponding author  \\\n",
       "18                                 NaN   \n",
       "0                                        \n",
       "1                                        \n",
       "2    Purvesh Khatri; Catherine A Blish   \n",
       "3                                        \n",
       "..                                 ...   \n",
       "227                      Diana A Gorog   \n",
       "228                                      \n",
       "229                                      \n",
       "230                     Rebecca S Bart   \n",
       "231                     Rebecca S Bart   \n",
       "\n",
       "                      corresponding author institution version number type  \\\n",
       "18                                                 NaN            NaN  NaN   \n",
       "0                                                                 NaN  NaN   \n",
       "1                                                                 NaN  NaN   \n",
       "2    Department of Medicine, Division of Infectious...            NaN  NaN   \n",
       "3                                                                 NaN  NaN   \n",
       "..                                                 ...            ...  ...   \n",
       "227  National Heart and Lung Institute, Imperial Co...            NaN  NaN   \n",
       "228                                                               NaN  NaN   \n",
       "229                                                               NaN  NaN   \n",
       "230  Donald Danforth Plant Science Center (DDPSC), ...            NaN  NaN   \n",
       "231  Donald Danforth Plant Science Center (DDPSC), ...            NaN  NaN   \n",
       "\n",
       "     ...                                     biohub author  \\\n",
       "18   ...                                               NaN   \n",
       "0    ...                                                     \n",
       "1    ...                   Manu Vanaerschot; Cristina Tato   \n",
       "2    ...                                 Catherine A Blish   \n",
       "3    ...                                   James A Olzmann   \n",
       "..   ...                                               ...   \n",
       "227  ...                                       Taia T Wang   \n",
       "228  ...  Anthea M Mitchell; Sabrina A Mann; Joseph DeRisi   \n",
       "229  ...                               Charles R Langelier   \n",
       "230  ...                                                     \n",
       "231  ...                                                     \n",
       "\n",
       "                              pdf url publish date      pmid  \\\n",
       "18   https://arxiv.org/pdf/2206.01271          NaN       NaN   \n",
       "0                                 NaN   2022/11/27  36428863   \n",
       "1                                 NaN   2022/11/25  36422597   \n",
       "2                                 NaN   2022/11/23  36414692   \n",
       "3                                 NaN   2022/11/22  36409314   \n",
       "..                                ...          ...       ...   \n",
       "227                               NaN    2022/1/15  35027697   \n",
       "228                               NaN     2022/1/4  34979030   \n",
       "229                               NaN     2021/9/7  34486503   \n",
       "230                               NaN    2021/4/14  33847871   \n",
       "231                               NaN    2021/2/20  33604743   \n",
       "\n",
       "                                               keyword  \\\n",
       "18                                                 NaN   \n",
       "0    NGS; SARS-CoV-2; epidemiology; molecular assay...   \n",
       "1    Luna virus; Mastomys natalensis; Zambia; foetu...   \n",
       "2    Antiviral immunity; Single-cell RNA sequencing...   \n",
       "3    CRISPR; Ceramide; Lipid; Lipotoxicity; Membran...   \n",
       "..                                                 ...   \n",
       "227                                                      \n",
       "228  B.1.1.529 (omicron); B.1.617.2 (delta); COVID-...   \n",
       "229                                                      \n",
       "230                                                      \n",
       "231  Cassava; Crop improvement; Genomics; Heterozyg...   \n",
       "\n",
       "                                                  COIS  \\\n",
       "18                                                 NaN   \n",
       "0                                                        \n",
       "1         The authors declare no conflict of interest.   \n",
       "2                                                        \n",
       "3                                                        \n",
       "..                                                 ...   \n",
       "227  P.A.G. has received consulting fees and/or hon...   \n",
       "228                                                      \n",
       "229                                                      \n",
       "230                                                      \n",
       "231      The authors declare no conflicts of interest.   \n",
       "\n",
       "            confirm preprint doi record change number epost date2  \\\n",
       "18                           NaN                    0  2022-06-02   \n",
       "0                                                   0  2022-11-15   \n",
       "1                                                   0  2022-11-14   \n",
       "2                                                   0  2022-11-21   \n",
       "3                                                   0  2022-11-21   \n",
       "..                           ...                  ...         ...   \n",
       "227                                                 0  2022-01-13   \n",
       "228  10.1101/2021.09.08.21263095                    0         NaT   \n",
       "229                                                 0  2021-09-06   \n",
       "230                                                 0         NaT   \n",
       "231                                                 0  2021-02-18   \n",
       "\n",
       "    publish date2  \n",
       "18            NaT  \n",
       "0      2022-11-27  \n",
       "1      2022-11-25  \n",
       "2      2022-11-23  \n",
       "3      2022-11-22  \n",
       "..            ...  \n",
       "227    2022-01-15  \n",
       "228    2022-01-04  \n",
       "229    2021-09-07  \n",
       "230    2021-04-14  \n",
       "231    2021-02-20  \n",
       "\n",
       "[233 rows x 29 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df.reset_index(drop=True,inplace=True)\n",
    "df['date'] = [df.loc[i, 'epost date2'] if ( pd.isnull(df.loc[i, 'publish date'])==True or df.loc[i, 'epost date2'] >= df.loc[i, 'publish date2']) else df.loc[i, 'publish date2'] for i in range(len(df))]\n",
    "df = df.sort_values(by=['date'], ascending=True).drop(\n",
    "    columns=['epost date2', 'publish date2'])\n",
    "df['save datetime']=end  # search end date\n",
    "#df['save datetime'] = datetime.datetime.now().strftime('%m/%d/%Y') # when we save it\n",
    "\n",
    "df.insert(0, 'record id', np.arange(1, len(df)+1))\n",
    "order = ['record id', 'save datetime', 'biohub author','possible biohub author','format biohub author','corresponding author','corresponding author institution',\n",
    "        'journal', 'doi', 'pmid',\n",
    "    'title','url','abstract', 'keyword',  'pdf url', 'version',  'version number', 'type', \n",
    "        'date','epost date', 'publish date',\n",
    "        'authors','authors2', 'affiliations list', 'author - affiliations',\n",
    "        'published or not', 'confirm published doi',  'confirm preprint doi',\n",
    "        'possible match result','match id', 'record change number']\n",
    "for col in order:\n",
    "    if col not in df.columns.to_list():\n",
    "        df[col]=''\n",
    "df = df[order]\n",
    "\n",
    "df.fillna('', inplace=True)\n",
    "df.rename(columns=lambda x: x.lower(), inplace=True)\n",
    "\n",
    "df=af.authormatch_pre(df)\n",
    "df=af.authormatch_pub(df)\n",
    "\n",
    "filename = end+'_4searchresult.csv'\n",
    "df.to_csv('daily output/'+filename,index=False ,encoding='utf-8-sig')\n",
    "print('Fetch done.')\n",
    "\n",
    "\n",
    "df.to_csv('database/basedb.csv', encoding='utf-8-sig',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>journal</th>\n",
       "      <th>version</th>\n",
       "      <th>doi</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>author_corresponding</th>\n",
       "      <th>author_corresponding_institution</th>\n",
       "      <th>type</th>\n",
       "      <th>abstract</th>\n",
       "      <th>epost date</th>\n",
       "      <th>published or not</th>\n",
       "      <th>published_doi</th>\n",
       "      <th>affiliations list</th>\n",
       "      <th>author - affiliations</th>\n",
       "      <th>biohub author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bioRxiv</td>\n",
       "      <td>1</td>\n",
       "      <td>10.1101/2022.05.13.491573</td>\n",
       "      <td>http://www.biorxiv.org/content/10.1101/2022.05...</td>\n",
       "      <td>Quantifying the adaptive landscape of commensa...</td>\n",
       "      <td>Daniel P.G.H. Wong; Benjamin H. Good</td>\n",
       "      <td>Benjamin H Good</td>\n",
       "      <td>Stanford University</td>\n",
       "      <td>new results</td>\n",
       "      <td>Gut microbiota can adapt to their host environ...</td>\n",
       "      <td>01/15/2022</td>\n",
       "      <td>NA</td>\n",
       "      <td>No yet.</td>\n",
       "      <td>Department of Applied Physics, Stanford Univer...</td>\n",
       "      <td>Daniel P.G.H. Wong, Department of Applied Phys...</td>\n",
       "      <td>Benjamin H. Good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bioRxiv</td>\n",
       "      <td>1</td>\n",
       "      <td>10.1101/2022.05.12.491656</td>\n",
       "      <td>http://www.biorxiv.org/content/10.1101/2022.05...</td>\n",
       "      <td>Astrocytic Gi-GPCR activation enhances stimulu...</td>\n",
       "      <td>Trisha V. Vaidyanathan; Vincent Tse; Esther M....</td>\n",
       "      <td>Kira  Poskanzer</td>\n",
       "      <td>University of California, San Francisco</td>\n",
       "      <td>new results</td>\n",
       "      <td>Astrocytes perform critical functions in the n...</td>\n",
       "      <td>01/13/2022</td>\n",
       "      <td>NA</td>\n",
       "      <td>No yet.</td>\n",
       "      <td>Neuroscience Graduate Program, University of C...</td>\n",
       "      <td>Trisha V. Vaidyanathan, Neuroscience Graduate ...</td>\n",
       "      <td>Kira E. Poskanzer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bioRxiv</td>\n",
       "      <td>1</td>\n",
       "      <td>10.1101/2022.05.11.490523</td>\n",
       "      <td>http://www.biorxiv.org/content/10.1101/2022.05...</td>\n",
       "      <td>Fermented foods restructure gut microbiota and...</td>\n",
       "      <td>Sean Paul Spencer; Evelyn Giselle Lemus Silva;...</td>\n",
       "      <td>Justin L Sonnenburg</td>\n",
       "      <td>Stanford University</td>\n",
       "      <td>new results</td>\n",
       "      <td>Fermented foods are ancient and ubiquitous, th...</td>\n",
       "      <td>01/12/2022</td>\n",
       "      <td>NA</td>\n",
       "      <td>No yet.</td>\n",
       "      <td>Department of Microbiology and Immunology, Sta...</td>\n",
       "      <td>Sean Paul Spencer, Department of Microbiology ...</td>\n",
       "      <td>Justin Laine Sonnenburg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bioRxiv</td>\n",
       "      <td>2</td>\n",
       "      <td>10.1101/2022.04.28.489942</td>\n",
       "      <td>http://www.biorxiv.org/content/10.1101/2022.04...</td>\n",
       "      <td>Immediate myeloid depot for SARS-CoV-2 in the ...</td>\n",
       "      <td>Mélia Magnen; Ran You; Arjun A. Rao; Ryan T. D...</td>\n",
       "      <td>Mark R Looney</td>\n",
       "      <td>UCSF</td>\n",
       "      <td>new results</td>\n",
       "      <td>In the severe acute respiratory syndrome coron...</td>\n",
       "      <td>01/11/2022</td>\n",
       "      <td>NA</td>\n",
       "      <td>No yet.</td>\n",
       "      <td>Department of Medicine, University of Californ...</td>\n",
       "      <td>Mélia Magnen, Department of Medicine, Universi...</td>\n",
       "      <td>Charles R. Langelier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bioRxiv</td>\n",
       "      <td>1</td>\n",
       "      <td>10.1101/2022.05.10.491266</td>\n",
       "      <td>http://www.biorxiv.org/content/10.1101/2022.05...</td>\n",
       "      <td>Activated interstitial macrophages are a predo...</td>\n",
       "      <td>Timothy Ting-Hsuan Wu; Kyle J. Travaglini; Arj...</td>\n",
       "      <td>Mark A. Krasnow</td>\n",
       "      <td>Department of Biochemistry, Stanford Universit...</td>\n",
       "      <td>new results</td>\n",
       "      <td>Early stages of deadly respiratory diseases su...</td>\n",
       "      <td>01/10/2022</td>\n",
       "      <td>NA</td>\n",
       "      <td>No yet.</td>\n",
       "      <td>Department of Biochemistry, Stanford Universit...</td>\n",
       "      <td>Timothy Ting-Hsuan Wu, Department of Biochemis...</td>\n",
       "      <td>Catherine A. Blish</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   journal version                        doi  \\\n",
       "0  bioRxiv       1  10.1101/2022.05.13.491573   \n",
       "1  bioRxiv       1  10.1101/2022.05.12.491656   \n",
       "2  bioRxiv       1  10.1101/2022.05.11.490523   \n",
       "3  bioRxiv       2  10.1101/2022.04.28.489942   \n",
       "4  bioRxiv       1  10.1101/2022.05.10.491266   \n",
       "\n",
       "                                                 url  \\\n",
       "0  http://www.biorxiv.org/content/10.1101/2022.05...   \n",
       "1  http://www.biorxiv.org/content/10.1101/2022.05...   \n",
       "2  http://www.biorxiv.org/content/10.1101/2022.05...   \n",
       "3  http://www.biorxiv.org/content/10.1101/2022.04...   \n",
       "4  http://www.biorxiv.org/content/10.1101/2022.05...   \n",
       "\n",
       "                                               title  \\\n",
       "0  Quantifying the adaptive landscape of commensa...   \n",
       "1  Astrocytic Gi-GPCR activation enhances stimulu...   \n",
       "2  Fermented foods restructure gut microbiota and...   \n",
       "3  Immediate myeloid depot for SARS-CoV-2 in the ...   \n",
       "4  Activated interstitial macrophages are a predo...   \n",
       "\n",
       "                                             authors author_corresponding  \\\n",
       "0               Daniel P.G.H. Wong; Benjamin H. Good      Benjamin H Good   \n",
       "1  Trisha V. Vaidyanathan; Vincent Tse; Esther M....      Kira  Poskanzer   \n",
       "2  Sean Paul Spencer; Evelyn Giselle Lemus Silva;...  Justin L Sonnenburg   \n",
       "3  Mélia Magnen; Ran You; Arjun A. Rao; Ryan T. D...        Mark R Looney   \n",
       "4  Timothy Ting-Hsuan Wu; Kyle J. Travaglini; Arj...      Mark A. Krasnow   \n",
       "\n",
       "                    author_corresponding_institution         type  \\\n",
       "0                                Stanford University  new results   \n",
       "1            University of California, San Francisco  new results   \n",
       "2                                Stanford University  new results   \n",
       "3                                               UCSF  new results   \n",
       "4  Department of Biochemistry, Stanford Universit...  new results   \n",
       "\n",
       "                                            abstract  epost date  \\\n",
       "0  Gut microbiota can adapt to their host environ...  01/15/2022   \n",
       "1  Astrocytes perform critical functions in the n...  01/13/2022   \n",
       "2  Fermented foods are ancient and ubiquitous, th...  01/12/2022   \n",
       "3  In the severe acute respiratory syndrome coron...  01/11/2022   \n",
       "4  Early stages of deadly respiratory diseases su...  01/10/2022   \n",
       "\n",
       "  published or not published_doi  \\\n",
       "0               NA       No yet.   \n",
       "1               NA       No yet.   \n",
       "2               NA       No yet.   \n",
       "3               NA       No yet.   \n",
       "4               NA       No yet.   \n",
       "\n",
       "                                   affiliations list  \\\n",
       "0  Department of Applied Physics, Stanford Univer...   \n",
       "1  Neuroscience Graduate Program, University of C...   \n",
       "2  Department of Microbiology and Immunology, Sta...   \n",
       "3  Department of Medicine, University of Californ...   \n",
       "4  Department of Biochemistry, Stanford Universit...   \n",
       "\n",
       "                               author - affiliations            biohub author  \n",
       "0  Daniel P.G.H. Wong, Department of Applied Phys...         Benjamin H. Good  \n",
       "1  Trisha V. Vaidyanathan, Neuroscience Graduate ...        Kira E. Poskanzer  \n",
       "2  Sean Paul Spencer, Department of Microbiology ...  Justin Laine Sonnenburg  \n",
       "3  Mélia Magnen, Department of Medicine, Universi...     Charles R. Langelier  \n",
       "4  Timothy Ting-Hsuan Wu, Department of Biochemis...       Catherine A. Blish  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_records_df=pd.DataFrame()\n",
    "\n",
    "for col in ['title', 'authors', 'author_corresponding', 'author_corresponding_institution', 'version', 'type','abstract','published']:\n",
    "    if col in list(data.keys()):\n",
    "        full_records_df.loc[i,col]=data[col]\n",
    "    else:\n",
    "        full_records_df.loc[i,col]=''\n",
    "# full_records_df.loc[i,'title']=data['title']\n",
    "# full_records_df.loc[i,'authors2']=data['authors']\n",
    "# full_records_df.loc[i,'corresponding author']=data['author_corresponding']\n",
    "# full_records_df.loc[i,'corresponding author institution']=data['author_corresponding_institution']\n",
    "# full_records_df.loc[i,'version number']=data['version']\n",
    "# full_records_df.loc[i,'type']=data['type']\n",
    "#full_records_df.loc[i,'abstract']=data['abstract']\n",
    "#full_records_df.loc[i,'published or not']=data['published'] # NA\n",
    "full_records_df=full_records_df.rename(columns={'authors':'authors2','published':'published or not'}) \n",
    "\n",
    "if 'date' in list(data.keys()):\n",
    "    full_records_df.loc[i,'epost date']=datetime.datetime.strptime(data['date'], '%Y-%M-%d').strftime('%m/%d/%Y')\n",
    "else:\n",
    "    full_records_df.loc[i,'epost date']=''\n",
    "\n",
    "print(full_records_df.loc[i,'published or not'])\n",
    "if (full_records_df.loc[i,'published or not'] != 'NA') & (full_records_df.loc[i,'published or not'] != 'NaN'):\n",
    "    pub_api='https://api.biorxiv.org/pubs/'+full_records_df.loc[i,'journal']+'/'+full_records_df.loc[i,'doi']+'/na/JSON'\n",
    "    data2=requests.get(url=pub_api).json()['collection'][-1]\n",
    "    full_records_df.loc[i,'confirm published doi']=data2['published_doi']\n",
    "    #full_records_df.loc[i,'published_journal']=data['published_journal']\n",
    "    #full_records_df.loc[i,'published_date']=data['published_date']\n",
    "else:\n",
    "    full_records_df.loc[i,'published_doi']='No yet.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['doi', 'title', 'authors', 'author_corresponding', 'author_corresponding_institution', 'date', 'version', 'type', 'license', 'category', 'jatsxml', 'abstract', 'published', 'server'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'doi': '10.1101/2022.01.10.475285',\n",
       " 'title': 'MEK inhibition enhances presentation of targetable MHC-I tumor antigens in mutant melanomas',\n",
       " 'authors': 'Stopfer, L. E.; Rettko, N.; Leddy, O.; Mesfin, J. M.; Brown, E.; Winski, S.; Bryson, B. D.; Wells, J. A.; White, F.',\n",
       " 'author_corresponding': 'Forest  White',\n",
       " 'author_corresponding_institution': 'Massachusetts Institute of Technology',\n",
       " 'date': '2022-05-23',\n",
       " 'version': '2',\n",
       " 'type': 'new results',\n",
       " 'license': 'cc_no',\n",
       " 'category': 'cancer biology',\n",
       " 'jatsxml': 'https://www.biorxiv.org/content/early/2022/05/23/2022.01.10.475285.source.xml',\n",
       " 'abstract': 'Combining multiple therapeutic strategies in NRAS/BRAF mutant melanoma - namely MEK/BRAF kinase inhibitors, immune checkpoint inhibitors, and targeted immunotherapies - may offer an improved survival benefit by overcoming limitations associated with any individual therapy. Still, optimal combination, order, and timing of administration remains under investigation. Here, we measure how MEK inhibition (MEKi) alters anti-tumor immunity by utilizing quantitative immunopeptidomics to profile changes in the peptide MHC (pMHC) repertoire. These data reveal a collection of tumor antigens whose presentation levels are selectively augmented following therapy, including several epitopes present at over 1000 copies-per-cell. We leveraged the tunable abundance of MEKi-modulated antigens by targeting 4 epitopes with pMHC-specific T cell engagers and antibody drug conjugates, enhancing cell killing in tumor cells following MEK inhibition. These results highlight drug treatment as a means to enhance immunotherapy efficacy by targeting specific upregulated pMHCs and provide a methodological framework for identifying, quantifying, and therapeutically targeting additional epitopes of interest.\\n\\nSIGNIFICANCEKinase inhibitor treatment in NRAS/BRAF mutant melanoma can sensitize tumors to immunotherapy, in part through an increase in average surface presentation of peptide MHC molecules. Here, we demonstrate that MEK inhibition selectively boosts epitope abundance of select tumor-associated antigens in vitro and in vivo, enhancing targeted immunotherapy efficacy against these treatment-modulated epitopes.',\n",
       " 'published': 'NA',\n",
       " 'server': 'biorxiv'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "col=['title', 'authors', 'author_corresponding', 'author_corresponding_institution', 'version', 'type','abstract','date',   'published', 'server']\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NA'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['published']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'doi': '10.1101/2022.01.10.475285',\n",
       " 'title': 'MEK inhibition enhances presentation of targetable MHC-I tumor antigens in mutant melanomas',\n",
       " 'authors': 'Stopfer, L. E.; Rettko, N.; Leddy, O.; Mesfin, J. M.; Brown, E.; Winski, S.; Bryson, B. D.; Wells, J. A.; White, F.',\n",
       " 'author_corresponding': 'Forest  White',\n",
       " 'author_corresponding_institution': 'Massachusetts Institute of Technology',\n",
       " 'date': '2022-05-23',\n",
       " 'version': '2',\n",
       " 'type': 'new results',\n",
       " 'license': 'cc_no',\n",
       " 'category': 'cancer biology',\n",
       " 'jatsxml': 'https://www.biorxiv.org/content/early/2022/05/23/2022.01.10.475285.source.xml',\n",
       " 'abstract': 'Combining multiple therapeutic strategies in NRAS/BRAF mutant melanoma - namely MEK/BRAF kinase inhibitors, immune checkpoint inhibitors, and targeted immunotherapies - may offer an improved survival benefit by overcoming limitations associated with any individual therapy. Still, optimal combination, order, and timing of administration remains under investigation. Here, we measure how MEK inhibition (MEKi) alters anti-tumor immunity by utilizing quantitative immunopeptidomics to profile changes in the peptide MHC (pMHC) repertoire. These data reveal a collection of tumor antigens whose presentation levels are selectively augmented following therapy, including several epitopes present at over 1000 copies-per-cell. We leveraged the tunable abundance of MEKi-modulated antigens by targeting 4 epitopes with pMHC-specific T cell engagers and antibody drug conjugates, enhancing cell killing in tumor cells following MEK inhibition. These results highlight drug treatment as a means to enhance immunotherapy efficacy by targeting specific upregulated pMHCs and provide a methodological framework for identifying, quantifying, and therapeutically targeting additional epitopes of interest.\\n\\nSIGNIFICANCEKinase inhibitor treatment in NRAS/BRAF mutant melanoma can sensitize tumors to immunotherapy, in part through an increase in average surface presentation of peptide MHC molecules. Here, we demonstrate that MEK inhibition selectively boosts epitope abundance of select tumor-associated antigens in vitro and in vivo, enhancing targeted immunotherapy efficacy against these treatment-modulated epitopes.',\n",
       " 'published': 'NA',\n",
       " 'server': 'biorxiv'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_api='https://api.biorxiv.org/details/biorxiv/10.1101/2022.01.10.475285/na/JSON'\n",
    "data=requests.get(url=paper_api).json()['collection'][-1]\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pj2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utility of [1,2,3,4,5,6,7,8,9], 12: 35.064\n",
      "Utility of [1,3,4,5,6,7,8,9], 12: 31.182\n",
      "Utility of [1,3,5,6,7,8,9], 12: 29.231\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from itertools import chain, combinations\n",
    "\n",
    "class State(tuple):\n",
    "\t# A state is defined as a tuple (numbers, dice_summation)\n",
    "\t# Access with the following command:\n",
    "\t# numbers, dice_summation = state\n",
    "\tdef __new__(self, numbers_left, dice_summation):\n",
    "\t\treturn tuple.__new__(State, (frozenset(numbers_left), dice_summation))\n",
    "\n",
    "class Environment:\n",
    "\tdef __all_states_and_actions(self):\n",
    "\t\tall_numbers_left = [[], [1]]\n",
    "\t\tfor i in range(2, self.total_numbers + 1):\n",
    "\t\t\tfor curr in range(len(all_numbers_left)):\n",
    "\t\t\t\tcurr_ = all_numbers_left[curr].copy()\n",
    "\t\t\t\tcurr_.append(i)\n",
    "\t\t\t\tall_numbers_left.append(curr_)\n",
    "\n",
    "\t\tall_dice_summation = list(range(2, 12 + 1))\n",
    "\n",
    "\t\tstates = []\n",
    "\t\tactions = {}\n",
    "\t\t\n",
    "\t\tfor number_list in all_numbers_left:\n",
    "\t\t\tfor dice in all_dice_summation:\n",
    "\t\t\t\tstates.append(State(number_list, dice))\n",
    "\t\t\t\tactions[State(number_list, dice)] = []\n",
    "\n",
    "\t\tfor numbers in all_numbers_left:\n",
    "\t\t\tall_combinations = chain.from_iterable(combinations(numbers, r) for r in range(len(numbers)+1))\n",
    "\t\t\tfor combination in all_combinations:\n",
    "\t\t\t\tdice = self.calc_sum(combination)\n",
    "\t\t\t\tif dice>=2 and dice<=12:\n",
    "\t\t\t\t\tactions[State(numbers, dice)].append(combination)\n",
    "\t\t\n",
    "\t\treturn states, actions\n",
    "\n",
    "\n",
    "\tdef __init__(self):\n",
    "\t\tself.total_numbers = 9\n",
    "\t\tself.prob_dist = {i:0 for i in range(2, 12 + 1)}\n",
    "\t\tfor i in range(1, 6 + 1):\n",
    "\t\t\tfor j in range(1, 6 + 1):\n",
    "\t\t\t\tself.prob_dist[i+j] += 1/6 * 1/6\n",
    "\t\tself.all_states, self.all_states_actions = self.__all_states_and_actions()\n",
    "\n",
    "\n",
    "\tdef get_all_states(self):\n",
    "\t\t# Get a list of all states\n",
    "\t\t# Each state is a tuple - (numbers_left, dice_summation) \n",
    "\t\treturn self.all_states\n",
    "\n",
    "\tdef calc_sum(self, numbers):\n",
    "\t\t# Calculate the summation of things in a list/set\n",
    "\t\ts = 0\n",
    "\t\tfor i in numbers:\n",
    "\t\t\ts += i\n",
    "\t\treturn s\n",
    "\n",
    "\n",
    "\n",
    "\t\n",
    "\tdef available_actions(self, state):\n",
    "\t\t# Return a list of actions that is allowed in this case\n",
    "\t\t# Each action is a set of numbers.\n",
    "\t\t#返回在这种情况下允许的操作列表\n",
    "\t\t#每个动作都是一组数字。\n",
    "  \n",
    "\t\t# 返回一系列（），里面都是总和=筛子总和的\n",
    "\t\treturn self.all_states_actions[state]\n",
    "\n",
    "\tdef all_transition_next(self, numbers_left, action_taken):\n",
    "\t\t# Return a list of all possible next steps with their probability.\n",
    "\t\t# Input: Current numbers and an action (a subset of previous numbers)\n",
    "\t\t# Each next step is represented in tuple (state, probability of the state)\n",
    "\t\t# State is a tuple itself - (numbers_left, dice_summation) \n",
    "\t\t#返回所有可能的下一步及其概率的列表。\n",
    "\t\t#输入：当前数字和动作（先前数字的子集）\n",
    "\t\t#每个下一步都以元组（状态，状态的概率）表示\n",
    "\t\t#状态本身是一个元组-（numbers_left，dice_summation）\n",
    "  \n",
    "\t\t# 返回剩下的数字集合，从2~12，2~12中每一个数字被拿走的概率\n",
    "\t\tnumbers_left = set(numbers_left)\n",
    "\t\tfor it in action_taken:\n",
    "\t\t\tnumbers_left.remove(it)\n",
    "\t\treturn [(State(numbers_left, sum_), self.prob_dist[sum_]) for sum_ in self.prob_dist]\n",
    "\n",
    "class Agent:\n",
    "\tdef __init__(self, env):\n",
    "\t\tself.env = env\n",
    "\t\tself.all_states = env.get_all_states()\n",
    "\t\tself.utilities = {state:0 for state in self.all_states}\n",
    "\n",
    "\t #这个返回的就是：放弃后。总reward值\n",
    "\tdef giveup_reward(self, numbers_left): \n",
    "\t\t# The reward for choosing give up at this state\n",
    "\t\tc = self.env.total_numbers\n",
    "\t\treturn c*(c+1)//2 - self.env.calc_sum(numbers_left)\n",
    "\n",
    "\t# 返回最佳的移动策略。\n",
    " \t# 例如：State([1,2,3,4,5,6,7,8,9], 12)最佳操作是从框中删除数字 3 和 9，因此策略函数应返回值 （3，9）\n",
    "\t# 给定状态 s=State（n，r），其中 n 是框中的数字列表，r 是当前滚动，self.utilities[s] 是该状态的实用程序\n",
    "\t# def value_iteration(self):\n",
    "\t# \tmax_change = 1e5\n",
    "\t# \twhile max_change >= 0.001:\n",
    "\t# \t\tutilities_pre = self.utilities.copy() # Copy the utility, e.g. U_{t-1}\n",
    "\t# \t\tmax_change = 0 # Measure the maximum change in all states for this iteration - if smaller than 0.001 we stop.\n",
    "\t# \t\tfor state in self.all_states:\n",
    "\t# \t\t\t# Complete this part with follwoing functions \n",
    "\t# \t\t\t# self.giveup_reward, self.env.available_actions, self.env.all_transition_next\n",
    "\t# \t\t\tutilities=np.zeros(len(self.env.all_transition_next))\n",
    "\t# \t\t\tactions=self.env.available_actions(state)\n",
    "\t# \t\t\tif len(actions)==0:\n",
    "\t# \t\t\t\tvalue=self.giveup_reward(state)\n",
    "\t# \t\t\t\tbreak\n",
    "\t# \t\t\tfor action in self.available_actions(state):\n",
    "\t# \t\t\t\tself.reward=giveuo_reward\n",
    "\t\t\t\t\t\n",
    "\t# \t\t\t\t...\n",
    "\n",
    "\t# \t\t\tnumbers_left=self.env.available_actions\n",
    "\t#\t \t\tnumbers_left)\n",
    "\t# \t\t\tself.utilities=最佳策略\n",
    "\t\n",
    "\tdef value_iteration(self):\n",
    "\t\t\tmax_change = 1e5\n",
    "\t\t\twhile max_change >= 0.001:\n",
    "\t\t\t\tutilities_pre = self.utilities.copy() # Copy the utility, e.g. U_{t-1}\n",
    "\t\t\t\tmax_change = 0 # Measure the maximum change in all states for this iteration - if smaller than 0.001 we stop.\n",
    "\t\t\t\tfor state in self.all_states:\n",
    "\t\t\t\t\t#u=utilities_pre[state]\n",
    "\t\t\t\t\tu_list=[]\n",
    "\t\t\t\t\tnumbers_left=state[0]\n",
    "\t\t\t\t\tfor action in self.env.available_actions(state):\n",
    "\t\t\t\t\t\tu=0\n",
    "\t\t\t\t\t\t#for prob, state_prime in [x[1],x[0] for x in self.env.all_transition_next(numbers_left, action)]:\n",
    "\t\t\t\t\t\tfor x in self.env.all_transition_next(numbers_left, action):\n",
    "\t\t\t\t\t\t\tstate_prime=x[0]\n",
    "\t\t\t\t\t\t\tprob=x[1]\n",
    "\t\t\t\t\t\t\tu += prob * (utilities_pre[state_prime])\n",
    "\t\t\t\t\t\tu_list.append(u)\n",
    "\t\t\t\t\tif len(self.env.available_actions(state))==0:\n",
    "\t\t\t\t\t\t#u = self.giveup_reward(numbers_left)#+u  #utilities_pre[state]\n",
    "\t\t\t\t\t\tu_list.append(self.giveup_reward(numbers_left))\n",
    "\t\t\t\t\tu=max(u_list)\n",
    "\t\t\t\t\tmax_change = max(max_change, u-self.utilities[state])\n",
    "\t\t\t\t\t#print(u_list)\n",
    "\t\t\t\t\t#print(u)\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\tself.utilities[state]=u\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "\tenv = Environment()\n",
    "\t# Try the following commands before coding\n",
    "\t# n=env.available_actions(State([1,2,3,4,5,6,7,8,9], 12))\n",
    "\t# print(env.available_actions(State([1,2,3,4,5,6,7,8,9], 12)))\n",
    "\t# m=env.all_transition_next([1,2,3,4,5,6,7,8,9], [1,2])\n",
    "\t# print(env.all_transition_next([1,2,3,4,5,6,7,8,9], [1,2]))\n",
    "\n",
    "\t#print(env.available_actions(State([1,2,3,4,5,6,7,8,9], 12)))\n",
    "\t#print(env.all_transition_next([1,2,3,4,5,6,7,8,9], [1,2]))\n",
    "\n",
    "\tagent = Agent(env)\n",
    "\t# print(agent.giveup_reward([1,2,3,4,5,6,7]))\n",
    "\t# Q1: Complete the Value iteration code here!\n",
    "\tagent.value_iteration()\n",
    "\tprint('Utility of [1,2,3,4,5,6,7,8,9], 12: %.3f' % agent.utilities[State([1,2,3,4,5,6,7,8,9], 12)])\n",
    "\tprint('Utility of [1,3,4,5,6,7,8,9], 12: %.3f' % agent.utilities[State([1,3,4,5,6,7,8,9], 12)])\n",
    "\tprint('Utility of [1,3,5,6,7,8,9], 12: %.3f' % agent.utilities[State([1,3,5,6,7,8,9], 12)])\n",
    "\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've included a few things below that I noticed are incorrect:\n",
    "1. When accumulating the utilities, you should not be adding the original utility (i.e. u should be initialized to 0, not utilities_pre[state]). The new utility should replace the old utility, not compound it. Otherwise, the algorithm could never converge (because every state's utility would, at a minimum, double every iteration).\n",
    "2. Similar to the above, when adding the reward to u when there are no actions available, you should not be adding utilities_pre[state] for the same reason.\n",
    "3. The utility calculated should be for each action (i.e. reset u for each loop of \"for action in ...\"). Remember from the Bellman equation that you are finding the maximum of the expected utilities of each action.\n",
    "4. When updating u in the for loop, the utility being multiplied should be the utility of the destination state, not the sourse state (i.e. U(s'), not U(s) in the Bellman equation). Note that U(s') is the state that is returned with each probability by self.env.all_transition_next.\n",
    "u should be used to update the self.utilities dictionary, not utilities_pre when finished calculating the new utility.\n",
    "\n",
    "Those are all the issues I see for now. Your solution seems correctly structured, and it looks like you've already noticed and avoided some of the common mistakes. If you fix the above, you should be very close to a correct solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Agent() takes no arguments",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn [58], line 30\u001b[0m\n",
      "\u001b[1;32m     26\u001b[0m env \u001b[39m=\u001b[39m Environment()\n",
      "\u001b[1;32m     27\u001b[0m \u001b[39m#print(env.available_actions(State([1,2,3,4,5,6,7,8,9], 12)))\u001b[39;00m\n",
      "\u001b[1;32m     28\u001b[0m \u001b[39m#print(env.all_transition_next([1,2,3,4,5,6,7,8,9], [1,2]))\u001b[39;00m\n",
      "\u001b[0;32m---> 30\u001b[0m agent \u001b[39m=\u001b[39m Agent(env)\n",
      "\u001b[1;32m     31\u001b[0m \u001b[39m#print(agent.giveup_reward([1,2,3,4,5,6,7]))\u001b[39;00m\n",
      "\u001b[1;32m     32\u001b[0m \u001b[39m# Q1: Complete the Value iteration code here!\u001b[39;00m\n",
      "\u001b[1;32m     33\u001b[0m agent\u001b[39m.\u001b[39mvalue_iteration()\n",
      "\n",
      "\u001b[0;31mTypeError\u001b[0m: Agent() takes no arguments"
     ]
    }
   ],
   "source": [
    "# 发kyle的版本\n",
    "class Agent:\n",
    "\tdef value_iteration(self):\n",
    "\t\t\tmax_change = 1e5\n",
    "\t\t\twhile max_change >= 0.001:\n",
    "\t\t\t\tutilities_pre = self.utilities.copy() # Copy the utility, e.g. U_{t-1}\n",
    "\t\t\t\tmax_change = 0 # Measure the maximum change in all states for this iteration - if smaller than 0.001 we stop.\n",
    "\t\t\t\tfor state in self.all_states:\n",
    "\t\t\t\t\t#u=utilities_pre[state]\n",
    "\t\t\t\t\tu_list=[]\n",
    "\t\t\t\t\tnumbers_left=state[0]\n",
    "\t\t\t\t\tfor action in self.env.available_actions(state):\n",
    "\t\t\t\t\t\tu=0\n",
    "\t\t\t\t\t\t#for prob, state_prime in [x[1],x[0] for x in self.env.all_transition_next(numbers_left, action)]:\n",
    "\t\t\t\t\t\tfor x in self.env.all_transition_next(numbers_left, action):\n",
    "\t\t\t\t\t\t\tstate_prime=x[0]\n",
    "\t\t\t\t\t\t\tprob=x[1]\n",
    "\t\t\t\t\t\t\tu += prob * (utilities_pre[state_prime])\n",
    "\t\t\t\t\t\tu_list.append(u)\n",
    "\t\t\t\t\tif len(self.env.available_actions(state))==0:\n",
    "\t\t\t\t\t\t#u = self.giveup_reward(numbers_left)#+u  #utilities_pre[state]\n",
    "\t\t\t\t\t\tu_list.append(self.giveup_reward(numbers_left))\n",
    "\t\t\n",
    "\t\t\t\t\tmax_change=abs(u-self.utilities[state])\n",
    "\t\t\t\t\tself.utilities[state]=u\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "\tenv = Environment()\n",
    "\t#print(env.available_actions(State([1,2,3,4,5,6,7,8,9], 12)))\n",
    "\t#print(env.all_transition_next([1,2,3,4,5,6,7,8,9], [1,2]))\n",
    "\n",
    "\tagent = Agent(env)\n",
    "\t#print(agent.giveup_reward([1,2,3,4,5,6,7]))\n",
    "\t# Q1: Complete the Value iteration code here!\n",
    "\tagent.value_iteration()\n",
    "\tprint('Utility of [1,2,3,4,5,6,7,8,9], 12: %.3f' % agent.utilities[State([1,2,3,4,5,6,7,8,9], 12)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3, 9), (4, 8), (5, 7), (1, 2, 9), (1, 3, 8), (1, 4, 7), (1, 5, 6), (2, 3, 7), (2, 4, 6), (3, 4, 5), (1, 2, 3, 6), (1, 2, 4, 5)]\n"
     ]
    }
   ],
   "source": [
    "print(env.available_actions(State([1,2,3,4,5,6,7,8,9], 12)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma=1\n",
    "\n",
    "def value_iteration(self):\n",
    "\t\tmax_change = 1e5\n",
    "\t\twhile max_change >= 0.001:\n",
    "\t\t\tutilities_pre = self.utilities.copy() # Copy the utility, e.g. U_{t-1}\n",
    "\t\t\tmax_change = 0 # Measure the maximum change in all states for this iteration - if smaller than 0.001 we stop.\n",
    "\t\t\tfor state in self.all_states:\n",
    "\t\t\t\t# Complete this part with follwoing functions \n",
    "\t\t\t\t# self.giveup_reward, self.env.available_actions, self.env.all_transition_next\n",
    "\t\t\t\t\n",
    "\t\t\t\tutilities=np.zeros(len(self.env.all_transition_next))\n",
    "\tactions=self.env.available_actions(state)\n",
    "\t\t\t\tif len(actions)==0:\n",
    "\t\t\t\t\tvalue=self.giveup_reward(state)\n",
    "\t\t\t\t\tbreak\n",
    "\t\t\t\tfor action in self.available_actions(state):\n",
    "\t\t\t\t\tself.reward=giveuo_reward\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\t...\n",
    "\n",
    "\t\t\t\tnumbers_left=self.env.available_actions\n",
    "\t\t\t\tnumbers_left)\n",
    "\t\t\t\tself.utilities=最佳策略\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, 9),\n",
       " (4, 8),\n",
       " (5, 7),\n",
       " (1, 2, 9),\n",
       " (1, 3, 8),\n",
       " (1, 4, 7),\n",
       " (1, 5, 6),\n",
       " (2, 3, 7),\n",
       " (2, 4, 6),\n",
       " (3, 4, 5),\n",
       " (1, 2, 3, 6),\n",
       " (1, 2, 4, 5)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.027777777777777776"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "m[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((frozenset({3, 4, 5, 6, 7, 8, 9}), 2), 0.027777777777777776),\n",
       " ((frozenset({3, 4, 5, 6, 7, 8, 9}), 3), 0.05555555555555555),\n",
       " ((frozenset({3, 4, 5, 6, 7, 8, 9}), 4), 0.08333333333333333),\n",
       " ((frozenset({3, 4, 5, 6, 7, 8, 9}), 5), 0.1111111111111111),\n",
       " ((frozenset({3, 4, 5, 6, 7, 8, 9}), 6), 0.1388888888888889),\n",
       " ((frozenset({3, 4, 5, 6, 7, 8, 9}), 7), 0.16666666666666669),\n",
       " ((frozenset({3, 4, 5, 6, 7, 8, 9}), 8), 0.1388888888888889),\n",
       " ((frozenset({3, 4, 5, 6, 7, 8, 9}), 9), 0.1111111111111111),\n",
       " ((frozenset({3, 4, 5, 6, 7, 8, 9}), 10), 0.08333333333333333),\n",
       " ((frozenset({3, 4, 5, 6, 7, 8, 9}), 11), 0.05555555555555555),\n",
       " ((frozenset({3, 4, 5, 6, 7, 8, 9}), 12), 0.027777777777777776)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "giveup_reward(self, numbers_left):\n",
    "\t\t# The reward for choosing give up at this state\n",
    "\t\tc = self.env.total_numbers\n",
    "\t\treturn c*(c+1)//2 - self.env.calc_sum(numbers_left)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3, 9), (4, 8), (5, 7), (1, 2, 9), (1, 3, 8), (1, 4, 7), (1, 5, 6), (2, 3, 7), (2, 4, 6), (3, 4, 5), (1, 2, 3, 6), (1, 2, 4, 5)]\n",
      "/n\n",
      "[((frozenset({3, 4, 5, 6, 7, 8, 9}), 2), 0.027777777777777776), ((frozenset({3, 4, 5, 6, 7, 8, 9}), 3), 0.05555555555555555), ((frozenset({3, 4, 5, 6, 7, 8, 9}), 4), 0.08333333333333333), ((frozenset({3, 4, 5, 6, 7, 8, 9}), 5), 0.1111111111111111), ((frozenset({3, 4, 5, 6, 7, 8, 9}), 6), 0.1388888888888889), ((frozenset({3, 4, 5, 6, 7, 8, 9}), 7), 0.16666666666666669), ((frozenset({3, 4, 5, 6, 7, 8, 9}), 8), 0.1388888888888889), ((frozenset({3, 4, 5, 6, 7, 8, 9}), 9), 0.1111111111111111), ((frozenset({3, 4, 5, 6, 7, 8, 9}), 10), 0.08333333333333333), ((frozenset({3, 4, 5, 6, 7, 8, 9}), 11), 0.05555555555555555), ((frozenset({3, 4, 5, 6, 7, 8, 9}), 12), 0.027777777777777776)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__=='__main__':\n",
    "\tenv = Environment()\n",
    "\t# Try the following commands before coding\n",
    "\t# n=env.available_actions(State([1,2,3,4,5,6,7,8,9], 12))\n",
    "\tprint(env.available_actions(State([1,2,3,4,5,6,7,8,9], 12)))\n",
    "\t# m=env.all_transition_next([1,2,3,4,5,6,7,8,9], [1,2])\n",
    " \n",
    "\tprint('/n')\n",
    "\tprint(env.all_transition_next([1,2,3,4,5,6,7,8,9], [1,2]))\n",
    " \n",
    " \n",
    "\t#print(env.available_actions(State([1, 3, 4, 6, 9], 11)))\n",
    "\t#print(env.all_transition_next([1,2,3,4,5,6,7,8,9], [1,2]))\n",
    "\n",
    "\t#agent = Agent(env)\n",
    "\t#print(agent.giveup_reward( [3 , 9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_iteration就是返回take action后的最大reward值 + utility?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Utility of [1,2,3,4,5,6,7,8,9], 12: 35.064\n",
    "Utility of [1,3,4,5,6,7,8,9], 12: 31.182\n",
    "Utility of [1,3,5,6,7,8,9], 12: 29.231\n",
    "Optimal action of [1,2,3,4,5,6,7,8,9], 12: (3, 9)\n",
    "Optimal action of [1,3,4,5,6,7,8,9], 12: (4, 8)\n",
    "Optimal action of [1,3,5,6,7,8,9], 12: (5, 7)\n",
    "\n",
    "\n",
    "Utility of [], 8: 45.000\n",
    "Utility of [1, 2, 7], 9: 44.000\n",
    "Utility of [2, 6, 8, 9], 5: 20.000\n",
    "Utility of [1, 3, 4, 6, 9], 11: 34.574\n",
    "Utility of [1, 2, 3, 4, 5, 6, 7, 8, 9], 3: 30.686\n",
    "\n",
    "Optimal action of [], 8: []\n",
    "Optimal action of [1, 2, 7], 9: (2, 7)\n",
    "Optimal action of [2, 6, 8, 9], 5: []\n",
    "Optimal action of [1, 3, 4, 6, 9], 11: (1, 4, 6)\n",
    "Optimal action of [1, 2, 3, 4, 5, 6, 7, 8, 9], 3: (3,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# . policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utility of [1,2,3,4,5,6,7,8,9], 12: 35.064\n",
      "Optimal action of [1,2,3,4,5,6,7,8,9], 12: [(3, 9)]\n",
      "Optimal action of [1,3,4,5,6,7,8,9], 12: [(4, 8)]\n",
      "Optimal action of [1,3,5,6,7,8,9], 12: [(5, 7)]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from itertools import chain, combinations\n",
    "import numpy as np\n",
    "\n",
    "class State(tuple):\n",
    "\t# A state is defined as a tuple (numbers, dice_summation)\n",
    "\t# Access with the following command:\n",
    "\t# numbers, dice_summation = state\n",
    "\tdef __new__(self, numbers_left, dice_summation):\n",
    "\t\treturn tuple.__new__(State, (frozenset(numbers_left), dice_summation))\n",
    "\n",
    "class Environment:\n",
    "\tdef __all_states_and_actions(self):\n",
    "\t\tall_numbers_left = [[], [1]]\n",
    "\t\tfor i in range(2, self.total_numbers + 1):\n",
    "\t\t\tfor curr in range(len(all_numbers_left)):\n",
    "\t\t\t\tcurr_ = all_numbers_left[curr].copy()\n",
    "\t\t\t\tcurr_.append(i)\n",
    "\t\t\t\tall_numbers_left.append(curr_)\n",
    "\n",
    "\t\tall_dice_summation = list(range(2, 12 + 1))\n",
    "\n",
    "\t\tstates = []\n",
    "\t\tactions = {}\n",
    "\t\t\n",
    "\t\tfor number_list in all_numbers_left:\n",
    "\t\t\tfor dice in all_dice_summation:\n",
    "\t\t\t\tstates.append(State(number_list, dice))\n",
    "\t\t\t\tactions[State(number_list, dice)] = []\n",
    "\n",
    "\t\tfor numbers in all_numbers_left:\n",
    "\t\t\tall_combinations = chain.from_iterable(combinations(numbers, r) for r in range(len(numbers)+1))\n",
    "\t\t\tfor combination in all_combinations:\n",
    "\t\t\t\tdice = self.calc_sum(combination)\n",
    "\t\t\t\tif dice>=2 and dice<=12:\n",
    "\t\t\t\t\tactions[State(numbers, dice)].append(combination)\n",
    "\t\t\n",
    "\t\treturn states, actions\n",
    "\n",
    "\n",
    "\tdef __init__(self):\n",
    "\t\tself.total_numbers = 9\n",
    "\t\tself.prob_dist = {i:0 for i in range(2, 12 + 1)}\n",
    "\t\tfor i in range(1, 6 + 1):\n",
    "\t\t\tfor j in range(1, 6 + 1):\n",
    "\t\t\t\tself.prob_dist[i+j] += 1/6 * 1/6\n",
    "\t\tself.all_states, self.all_states_actions = self.__all_states_and_actions()\n",
    "\n",
    "\tdef available_actions(self, state):\n",
    "\t\t# Return a list of actions that is allowed in this case\n",
    "\t\t# Each action is a set of numbers.\n",
    "\t\treturn self.all_states_actions[state]\n",
    "\n",
    "\tdef all_transition_next(self, numbers_left, action_taken):\n",
    "\t\t# Return a list of all possible next steps with their probability.\n",
    "\t\t# Input: Current numbers and an action (a subset of previous numbers)\n",
    "\t\t# Each next step is represented in tuple (state, probability of the state)\n",
    "\t\t# State is a tuple itself - (numbers_left, dice_summation) \n",
    "\t\tnumbers_left = set(numbers_left)\n",
    "\t\tfor it in action_taken:\n",
    "\t\t\tnumbers_left.remove(it)\n",
    "\t\treturn [(State(numbers_left, sum_), self.prob_dist[sum_]) for sum_ in self.prob_dist]\n",
    "\n",
    "\tdef get_all_states(self):\n",
    "\t\t# Get a list of all states\n",
    "\t\t# Each state is a tuple - (numbers_left, dice_summation) \n",
    "\t\treturn self.all_states\n",
    "\n",
    "\tdef calc_sum(self, numbers):\n",
    "\t\t# Calculate the summation of things in a list/set\n",
    "\t\ts = 0\n",
    "\t\tfor i in numbers:\n",
    "\t\t\ts += i\n",
    "\t\treturn s\n",
    "\n",
    "\n",
    "\n",
    "class Agent:\n",
    "\tdef __init__(self, env):\n",
    "\t\tself.env = env\n",
    "\t\tself.all_states = env.get_all_states()\n",
    "\t\tself.utilities = {state:0 for state in self.all_states}\n",
    "\n",
    "\tdef giveup_reward(self, numbers_left):\n",
    "\t\t# The reward for choosing give up at this state\n",
    "\t\tc = self.env.total_numbers\n",
    "\t\treturn c*(c+1)//2 - self.env.calc_sum(numbers_left)\n",
    "\n",
    "\tdef value_iteration(self):\n",
    "\t\t\tmax_change = 1e5\n",
    "\t\t\twhile max_change >= 0.001:\n",
    "\t\t\t\tutilities_pre = self.utilities.copy() # Copy the utility, e.g. U_{t-1}\n",
    "\t\t\t\tmax_change = 0 # Measure the maximum change in all states for this iteration - if smaller than 0.001 we stop.\n",
    "\t\t\t\tfor state in self.all_states:\n",
    "\t\t\t\t\tu_list=[]\n",
    "\t\t\t\t\tnumbers_left=state[0]\n",
    "\t\t\t\t\tfor action in self.env.available_actions(state):\n",
    "\t\t\t\t\t\tu=0\n",
    "\t\t\t\t\t\tfor x in self.env.all_transition_next(numbers_left, action):\n",
    "\t\t\t\t\t\t\tstate_prime=x[0]\n",
    "\t\t\t\t\t\t\tprob=x[1]\n",
    "\t\t\t\t\t\t\tu += prob * (utilities_pre[state_prime])\n",
    "\t\t\t\t\t\tu_list.append(u)\n",
    "\t\t\t\t\tif len(self.env.available_actions(state))==0:\n",
    "\t\t\t\t\t\tu_list.append(self.giveup_reward(numbers_left))\n",
    "\t\t\t\t\tu=max(u_list)\n",
    "\t\t\t\t\tmax_change = max(max_change, u-self.utilities[state])\n",
    "\t\t\t\t\tself.utilities[state]=u\n",
    "\n",
    "\tdef policy(self, state):\n",
    "\t\tpossible_actions = self.env.available_actions(state)\n",
    "\t\tnumbers_left, dice_summation = state\n",
    "\t\t# Initialize with give up directly\n",
    "\t\tmax_utility = self.giveup_reward(numbers_left)  # =0\n",
    "\t\tbest_action = []\n",
    "\t\tfor action in possible_actions:\n",
    "\t\t\t# Finish this part - Find the best action with utility computed in value iteration\n",
    "\t\t\tu=0\n",
    "\t\t\tfor x in self.env.all_transition_next(numbers_left, action):\n",
    "\t\t\t\tstate_prime=x[0]\n",
    "\t\t\t\tprob=x[1]\n",
    "\t\t\t\tu += prob * (self.utilities[state_prime])\n",
    "\t\t\tif self.utilities[state] == u:\n",
    "\t\t\t\tbest_action.append(action)\n",
    "\t\t\t#max_utility = max(max_utility , u)\n",
    "\t\treturn best_action\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "\tenv = Environment()\n",
    "\t# Try the following commands before coding\n",
    "\t#print(env.available_actions(State([1,2,3,4,5,6,7,8,9], 12)))\n",
    "\t#print(env.all_transition_next([1,2,3,4,5,6,7,8,9], [1,2]))\n",
    " \n",
    "\tagent = Agent(env)\n",
    "\t# Q1: Complete the Value iteration code here!\n",
    "\tagent.value_iteration()\n",
    "\tprint('Utility of [1,2,3,4,5,6,7,8,9], 12: %.3f' % agent.utilities[State([1,2,3,4,5,6,7,8,9], 12)])\n",
    "\t#print('Utility of [1,3,4,5,6,7,8,9], 12: %.3f' % agent.utilities[State([1,3,4,5,6,7,8,9], 12)])\n",
    "\t#print('Utility of [1,3,5,6,7,8,9], 12: %.3f' % agent.utilities[State([1,3,5,6,7,8,9], 12)])\n",
    "\t\n",
    "\t# Q2: Complete policy function and run the code here!\n",
    "\tprint('Optimal action of [1,2,3,4,5,6,7,8,9], 12: %s' % str(agent.policy(State([1,2,3,4,5,6,7,8,9], 12))))\n",
    "\tprint('Optimal action of [1,3,4,5,6,7,8,9], 12: %s' % str(agent.policy(State([1,3,4,5,6,7,8,9], 12))))\n",
    "\tprint('Optimal action of [1,3,5,6,7,8,9], 12: %s' % str(agent.policy(State([1,3,5,6,7,8,9], 12))))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#for day in range(1,5):\n",
    "for day in [7,6,5,4,3,2,1]:\n",
    "    start=(datetime.date.today() - datetime.timedelta(day+4)).strftime('%Y-%m-%d')\n",
    "    end=(datetime.date.today() - datetime.timedelta(day)).strftime('%Y-%m-%d') \n",
    "    df=af.Bibliometrics_Collect(start,end)\n",
    "    df.fillna('', inplace=True)\n",
    "    df.to_csv('database/basedb.csv', mode='a', index=False,header=False, encoding='utf-8-sig')\n",
    "    print('done.',start,':',end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 试一下arxiv的api\n",
    "看了。感觉还是不行。。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. 用bill的来match author\n",
    "\n",
    "nickname middle lastname  #3 NN-NMI \n",
    " 加一下如果有两个middle name且比较长"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 版本1. 只适用于pubmed。 -> 可以改成从指定的index开始循环"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def authormatch_pub(df):\n",
    "    authors=pd.read_csv('database/pubmed api author.csv', encoding='utf-8-sig')\n",
    " \n",
    "    # get Biohub author names/data\n",
    "\n",
    "    def ORCID_format (orcid):\n",
    "        try:\n",
    "            find_orcid = re.search (r\"([0-9]{4})-?([0-9]{4})-?([0-9]{4})-?([0-9]{3}[0-9X])\", orcid)\n",
    "        except:\n",
    "            find_orcid = False\n",
    "        if find_orcid:\n",
    "            formatted_orcid = \"https://orcid.org/\"+find_orcid.group(1)+\"-\"+find_orcid.group(2)+\"-\"+find_orcid.group(3)+\"-\"+find_orcid.group(4)\n",
    "        else:\n",
    "            formatted_orcid = \"nan\"\n",
    "        return (formatted_orcid)\n",
    "\n",
    "    def strip_accents(text):\n",
    "        try:\n",
    "            text = unicode(text, 'utf-8')\n",
    "        except NameError: # unicode is a default on python 3 \n",
    "            pass\n",
    "\n",
    "        text = unicodedata.normalize('NFD', text)\\\n",
    "            .encode('ascii', 'ignore')\\\n",
    "            .decode(\"utf-8\")\n",
    "\n",
    "        return (str(text))\n",
    "\n",
    "    def text_field_set_null_to_blank (text): \n",
    "        if isinstance(text,str):\n",
    "            return (text)\n",
    "        else:\n",
    "            return (\"\")\n",
    "        \n",
    "\n",
    "    condition = set() # for scoring Biohub authors\n",
    "\n",
    "    Email_address_found = set()\n",
    "\n",
    "    BiohubAuthors_df = pd.read_excel('database/Biohub authors.xlsx',\n",
    "                                    dtype = { \n",
    "                                        'Middle' : str,\n",
    "                                        'ORCID' : str,\n",
    "                                        'Cohort' : str,\n",
    "                                        'Email-Preferred' : str,\n",
    "                                        'Email 2' : str},\n",
    "                                    converters = { \n",
    "                                        'Ambiguous initials' : lambda x: np.where(x == True, True, False),\n",
    "                                        'Ambiguous incomplete full' : lambda x: np.where(x == True, True, False),\n",
    "                                    })\n",
    "\n",
    "    BiohubAuthors_df['ORCID'] = BiohubAuthors_df['ORCID'].apply(ORCID_format)\n",
    "\n",
    "    BiohubAuthors_df['MatchName'] = BiohubAuthors_df['MatchName'].apply(strip_accents)\n",
    "    BiohubAuthors_df['Last Name'] = BiohubAuthors_df['Last Name'].apply(strip_accents)\n",
    "    BiohubAuthors_df['First Name'] = BiohubAuthors_df['First Name'].apply(strip_accents)\n",
    "    BiohubAuthors_df['Nickname'] = BiohubAuthors_df['Nickname'].apply(strip_accents)\n",
    "\n",
    "    BiohubAuthors_df['Email 2'] = BiohubAuthors_df['Email 2'].apply(text_field_set_null_to_blank)\n",
    "\n",
    "    BiohubAuthors_df['Length of award'] = BiohubAuthors_df['Length of award'].fillna(0).astype(int)\n",
    "\n",
    "    BiohubAuthors_df['Award start date'] = BiohubAuthors_df['Award start date'].dt.date\n",
    "    BiohubAuthors_df['Award end date'] = BiohubAuthors_df['Award end date'].dt.date\n",
    "\n",
    "    BiohubAuthors_list = BiohubAuthors_df.values.tolist()\n",
    "    BiohubAuthors_columns = BiohubAuthors_df.columns.tolist()\n",
    "\n",
    "\n",
    "    List_MatchNames = BiohubAuthors_df['MatchName'].unique().tolist()\n",
    "    biohub_authors = {} # column names are indexed in BiohubAuthors_columns.index(\"column name\")\n",
    "    biohub_authors_ORCID = {}\n",
    "    biohub_authors_email = {}\n",
    "    biohub_authors_variations = {} \n",
    "    biohub_authors_awarddates = {}\n",
    "\n",
    "    name_match_weight = {\n",
    "        \"FN-NMI\" : 3, # \"FN-NMI\" : first name-no middle initial\n",
    "        \"NN-NMI\" : 3, # \"NN-NMI\" : Nickname-no middle initial\n",
    "        \"FN-MN\" : 3,  # \"FN-MN\" : first name-middle name\n",
    "        \"FI-MN\" : 3,  # \"FI-MN\" : first initial-middle name-(when preferred)\n",
    "        \"FN-MI\" : 3,  # \"FN-MI\" : first name-middle initial\n",
    "        \"FN\" : 2,     # \"FN\" : first name, omitting middle initial\n",
    "        \"NN\" : 2,     # \"NN\" : Nickname-omitting middle initial\n",
    "        \"FI-MI\" : 1,  # \"FI-MI\" : first initial-middle initial\n",
    "        \"FI-NMI\" : 1, # \"FI-NMI\" : first initial-no middle initial\n",
    "        \"FI\" : 0      # \"FI\" : first initial-omitting middle initial\n",
    "    }\n",
    "\n",
    "    for i in range (len(BiohubAuthors_df)):\n",
    "        if not BiohubAuthors_df.loc[i]['MatchName'] in biohub_authors_awarddates:\n",
    "            biohub_authors_awarddates[BiohubAuthors_df.loc[i]['MatchName']] = []\n",
    "            if  BiohubAuthors_df.loc[i]['Award start date'] ==  BiohubAuthors_df.loc[i]['Award start date']: # check for null\n",
    "                biohub_authors_awarddates[BiohubAuthors_df.loc[i]['MatchName']].append(BiohubAuthors_df.loc[i]['Award start date'])\n",
    "                biohub_authors_awarddates[BiohubAuthors_df.loc[i]['MatchName']].append(BiohubAuthors_df.loc[i]['Award end date'])\n",
    "            else:\n",
    "                biohub_authors_awarddates[BiohubAuthors_df.loc[i]['MatchName']].append(datetime.date(2000, 1, 1))\n",
    "                biohub_authors_awarddates[BiohubAuthors_df.loc[i]['MatchName']].append(datetime.date(3000, 1, 1))\n",
    "\n",
    "\n",
    "    for row in BiohubAuthors_list:\n",
    "        MatchName = row[BiohubAuthors_columns.index(\"MatchName\")]\n",
    "        biohub_authors[MatchName] = row\n",
    "            \n",
    "        LastName = row[BiohubAuthors_columns.index(\"Last Name\")].lower()\n",
    "        FirstName = row[BiohubAuthors_columns.index(\"First Name\")].lower()\n",
    "        find_bracket = FirstName.find(\"[\") # brackets used to indicate use of first initial as alternate to first name: \"J[ames]\"\n",
    "        if find_bracket != -1:\n",
    "            FirstName = FirstName.replace(\"[\",\"\").replace(\"]\",\"\")\n",
    "        Nickname = row[BiohubAuthors_columns.index(\"Nickname\")].lower()\n",
    "        Middle = row[BiohubAuthors_columns.index(\"Middle\")].lower()\n",
    "\n",
    "        EntryName = LastName+\", \"+FirstName\n",
    "        EntryFI = LastName+\", \"+FirstName[0:1]\n",
    "        if Middle == \"nmi\":\n",
    "            biohub_authors_variations[EntryName] = [MatchName, \"FN-NMI\"] # first name-no middle initial\n",
    "            biohub_authors_variations[EntryFI] = [MatchName, \"FI-NMI\"] # first initial-no middle initial\n",
    "            if Nickname != FirstName:\n",
    "                EntryName = LastName+\", \"+Nickname\n",
    "                biohub_authors_variations[EntryName] = [MatchName, \"NN-NMI\"] # Nickname-no middle initial\n",
    "        else:\n",
    "            biohub_authors_variations[EntryName] = [MatchName, \"FN\"] # first name, omitting middle initial\n",
    "            biohub_authors_variations[EntryFI] = [MatchName, \"FI\"] # first initial-omitting middle initial\n",
    "            if Nickname != FirstName:\n",
    "                EntryName = LastName+\", \"+Nickname\n",
    "                biohub_authors_variations[EntryName] = [MatchName, \"NN\"] # Nickname-omitting middle initial\n",
    "            if len(Middle) > 1:\n",
    "                EntryName = LastName+\", \"+FirstName+\" \"+Middle\n",
    "                biohub_authors_variations[EntryName] = [MatchName, \"FN-MN\"] # first name-middle name\n",
    "            if find_bracket != -1: \n",
    "                EntryName = EntryFI+\" \"+Middle\n",
    "                biohub_authors_variations[EntryName] = [MatchName, \"FI-MN\"] # first initial-middle name-preferred\n",
    "            EntryName = LastName+\", \"+FirstName+\" \"+Middle[0:1]\n",
    "            biohub_authors_variations[EntryName] = [MatchName, \"FN-MI\"] # first name-middle initial\n",
    "            EntryName = EntryFI+\" \"+Middle[0:1]\n",
    "            if EntryName not in biohub_authors_variations:\n",
    "                biohub_authors_variations[EntryName] = [MatchName, \"FI-MI\"] # first initial-middle initial\n",
    "        \n",
    "        email_list = re.findall (r\"[a-zA-Z][a-zA-Z0-9_.-]*@[a-zA-Z][a-zA-Z0-9_.-]*\", row[BiohubAuthors_columns.index(\"Email-Preferred\")]+\" \"+row[BiohubAuthors_columns.index(\"Email 2\")])\n",
    "        for item in email_list:\n",
    "            item = item.lower()\n",
    "            biohub_authors_email[item] = row[BiohubAuthors_columns.index(\"MatchName\")]\n",
    "            \n",
    "        if row[BiohubAuthors_columns.index(\"ORCID\")] != 'nan':\n",
    "            biohub_authors_ORCID[row[BiohubAuthors_columns.index(\"ORCID\")]] = row[BiohubAuthors_columns.index(\"MatchName\")]\n",
    "\n",
    "    biohub_authors_variations_full = {} # includes compressed versions of names, omitting spaces, dashes, apostrophes, etc   \n",
    "\n",
    "    for key,value in biohub_authors_variations.items():\n",
    "        biohub_authors_variations_full[key] = value\n",
    "        compress = key.replace(\" \",\"\").replace(\"-\",\"\").replace(\"\\'\",\"\").replace(\",\",\", \")\n",
    "        if key[-2:-1] == \" \" and key[-3:-2] != \",\":\n",
    "            compress = compress[:-1]+key[-2:] # restore the penultimate space if there is one\n",
    "        if compress != key:\n",
    "            biohub_authors_variations_full[compress] = value\n",
    "\n",
    "\n",
    "    # author_fields notes:\n",
    "    #    \"ForeName\" includes middle initials\n",
    "    #    \"Initials\" includes first name initial\n",
    "    #    \"Email\" field may include email addresses for all authors with listed email addresses, not just the current author\n",
    "    #    \"BiohubAuthor\" = \"investigator\", \"intramural\" (group/platform leader, etc)\n",
    "    #    \"MatchName\" = uniform version of matched name for investigator or group leader, used for joining tables\n",
    "    #    \"MatchType\" = Abbreviations for first name, first initial, NMI, etc. in biohub_authors_variations_full\n",
    "    #    \"TrustMatch\" = Yes, Maybe, No - based on MatchType (if match based on initials) and if there are matching\n",
    "    #                   or mis-matching ORCID IDs, email addresses, or campus affiliations\n",
    "    #    \"OrcidMatch\" = True if record ORCID matches with Biohub author ORCID; False only if there are values for both that don't match\n",
    "    #    \"EmailMatch\" = True if record email matches with Biohub author email\n",
    "    #                       - ignore if there is a mis-match: Affiliation records for an author occasionally include \n",
    "    #                         email addresses for co-authors\n",
    "    #    \"AffiliationMatch\" = True if record Affiliation includes a match to Biohub author campus affiliation\n",
    "    #    \"Biohub\" -> \"Chan Zuckerberg Biohub\" or variants found in affiliation or email address\n",
    "    #    \"Biohub-Funding\" -> \"Chan Zuckerberg Biohub\" or variants found in grant list\n",
    "    authors.drop_duplicates(subset=authors.columns[0:10],inplace=True)\n",
    "    author_fields = ['AuthorNo', 'pmid', 'name', 'ORCID', 'LastName', 'ForeName', 'Initials',\n",
    "                    'affiliation', 'ISEmail', 'ISBiohub author','Suffix',\n",
    "                    'BiohubAuthor',\n",
    "                    'MatchName',\n",
    "                    'TrustMatch', \n",
    "                    'MatchType',\n",
    "                    'OrcidMatch',\n",
    "                    'EmailMatch',\n",
    "                    'AffiliationMatch',\n",
    "                    'Biohub', \n",
    "                    'Biohub-Funding', \n",
    "                    'Stanford',\n",
    "                    'UCSF',\n",
    "                    'Berkeley',\n",
    "                    'Email',  \n",
    "                    'EqualContrib']\n",
    "\n",
    "    for col in author_fields:\n",
    "        if col not in authors.columns.to_list():\n",
    "            authors[col]=''\n",
    "    authors = authors[author_fields]\n",
    "    authors.fillna('', inplace=True)\n",
    "\n",
    "\n",
    "    # score for Biohub authorship\n",
    "\n",
    "    for i in range(len(authors)):\n",
    "        Name = strip_accents(authors.loc[i,\"LastName\"].lower()+\", \"+authors.loc[i,\"ForeName\"].lower())\n",
    "        MatchName = \"\"\n",
    "        MatchType = \"\"\n",
    "        if Name in biohub_authors_variations_full:\n",
    "            MatchName = biohub_authors_variations_full[Name][0]\n",
    "            MatchType = biohub_authors_variations_full[Name][1] \n",
    "        else:\n",
    "            compress = Name.replace(\" \",\"\").replace(\"-\",\"\").replace(\"\\'\",\"\").replace(\",\",\", \")\n",
    "            if Name[-2:-1] == \" \" and Name[-3:-2] != \",\":\n",
    "                compress = compress[:-1]+Name[-2:] # restore the penultimate space if there is one\n",
    "            if compress != Name and compress in biohub_authors_variations_full:\n",
    "                MatchName = biohub_authors_variations_full[compress][0]\n",
    "                MatchType = biohub_authors_variations_full[compress][1] \n",
    "            elif len(authors.loc[i,'ForeName'])>=2:\n",
    "                if authors.loc[i,'ForeName'][-2] != \" \" and authors.loc[i,'ForeName'].find(\" \") != -1:\n",
    "                    # if the full middle name is given in the Pubmed record, which may not be present in the Biohub author record\n",
    "                    forename = authors.loc[i,'ForeName'][:authors.loc[i,'ForeName'].find(\" \")+2]\n",
    "                    Name2 = strip_accents(authors.loc[i,'LastName'].lower()+\", \"+forename.lower())\n",
    "                    if Name2 in biohub_authors_variations_full:\n",
    "                        MatchName = biohub_authors_variations_full[Name2][0]\n",
    "                        MatchType = biohub_authors_variations_full[Name2][1] \n",
    "                        print (\"Found full middle name for Biohub author\",Name,\"PMID\",authors.loc[i,'pmid'])\n",
    "                    else:\n",
    "                        compress = Name2.replace(\" \",\"\").replace(\"-\",\"\").replace(\"\\'\",\"\").replace(\",\",\", \")\n",
    "                        if Name2[-2:-1] == \" \" and Name[-3:-2] != \",\":\n",
    "                            compress = compress[:-1]+Name[-2:] # restore the penultimate space if there is one\n",
    "                        if compress != Name2:\n",
    "                            if compress in biohub_authors_variations_full:\n",
    "                                MatchName = biohub_authors_variations_full[compress][0]\n",
    "                                MatchType = biohub_authors_variations_full[compress][1] \n",
    "                                print (\"Found full middle name for Biohub author\",Name,\"PMID\",authors.loc[i,'pmid'])\n",
    "                    \n",
    "                # ToDo - backup if there wasn't a match of the author name: \n",
    "                # else: \n",
    "                #       if the author record has an ORCID ID, try searching for it in the Biohub Author ORCID ID dictionary\n",
    "                #       and alert user if there is a match; ditto if there is an email address\n",
    "                \n",
    "        if MatchName != \"\":\n",
    "            authors.loc[i,'MatchName'] = MatchName\n",
    "            authors.loc[i,\"MatchType\"] = MatchType\n",
    "            authors.loc[i,\"BiohubAuthor\"] = biohub_authors[MatchName][BiohubAuthors_columns.index(\"Role\")]\n",
    "            authors.loc[i,\"AffiliationMatch\"] =  authors.iloc[i,author_fields.index(biohub_authors[MatchName][BiohubAuthors_columns.index(\"Campus (simple)\")])]\n",
    "            if authors.loc[i,\"AffiliationMatch\"] != True:\n",
    "                if len(authors.loc[i,\"BiohubAuthor\"]) > 0:\n",
    "                    if authors.loc[i,\"Biohub\"] == True:\n",
    "                        authors.loc[i,\"AffiliationMatch\"] = True \n",
    "            orcid_from_record = authors.loc[i,\"ORCID\"]\n",
    "            orcid_from_biohub_authors = biohub_authors[MatchName][BiohubAuthors_columns.index(\"ORCID\")]\n",
    "            if orcid_from_record == orcid_from_biohub_authors:\n",
    "                authors.loc[i,\"OrcidMatch\"] = True\n",
    "            elif len(orcid_from_record) > 0:\n",
    "                if orcid_from_biohub_authors == \"nan\":\n",
    "                    print (\"ORCID ID found for \\\"\"+MatchName+\"\\\" in Pubmed\",authors.loc[i,'pmid'],\":\\n\",orcid_from_record,\"\\n\")\n",
    "                else:\n",
    "                    authors.loc[i,\"OrcidMatch\"] = False\n",
    "                    print (\"Mismatched ORCID ID found for \\\"\"+MatchName+\"\\\" in Pubmed\",authors.loc[i,'pmid'],\"\\n  ORCID ID in publication:\",orcid_from_record,\"\\n  ORCID ID from \\\"Biohub authors.xlsx\\\" file:\",orcid_from_biohub_authors,\"\\n\")\n",
    "            if str(authors.loc[i,'pmid'])+\" \"+authors.loc[i,'MatchName'] in Email_address_found:\n",
    "                authors.loc[i,'EmailMatch'] = True\n",
    "            \n",
    "            # set TrustMatch:\n",
    "            if authors.loc[i,\"OrcidMatch\"] or authors.loc[i,'EmailMatch'] or authors.loc[i,\"Biohub\"]:\n",
    "                authors.loc[i,\"TrustMatch\"] = \"Yes\"\n",
    "                condition.add(1)\n",
    "            elif authors.loc[i,\"AffiliationMatch\"] and name_match_weight[MatchType] >= 2:\n",
    "                authors.loc[i,\"TrustMatch\"] = \"Yes\"\n",
    "                condition.add(2)\n",
    "            elif authors.loc[i,\"AffiliationMatch\"] != True and name_match_weight[MatchType] >= 2:\n",
    "                if biohub_authors[MatchName][BiohubAuthors_columns.index(\"Ambiguous incomplete full\")] == True and name_match_weight[MatchType] == 2:\n",
    "                    authors.loc[i,'TrustMatch'] = \"Maybe\"\n",
    "                    condition.add(3)\n",
    "                else:\n",
    "                    authors.loc[i,'TrustMatch'] = \"Yes\"\n",
    "                    condition.add(4)\n",
    "            elif authors.loc[i,'AffiliationMatch'] and name_match_weight[MatchType] == 1 and biohub_authors[MatchName][BiohubAuthors_columns.index(\"Ambiguous initials\")] == False:\n",
    "                authors.loc[i,'TrustMatch'] = \"Yes\"\n",
    "                condition.add(5)\n",
    "            elif authors.loc[i,'AffiliationMatch'] and name_match_weight[MatchType] == 0 and biohub_authors[MatchName][BiohubAuthors_columns.index(\"Ambiguous initials\")] == False:\n",
    "                authors.loc[i,'TrustMatch'] = \"Maybe\"\n",
    "                condition.add(6)\n",
    "                \n",
    "            elif authors.loc[i,'AffiliationMatch'] and name_match_weight[MatchType] == 1 and biohub_authors[MatchName][BiohubAuthors_columns.index(\"Ambiguous initials\")] == True:\n",
    "                authors.loc[i,'TrustMatch'] = \"Maybe\"\n",
    "                condition.add(7)\n",
    "            elif authors.loc[i,'AffiliationMatch'] and name_match_weight[MatchType] == 0 and biohub_authors[MatchName][BiohubAuthors_columns.index(\"Ambiguous initials\")] == True:\n",
    "                authors.loc[i,'TrustMatch'] = \"No\"\n",
    "                condition.add(8)\n",
    "                \n",
    "            elif authors.loc[i,'AffiliationMatch'] != True and name_match_weight[MatchType] == 1 and biohub_authors[MatchName][BiohubAuthors_columns.index(\"Ambiguous initials\")] == False:\n",
    "                authors.loc[i,'TrustMatch'] = \"Maybe\"\n",
    "                condition.add(9)\n",
    "            elif authors.loc[i,'AffiliationMatch'] != True and name_match_weight[MatchType] == 0 and biohub_authors[MatchName][BiohubAuthors_columns.index(\"Ambiguous initials\")] == False:\n",
    "                authors.loc[i,'TrustMatch'] = \"No\"\n",
    "                condition.add(10)\n",
    "            elif authors.loc[i,'AffiliationMatch'] != True and name_match_weight[MatchType] <= 1 and biohub_authors[MatchName][BiohubAuthors_columns.index(\"Ambiguous initials\")] == True:\n",
    "                authors.loc[i,'TrustMatch'] = \"No\"\n",
    "                condition.add(11)\n",
    "            else:\n",
    "                print (\"TrustMatch error - didn't match any conditions, Author-PMID\",Name,authors[i][author_fields.index(\"PMID\")])\n",
    "                print (authors.loc[i,'AffiliationMatch'],name_match_weight[MatchType], biohub_authors[MatchName][BiohubAuthors_columns.index(\"Ambiguous initials\")])\n",
    "\n",
    "            # Suffix match - \n",
    "            # for now, no Biohub authors have a suffix; when one does, we'll have to create a field for it; for now:\n",
    "            \n",
    "            if len(authors.loc[i,'Suffix'])>0:\n",
    "                authors.loc[i,'TrustMatch'] = \"No\"\n",
    "\n",
    "    authors.to_csv('database/pubmed api author.csv',index=False, encoding='utf-8-sig')\t\t    \n",
    "    \n",
    "    \n",
    "    # save them to dataframe.\n",
    "    for ind,i in enumerate(authors['pmid']):\n",
    "        if authors.loc[ind,'TrustMatch']=='Yes':\n",
    "            if authors.loc[ind,'MatchName'] not in df.loc[df[df['pmid']==authors.loc[ind,'pmid']].index,'biohub author'].str.lower().replace(', ',' '):\n",
    "                df.loc[df[df['pmid']==authors.loc[ind,'pmid']].index,'biohub author'] += authors.loc[ind,'MatchName']\n",
    "        elif authors.loc[ind,'TrustMatch']=='Maybe':\n",
    "            df.loc[df[df['pmid']==authors.loc[ind,'pmid']].index,'possible biohub author'] += authors.loc[ind,'MatchName']\n",
    "    \n",
    "    return df\n",
    "\n",
    "authormatch_pub(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.版本2.用于preprint的\n",
    "\n",
    "除了分号以外的都去掉。比如. , -\n",
    "\n",
    "if the author record has an ORCID ID, try searching for it in the Biohub Author ORCID ID dictionary\n",
    "and alert user if there is a match; ditto if there is an email address\n",
    "\n",
    "\n",
    "经测试。没有last name=nickname的. middle也没有空的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def authormatch_pre(df):\n",
    "\n",
    "    def strip_accents(text):\n",
    "        try:\n",
    "            text = unicode(text, 'utf-8')\n",
    "        except NameError: # unicode is a default on python 3 \n",
    "            pass\n",
    "\n",
    "        text = unicodedata.normalize('NFD', text)\\\n",
    "            .encode('ascii', 'ignore')\\\n",
    "            .decode(\"utf-8\")\n",
    "\n",
    "        return (str(text))\n",
    "\n",
    "    standard=pd.read_excel('database/Biohub authors.xlsx',\n",
    "                        dtype = { \n",
    "                            'Middle' : str,\n",
    "                            'ORCID' : str,\n",
    "                            'Cohort' : str,\n",
    "                            'Email-Preferred' : str,\n",
    "                            'Email 2' : str},\n",
    "                        converters = { \n",
    "                            'Ambiguous initials' : lambda x: np.where(x == True, True, False),\n",
    "                            'Ambiguous incomplete full' : lambda x: np.where(x == True, True, False),\n",
    "                        })\n",
    "    standard.dropna(how='all', axis=1,inplace=True)\t\n",
    "    for i in standard.columns[2:7]:\n",
    "        standard[i]=standard[i].apply(strip_accents).str.lower().replace(\"[\",\"\").replace(\"]\",\"\")\n",
    "\n",
    "    #use a new df to store\n",
    "    test=standard.iloc[:,2:7]  #后面可以加上2:9, & email相等的话\n",
    "\n",
    "    from itertools import permutations \n",
    "\n",
    "    for ind,i in enumerate(test['Middle']):\n",
    "        combines_3=[]\n",
    "        combines_2=[]\n",
    "        combines_1=[]\n",
    "        combines_0=[]\n",
    "        \n",
    "        if i=='nmi': # no middle name\n",
    "            # \"FN-NMI\" : 3  # firstname lastname  & Lastname firstname\n",
    "            for result in permutations(test.iloc[ind,1:3], 2):\n",
    "                combines_3.append(\" \".join(result))\n",
    "            \n",
    "            # FI-NMI  # firstname[0] lastname  & lastname firstname[0]\n",
    "            for result in permutations([test.loc[ind,'First Name'][0],test.loc[ind,'Last Name']], 2):\n",
    "                combines_1.append(\" \".join(result))\n",
    "            \n",
    "            # #   ？      # firstname lastname[0]  & Lastname[0] firstname\n",
    "            # for result in permutations([test.loc[ind,'First Name'],test.loc[ind,'Last Name'][0]], 2):\n",
    "            #     combines.append(\" \".join(result))\n",
    "            \n",
    "            if (test.loc[ind,'Nickname'] != test.loc[ind,'First Name']):\n",
    "                # NN-NMI\n",
    "                for result in permutations([test.loc[ind,'Nickname'],test.loc[ind,'Last Name']], 2):\n",
    "                    combines_3.append(\" \".join(result))\n",
    "                    \n",
    "                # for result in permutations([test.loc[ind,'Nickname'],test.loc[ind,'Last Name'][0]], 2):\n",
    "                #     combines.append(\" \".join(result))\n",
    "                        \n",
    "        else:\n",
    "            # FN \n",
    "            for result in permutations(test.iloc[ind,1:3], 2):\n",
    "                combines_2.append(\" \".join(result))\n",
    "                \n",
    "            # FI-NMI  # firstname[0] lastname  & lastname firstname[0]\n",
    "            for result in permutations([test.loc[ind,'First Name'][0],test.loc[ind,'Last Name']], 2):\n",
    "                combines_0.append(\" \".join(result))\n",
    "            \n",
    "            # FN-MN\"            \n",
    "            combines_3.append(test.loc[ind,'First Name']+' '+test.loc[ind,'Middle'][0]+' '+test.loc[ind,'Last Name'])\n",
    "            combines_3.append(test.loc[ind,'Last Name']+' '+test.loc[ind,'Middle'][0]+' '+test.loc[ind,'First Name'])\n",
    "            \n",
    "            # FN-MI\n",
    "            combines_3.append(test.loc[ind,'Last Name']+' '+test.loc[ind,'First Name']+' '+test.loc[ind,'Middle'][0])\n",
    "            \n",
    "            # FI-MI\n",
    "            combines_1.append(test.loc[ind,'Last Name']+' '+test.loc[ind,'First Name'][0]+' '+test.loc[ind,'Middle'][0])\n",
    "            \n",
    "            if (test.loc[ind,'Nickname'] != test.loc[ind,'First Name']):\n",
    "                # NN\n",
    "                for result in permutations([test.loc[ind,'Nickname'],test.loc[ind,'Last Name']], 2):\n",
    "                    combines_2.append(\" \".join(result))\n",
    "                \n",
    "            if len(test['Middle'])>1:\n",
    "                # FN-MN\"\n",
    "                combines_3.append(test.loc[ind,'First Name']+' '+test.loc[ind,'Middle']+' '+test.loc[ind,'Last Name'])\n",
    "                combines_3.append(test.loc[ind,'Last Name']+' '+test.loc[ind,'Middle']+' '+test.loc[ind,'First Name'])\n",
    "                \n",
    "                # FI-MN\n",
    "                combines_3.append(test.loc[ind,'Last Name']+' '+test.loc[ind,'First Name'][0]+' '+test.loc[ind,'Middle'])\n",
    "                \n",
    "        test.loc[ind,'combination3']='; '.join(combines_3)\n",
    "        test.loc[ind,'combination2']='; '.join(combines_2)\n",
    "        test.loc[ind,'combination1']='; '.join(combines_1)\n",
    "        test.loc[ind,'combination0']='; '.join(combines_0)\n",
    "\n",
    "    test['combination_all']= test['combination0'] + test['combination1'] + test['combination2'] + test['combination3']\n",
    "    test['combination_23']= test['combination2'] + test['combination3']\n",
    "    test['combination_01']= test['combination0'] + test['combination1'] \n",
    "\n",
    "\n",
    "    df.fillna('', inplace=True)\n",
    "    preprint_list=['biorxiv','bioRxiv','medrxiv','medRxiv','arxiv','arXiv']\n",
    "    pre_ind=df[df['journal'].isin(preprint_list)].index\n",
    "    for ind,i in enumerate(df.loc[pre_ind,'authors']):\n",
    "        try:\n",
    "            i=i.split(';')\n",
    "        except:\n",
    "            i=i.split(',')\n",
    "\n",
    "        yes_name_list=list()\n",
    "        maybe_name_list=list()\n",
    "        \n",
    "        for j in i:  # j is every single author name\n",
    "            j=re.sub(r'[^\\w]', ' ', j.strip('#'))\n",
    "            j=j.replace('  ',' ').strip(' ').replace('-','').lower()\n",
    "\n",
    "            for ind2 in test.index:\n",
    "                if j in test.loc[ind2,'combination_01']:\n",
    "                    x=standard.loc[ind2,'MatchName']\n",
    "                    maybe_name_list.append(x)\n",
    "                \n",
    "                if j in test.loc[ind2,'combination_23']:\n",
    "                    x=standard.loc[ind2,'MatchName']\n",
    "                    yes_name_list.append(x)\n",
    "                    \n",
    "        df.loc[ind,'possible biohub author']='; '.join(maybe_name_list)\n",
    "        df.loc[ind,'biohub author']='; '.join(yes_name_list)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_accents(text):\n",
    "    try:\n",
    "        text = unicode(text, 'utf-8')\n",
    "    except NameError: # unicode is a default on python 3 \n",
    "        pass\n",
    "\n",
    "    text = unicodedata.normalize('NFD', text)\\\n",
    "           .encode('ascii', 'ignore')\\\n",
    "           .decode(\"utf-8\")\n",
    "\n",
    "    return (str(text))\n",
    "\n",
    "standard=pd.read_excel('database/Biohub authors.xlsx',\n",
    "                    dtype = { \n",
    "                        'Middle' : str,\n",
    "                        'ORCID' : str,\n",
    "                        'Cohort' : str,\n",
    "                        'Email-Preferred' : str,\n",
    "                        'Email 2' : str},\n",
    "                    converters = { \n",
    "                        'Ambiguous initials' : lambda x: np.where(x == True, True, False),\n",
    "                        'Ambiguous incomplete full' : lambda x: np.where(x == True, True, False),\n",
    "                    })\n",
    "standard.dropna(how='all', axis=1,inplace=True)\t\n",
    "for i in standard.columns[2:7]:\n",
    "    standard[i]=standard[i].apply(strip_accents).str.lower().replace(\"[\",\"\").replace(\"]\",\"\")\n",
    "\n",
    "\n",
    "#储存biohub author所有可能的match\n",
    "test=standard.iloc[:,2:7]  #后面可以加上2:9, & email相等的话\n",
    "\n",
    "\n",
    "#经测试。没有last name=nickname的. middle也没有空的\n",
    "\n",
    "from itertools import permutations \n",
    "\n",
    "for ind,i in enumerate(test['Middle']):\n",
    "    combines_3=[]\n",
    "    combines_2=[]\n",
    "    combines_1=[]\n",
    "    combines_0=[]\n",
    "    \n",
    "    if i=='nmi': # no middle name\n",
    "        # \"FN-NMI\" : 3  # firstname lastname  & Lastname firstname\n",
    "        for result in permutations(test.iloc[ind,1:3], 2):\n",
    "            combines_3.append(\" \".join(result))\n",
    "        \n",
    "        # FI-NMI  # firstname[0] lastname  & lastname firstname[0]\n",
    "        for result in permutations([test.loc[ind,'First Name'][0],test.loc[ind,'Last Name']], 2):\n",
    "            combines_1.append(\" \".join(result))\n",
    "        \n",
    "        # #   ？      # firstname lastname[0]  & Lastname[0] firstname\n",
    "        # for result in permutations([test.loc[ind,'First Name'],test.loc[ind,'Last Name'][0]], 2):\n",
    "        #     combines.append(\" \".join(result))\n",
    "        \n",
    "        if (test.loc[ind,'Nickname'] != test.loc[ind,'First Name']):\n",
    "            # NN-NMI\n",
    "            for result in permutations([test.loc[ind,'Nickname'],test.loc[ind,'Last Name']], 2):\n",
    "                combines_3.append(\" \".join(result))\n",
    "                \n",
    "            # for result in permutations([test.loc[ind,'Nickname'],test.loc[ind,'Last Name'][0]], 2):\n",
    "            #     combines.append(\" \".join(result))\n",
    "                       \n",
    "    else:\n",
    "        # FN \n",
    "        for result in permutations(test.iloc[ind,1:3], 2):\n",
    "            combines_2.append(\" \".join(result))\n",
    "            \n",
    "        # FI-NMI  # firstname[0] lastname  & lastname firstname[0]\n",
    "        for result in permutations([test.loc[ind,'First Name'][0],test.loc[ind,'Last Name']], 2):\n",
    "            combines_0.append(\" \".join(result))\n",
    "        \n",
    "        # FN-MN\"            \n",
    "        combines_3.append(test.loc[ind,'First Name']+' '+test.loc[ind,'Middle'][0]+' '+test.loc[ind,'Last Name'])\n",
    "        combines_3.append(test.loc[ind,'Last Name']+' '+test.loc[ind,'Middle'][0]+' '+test.loc[ind,'First Name'])\n",
    "        \n",
    "        # FN-MI\n",
    "        combines_3.append(test.loc[ind,'Last Name']+' '+test.loc[ind,'First Name']+' '+test.loc[ind,'Middle'][0])\n",
    "        \n",
    "        # FI-MI\n",
    "        combines_1.append(test.loc[ind,'Last Name']+' '+test.loc[ind,'First Name'][0]+' '+test.loc[ind,'Middle'][0])\n",
    "        \n",
    "        if (test.loc[ind,'Nickname'] != test.loc[ind,'First Name']):\n",
    "            # NN\n",
    "            for result in permutations([test.loc[ind,'Nickname'],test.loc[ind,'Last Name']], 2):\n",
    "                combines_2.append(\" \".join(result))\n",
    "             \n",
    "        if len(test['Middle'])>1:\n",
    "            # FN-MN\"\n",
    "            combines_3.append(test.loc[ind,'First Name']+' '+test.loc[ind,'Middle']+' '+test.loc[ind,'Last Name'])\n",
    "            combines_3.append(test.loc[ind,'Last Name']+' '+test.loc[ind,'Middle']+' '+test.loc[ind,'First Name'])\n",
    "            \n",
    "            # FI-MN\n",
    "            combines_3.append(test.loc[ind,'Last Name']+' '+test.loc[ind,'First Name'][0]+' '+test.loc[ind,'Middle'])\n",
    "            \n",
    "    test.loc[ind,'combination3']='; '.join(combines_3)\n",
    "    test.loc[ind,'combination2']='; '.join(combines_2)\n",
    "    test.loc[ind,'combination1']='; '.join(combines_1)\n",
    "    test.loc[ind,'combination0']='; '.join(combines_0)\n",
    "\n",
    "test['combination_all']= test['combination0'] + test['combination1'] + test['combination2'] + test['combination3']\n",
    "test['combination_23']= test['combination2'] + test['combination3']\n",
    "test['combination_01']= test['combination0'] + test['combination1'] \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('database/basedb.csv', encoding='utf-8-sig')\n",
    "df.fillna('', inplace=True)\n",
    "\n",
    "preprint_list=['biorxiv','bioRxiv','medrxiv','medRxiv','arxiv','arXiv']\n",
    "pre_ind=df[df['journal'].isin(preprint_list)].index\n",
    "for ind,i in enumerate(df.loc[pre_ind,'authors']):\n",
    "    try:\n",
    "        i=i.split(';')\n",
    "    except:\n",
    "        i=i.split(',')\n",
    "\n",
    "    yes_name_list=list()\n",
    "    maybe_name_list=list()\n",
    "    \n",
    "    for j in i:  # j is every single author name\n",
    "        j=re.sub(r'[^\\w]', ' ', j.strip('#'))\n",
    "        j=j.replace('  ',' ').strip(' ').replace('-','').lower()\n",
    "\n",
    "        #mat_col=['combination3','combination2','combination1','combination0']\n",
    "        for ind2 in test.index:\n",
    "            if j in test.loc[ind2,'combination_01']:\n",
    "                x=standard.loc[ind2,'MatchName']\n",
    "                maybe_name_list.append(x)\n",
    "            \n",
    "            if j in test.loc[ind2,'combination_23']:\n",
    "                x=standard.loc[ind2,'MatchName']\n",
    "                yes_name_list.append(x)\n",
    "                \n",
    "    df.loc[ind,'possible biohub author']='; '.join(maybe_name_list)\n",
    "    df.loc[ind,'biohub author']='; '.join(yes_name_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Pubmed_search_author(start_date,end_date):\n",
    "    # start_date=end_date\n",
    "    author=pd.read_excel('database/Biohub authors.xlsx')\n",
    "    author_list=author['Last Name']+', '+author['First Name']+ '[FAU]'\n",
    "    df=pd.DataFrame()\n",
    "\n",
    "    for term in author_list:\n",
    "        try:\n",
    "            dt=af.Pubmed_search2(start_date, end_date,TERM=term,save_AuthorInfo=False)\n",
    "            dt['biohub author']=term.replace('[FAU]','')\n",
    "            if isinstance(dt, pd.DataFrame):\n",
    "                df=pd.concat([df,dt],ignore_index=True)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    m=list(df.columns)\n",
    "    m.remove('biohub author')\n",
    "    df=df.groupby(m)['biohub author'].apply('; '.join).reset_index()      #df2.groupby('pmid', as_index=False).agg(sum)\n",
    "    df=df.drop_duplicates(subset='pmid', keep=\"last\") # Q: how to drop duplicates then combine 'biohub author column?'\n",
    "    return df\n",
    "\n",
    "end=(datetime.date.today() - datetime.timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "df4 = Pubmed_search_author(start_date=end,end_date=end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "改进一下 \n",
    "1. middle name search\n",
    "2. 作者的对应机构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10.report\n",
    "put this in a new page. add side bar.\n",
    "create a iterative process.  \n",
    "\n",
    "\n",
    "+ reference\n",
    "\n",
    "## 1. condition: time period\n",
    "- check recent report example\n",
    "just list publication are by intramural author (only from biohub_author.csv. people only from campus simple = biohub)\n",
    "\n",
    "list publication & summary statics\n",
    "\n",
    "## 2. preprint compliance report  (around 28min'\n",
    "number + percentage\n",
    "only for inverstagtor. organized by inverstagtor MatchName.\n",
    "\n",
    "statistic is :\n",
    "    how many publication paper they have , \n",
    "    how many publication paper they have they are coressbding author \n",
    "    how many publication paper's publication type -> preprint policy apply ( they are research article not a review)\n",
    "    how many they have preprint & how many they don't\n",
    "    how many preprint they have \\ percentage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprint policy\n",
    "1. 出版类型。\n",
    "   1. pubmed种有记录 eg  review article 有review这一术语\n",
    "   2. 有部分的journal只发表review或者别的我们的政策 not apply to的 （check txt file。 这个文件包括全名和缩写。如果一些文章在这些期刊上发表，那么我们的政策不适用\n",
    "       (~df['journal'].isin(review_list))\n",
    "\n",
    "2. 当我们运行并生成报告时，我们需要的不仅是\n",
    "   1. 摘要统计数据 + 一份详细的报告，     【【一个是出版物列表，另一个是摘要统计】】\n",
    "   2. 可以逐个作者查看第一作者，然后逐个记录，\n",
    "   3. 对于在首选策略中被称为违规的已发表论文，只需手动管理它们。然后他们可能会说，你知道，当我看记录时，我会说，哦，好吧，出于某种原因，这项政策不适用于此。然后勾选一个字段，表示存在异常。这是个例外。\n",
    "      1. 对于报告作者栏：只展示前三个作者，其余改成et al  title (year) 但是不重要！后面在搞。先留个函数的框\n",
    "\n",
    "\n",
    "## 详细报告：\n",
    "第一行： 加粗的 通讯作者\n",
    "\n",
    "Author (Year) Title. Journal| Volume|: Pages|. DOI|. PMID: PMID/Accession Number. History: Original Publication|.\n",
    "\n",
    "List of authors (up to three): LastName, FirstName, Middle Initial ... if there are more than N, than first author only followed by et al. (Year) Title. Journal| Volume|: Pages|. DOI|. PMID: PMID/Accession Number. History: Original Publication|.\n",
    "\n",
    "Journal = abbreviated version of journal name\n",
    "\n",
    "\n",
    "# \n",
    "\n",
    "preprint compliance report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import all_function as af\n",
    "import pandas as pd\n",
    "\n",
    "start='2022-7-01'\n",
    "end='2022-10-30'\n",
    "\n",
    "# 1. Select author list \n",
    "author=pd.read_excel('database/Biohub authors.xlsx')  #  biohub author\n",
    "\n",
    "# 需要确定下\n",
    "intramural=author[author['Campus (simple)'] == 'Biohub']\n",
    "investigator=author.loc[author['Role'] == 'Investigator']\n",
    "\n",
    "#author.loc[((author['Campus (simple)'] == 'Biohub') & (author['Role'] == 'Investigator')]\n",
    "df_a=author.loc[(author['Campus (simple)'] == 'Biohub') | (author['Role'] == 'Investigator') ]\n",
    "\n",
    "\n",
    "# 2. Choose publication & prerpint within specific time period  # 指定时间内的pub & pre\n",
    "df = pd.read_csv('database/basedb.csv', encoding='utf-8-sig')\n",
    "df.fillna('', inplace=True)\n",
    "#df['date'] = df['date'].str.strip('/6/0').str.split(' ')[0]\n",
    "#df=af.transfer_date_format(df)\n",
    "\n",
    "\n",
    "# 2.1 Exclude 'review list'\n",
    "with open('database/list of review journals.txt') as file:\n",
    "    review_list = [line.rstrip() for line in file]\n",
    "\n",
    "df=df[~df['journal'].isin(review_list)]\n",
    "\n",
    "# 2.2 divided into two categories: publication and preprint\n",
    "#得把date格式改一下然后。换成date\n",
    "preprint_list=['biorxiv','bioRxiv','medrxiv','medRxiv','arxiv','arXiv']\n",
    "preprint = df[df['journal'].isin(preprint_list)]\n",
    "publication = df[~df['journal'].isin(preprint_list)]\n",
    "\n",
    "df['epost date'] = pd.to_datetime(df['epost date'])  \n",
    "\n",
    "pre_condition = (df['epost date'] >= start) & (df['epost date'] <= end) & (df['journal'].isin(preprint_list))\n",
    "op1_pre=df.loc[pre_condition]\n",
    "\n",
    "pub_condition = (df['epost date'] >= start) & (df['epost date'] <= end) & (~df['journal'].isin(preprint_list))\n",
    "op1_pub=df.loc[pub_condition]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "op1=\"\"\"\n",
    "Biohub intramural research program – Papers published and preprints first-deposited\n",
    "{start} ~ {end}:\n",
    "Includes papers, conference proceedings, and preprints published or first-deposited since the last Biohub All-Hands meeting that cite Biohub affiliation or funding and that include a Biohub employee or close, non-investigator affiliate as a co-author (we may easily have missed something, so please feel free to send Bill Burkholder any additions or corrections)\n",
    "Papers (Research articles, methods papers, reviews, etc.) and conference proceedings:\\n\n",
    "\"\"\"\n",
    "\n",
    "print(sub(op1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "\n",
    "d = Document()\n",
    "d.add_heading('Biohub intramural research program – Papers published and preprints first-deposited\\n ', level=1)\n",
    "\n",
    "\n",
    "paragraph = d.add_paragraph()\n",
    "paragraph.add_run('dolor').bold\n",
    "paragraph.add_run(' and some ')\n",
    "\n",
    "run = paragraph.add_run('dolor22')\n",
    "run.bold = True\n",
    "\n",
    "d.save('docx_file.docx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "run.bold = True\n",
    "document.add_heading('Biohub intramural research program – Papers published and preprints first-deposited\\n ', level=1)\n",
    "\n",
    "\\033[1mBiohub intramural research program – Papers published and preprints first-deposited\n",
    "\\033[1m{start} ~ {end}:\n",
    "\\033[0mIncludes papers, conference proceedings, and preprints published or first-deposited since the last Biohub All-Hands meeting that cite Biohub affiliation or funding and that include a Biohub employee or close, non-investigator affiliate as a co-author (we may easily have missed something, so please feel free to send Bill Burkholder any additions or corrections)\n",
    "\\n\\033[1mPapers (Research articles, methods papers, reviews, etc.) and conference proceedings:\\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 Create detail report\n",
    "# 输出word\n",
    "\n",
    "\n",
    "\n",
    "## '\\033[1m' bold following text \n",
    "## '\\033[0m' turn to normal\n",
    "import sys\n",
    "class safesub(dict):\n",
    "    def __missing__(self, key):\n",
    "        return '{' + key + '}'\n",
    "def sub(text):\n",
    "    return text.format_map(safesub(sys._getframe(1).f_locals))\n",
    "\n",
    "# 3.1.0 heading\n",
    "op1=\"\"\"\n",
    "\\033[1mBiohub intramural research program – Papers published and preprints first-deposited\n",
    "\\033[1m{start} ~ {end}:\n",
    "\\033[0mIncludes papers, conference proceedings, and preprints published or first-deposited since the last Biohub All-Hands meeting that cite Biohub affiliation or funding and that include a Biohub employee or close, non-investigator affiliate as a co-author (we may easily have missed something, so please feel free to send Bill Burkholder any additions or corrections)\n",
    "\\n\\033[1mPapers (Research articles, methods papers, reviews, etc.) and conference proceedings:\\n\n",
    "\"\"\"\n",
    "\n",
    "# 3.1.1 publication part\n",
    "for ind,row in op1_pub.iterrows():\n",
    "    op1 +='\\033[1m'+row['biohub author']+row['possible biohub author']+'\\n'+'\\033[0m   '+row['title']+str(row['pmid'])+'\\n\\n'\n",
    "\n",
    "# 3.1.2 preprint part\n",
    "op1+='\\033[1m'+\"Preprints:\\n\"\n",
    "\n",
    "for ind,row in op1_pre.iterrows():\n",
    "    op1+='\\033[1m'+row['biohub author']+row['possible biohub author']+'\\n'+'\\033[0m   '+row['title']+str(row['pmid'])+'\\n\\n'\n",
    "\n",
    "print(sub(op1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CREATE DF report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "             Biohub staff authors    All authors\n",
    "Papers (Research articles, methods papers, reviews, etc.) and conference proceedings\n",
    "Preprints\n",
    "Total\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_au=df_a['MatchName'].str.replace(',','').values.tolist()\n",
    "\n",
    "df[df['biohub author'].isin(b_au)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=pd.DataFrame(columns =['Biohub staff authors', 'All authors'],index = ['Papers (Research articles, methods papers, reviews, etc.) and conference proceedings', 'Preprints', 'Total'])\n",
    "\n",
    "df2.iloc[0,1]=op1_pub.shape[0]\n",
    "df2.iloc[1,1]=op1_pre.shape[0]\n",
    "df2.iloc[2,1]=df2.iloc[0,1]+df2.iloc[1,1]\n",
    "\n",
    "df2.iloc[0,0]=op1_pub[(op1_pub['biohub author']!='') | (op1_pub['possible biohub author']!='')].shape[0]\n",
    "df2.iloc[1,0]=op1_pre[(op1_pre['biohub author']!='') | (op1_pre['possible biohub author']!='')].shape[0]\n",
    "df2.iloc[2,0]=df2.iloc[0,0]+df2.iloc[1,0]\n",
    "\n",
    "\n",
    "print(\"Papers published and preprints first-deposited from\",start,'to',end)\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 表3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3=df.iloc[df[df['b_au']!=''].index,:]\n",
    "df3['b_au'] = df3['b_au'].map(lambda x:x.split(', '))  # Q: split by '; '\n",
    "df3=df3.explode('b_au')\n",
    "t=pd.DataFrame(df3.groupby('b_au')['record id'].count()).rename(columns={'record id':'Compliance'}) \n",
    "t.index.name = None\n",
    "#t=t[t.index.isin(author['MatchName'])]\n",
    "\n",
    "p3=pd.DataFrame(columns =['Total articles as corresponding author','Qualifying articles as corresponding author','Qualifying articles as corresponding author with preprints','Compliance'],index = author['MatchName'])\n",
    "p3.index.name = None\n",
    "p3['Compliance'].fillna(t['Compliance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preprint policy 标准:\n",
    "    publication type\n",
    "        1 pubmed: review article\n",
    "            /PublicationTypeList\n",
    "            if publication belong to this journal（看bill发的文件）, we can say the preprint policy is not apply\n",
    "        2 personal curation\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Markdown2docx import Markdown2docx\n",
    "project = Markdown2docx('Report')\n",
    "project.eat_soup()\n",
    "project.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "author file里的格式是 last_name, first_name\n",
    "\n",
    "首先。需要是每一行中存在就继续、所以不能用isin 得用循环\n",
    "\n",
    "其次。match name的大小写、中间是否有逗号得统一\n",
    "'biohub author','possible biohub author','corresponding author'\n",
    "\n",
    "现在是全 matchname格式只是小写了而已"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=af.authormatch_pre(df)\n",
    "df=af.authormatch_pub(df)\n",
    "df.to_csv('database/basedb.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start='2022-11-1'\n",
    "end='2022-11-10'\n",
    "keyword='biohub'\n",
    "df3 = af.Pubmed_search2(start_date=start, end_date=end,TERM='(zuckerb* AND biohub) OR \"cz biohub\" OR \"czi biohub\"',save_AuthorInfo=True)\n",
    "df4 = af.Pubmed_search_author(start_date=end,end_date=end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors=pd.read_csv('database/pubmed api author.csv',encoding='utf-8-sig')\t\t\n",
    "df = pd.read_csv('database/basedb.csv', encoding='utf-8-sig')      \n",
    "authors\n",
    "\n",
    "\n",
    "# save them to dataframe.\n",
    "df['format biohub author']=''\n",
    "for ind,i in enumerate(authors['pmid']):\n",
    "    if authors.loc[ind,'TrustMatch']=='Yes':\n",
    "        #if authors.loc[ind,'MatchName'] not in df.loc[df[df['pmid']==authors.loc[ind,'pmid']].index,'biohub author'].str.lower().replace(', ',' '):\n",
    "        df.loc[df[df['pmid']==authors.loc[ind,'pmid']].index,'format biohub author'] += '; '+authors.loc[ind,'MatchName']\n",
    "    if authors.loc[ind,'TrustMatch']=='Maybe':\n",
    "        df.loc[df[df['pmid']==authors.loc[ind,'pmid']].index,'possible biohub author'] += '; '+authors.loc[ind,'MatchName']\n",
    "df['format biohub author']=df['format biohub author'].str.strip('; ')\n",
    "set(df['format biohub author'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors.loc[ind,'pmid']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind=3\n",
    "authors.loc[ind,'MatchName']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import all_function as af #import the module here, so that it can be reloaded.\n",
    "importlib.reload(af)\n",
    "df6,dfa=af.Pubmed_search2(start_date='2022-11-1',\n",
    "                 end_date='2022-11-12',TERM='biohub',save_AuthorInfo=False)\n",
    "\n",
    "# df1=af.BioMedrxiv_Search2(start_date='2022-11-1',\n",
    "#                 end_date='2022-11-10',\n",
    "#                 keyword='biohub')\n",
    "\n",
    "set(af.authormatch_pub(df6)['format biohub author'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "report 增加filter。根据选择可以返回不同的内容\n",
    "    Campus (simple)\tCampus Role\tTeam\tStatus\n",
    "    \n",
    "    \n",
    "Tab2:\n",
    "biohub staff author: Campus (simple)=='Biohub'\n",
    "    \n",
    "all author: biohub csv里的\n",
    "\n",
    "Tab3:\n",
    "    review article is not Qualifying \n",
    "\n",
    "1. Qualifying articles as corresponding author: \tnot review paper\n",
    "2. Qualifying articles as corresponding author with preprints:\t\n",
    "3. Compliance: percentage %.\n",
    "   \n",
    "    8 paper = corresponding author & qualifying\n",
    "        none perprint  Compliance=0\n",
    "        all have preprint Compliance=100\n",
    "        4 have preprint Compliance=50\n",
    "\n",
    "\n",
    "#['Campus (simple)']=='Biohub'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('database/basedb.csv', encoding='utf-8-sig')\n",
    "df.fillna('', inplace=True)\n",
    "author=pd.read_csv('database/Biohub authors.csv', encoding='utf-8-sig')  #  biohub author\n",
    "\n",
    "filter_author=author['MatchName'].str.replace(',','').to_list()\n",
    "\n",
    "\n",
    "# 2.1 Exclude 'review list'\n",
    "with open('database/list of review journals.txt') as file:\n",
    "    review_list = [line.rstrip() for line in file]\n",
    "\n",
    "df=df[~df['journal'].isin(review_list)]\n",
    "df['b_au']=df[['biohub author','possible biohub author','corresponding author','format biohub author']].agg('; '.join, axis=1).str.replace('; ; ','; ').str.strip('; ')\n",
    "\n",
    "ind_list=[]\n",
    "for ind,i in enumerate(df['b_au']):\n",
    "    for j in i.split(';'): \n",
    "        print(j)\n",
    "        if j.strip(' ').replace(', ',' ') in filter_author:\n",
    "            print(j)\n",
    "            ind_list.append(ind)\n",
    "\n",
    "print(ind_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['b_au']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Charles R Langelier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. 加两个按钮来输出报告 & report\n",
    "\n",
    "start="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 8. 加登录界面。只有特定的人才能修改。\n",
    "\teg。 只能用biohub的邮箱注册"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. author的csv也得保证更新是那种的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "把 award date放进author csv\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加citation - 失败。bill说先搁置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# author match找到missing的之后可以把他replace一下呢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# debug： 只有delete db部分有问题。他变成全delete了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11.24 FILTER [Done]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11.26 report数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 加一个方法。输入名字，输出改好的名字\n",
    "\n",
    "如何把方法改成：\n",
    "def (input=名字)\n",
    "    return format名字，可能等级Yes/No\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "简单版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard=pd.read_excel('database/Biohub authors.xlsx',\n",
    "                    dtype = { \n",
    "                        'Middle' : str,\n",
    "                        'ORCID' : str,\n",
    "                        'Cohort' : str,\n",
    "                        'Email-Preferred' : str,\n",
    "                        'Email 2' : str},\n",
    "                    converters = { \n",
    "                        'Ambiguous initials' : lambda x: np.where(x == True, True, False),\n",
    "                        'Ambiguous incomplete full' : lambda x: np.where(x == True, True, False),\n",
    "                    })\n",
    "standard.dropna(how='all', axis=1,inplace=True)\t\n",
    "for i in standard.columns[3:7]:\n",
    "    standard[i]=standard[i].apply(strip_accents).str.lower().str.replace(\"[\",\"\").str.replace(\"]\",\"\")\n",
    "\n",
    "#use a new df to store\n",
    "test=standard.iloc[:,2:7]  \n",
    "\n",
    "for ind,i in enumerate(test['Middle']):\n",
    "    combines_3=[]\n",
    "    combines_2=[]\n",
    "    combines_1=[]\n",
    "    combines_0=[]\n",
    "    \n",
    "    if i=='nmi': # no middle name\n",
    "        # \"FN-NMI\" : 3  # firstname lastname  & Lastname firstname\n",
    "        for result in permutations(test.iloc[ind,1:3], 2):\n",
    "            combines_3.append(\" \".join(result))\n",
    "        \n",
    "        # FI-NMI  # firstname[0] lastname  & lastname firstname[0]\n",
    "        for result in permutations([test.loc[ind,'First Name'][0],test.loc[ind,'Last Name']], 2):\n",
    "            combines_1.append(\" \".join(result))\n",
    "\n",
    "        if (test.loc[ind,'Nickname'] != test.loc[ind,'First Name']):\n",
    "            # NN-NMI\n",
    "            for result in permutations([test.loc[ind,'Nickname'],test.loc[ind,'Last Name']], 2):\n",
    "                combines_3.append(\" \".join(result))\n",
    "                \n",
    "            # for result in permutations([test.loc[ind,'Nickname'],test.loc[ind,'Last Name'][0]], 2):\n",
    "            #     combines.append(\" \".join(result))\n",
    "        \n",
    "    else:\n",
    "        # FN \n",
    "        for result in permutations(test.iloc[ind,1:3], 2):\n",
    "            combines_2.append(\" \".join(result))\n",
    "            \n",
    "        # FI-NMI  # firstname[0] lastname  & lastname firstname[0]\n",
    "        for result in permutations([test.loc[ind,'First Name'][0],test.loc[ind,'Last Name']], 2):\n",
    "            combines_0.append(\" \".join(result))\n",
    "        \n",
    "        # FN-MN\"            \n",
    "        combines_3.append(test.loc[ind,'First Name']+' '+test.loc[ind,'Middle'][0]+' '+test.loc[ind,'Last Name'])\n",
    "        combines_3.append(test.loc[ind,'Last Name']+' '+test.loc[ind,'Middle'][0]+' '+test.loc[ind,'First Name'])\n",
    "        \n",
    "        # FN-MI\n",
    "        combines_3.append(test.loc[ind,'Last Name']+' '+test.loc[ind,'First Name']+' '+test.loc[ind,'Middle'][0])\n",
    "        \n",
    "        # FI-MI\n",
    "        combines_1.append(test.loc[ind,'Last Name']+' '+test.loc[ind,'First Name'][0]+' '+test.loc[ind,'Middle'][0])\n",
    "        \n",
    "        if (test.loc[ind,'Nickname'] != test.loc[ind,'First Name']):\n",
    "            # NN\n",
    "            for result in permutations([test.loc[ind,'Nickname'],test.loc[ind,'Last Name']], 2):\n",
    "                combines_2.append(\" \".join(result))\n",
    "            \n",
    "        if len(test['Middle'])>1:\n",
    "            # FN-MN\"\n",
    "            combines_3.append(test.loc[ind,'First Name']+' '+test.loc[ind,'Middle']+' '+test.loc[ind,'Last Name'])\n",
    "            combines_3.append(test.loc[ind,'Last Name']+' '+test.loc[ind,'Middle']+' '+test.loc[ind,'First Name'])\n",
    "            \n",
    "            # FI-MN\n",
    "            combines_3.append(test.loc[ind,'Last Name']+' '+test.loc[ind,'First Name'][0]+' '+test.loc[ind,'Middle'])\n",
    "            \n",
    "    test.loc[ind,'combination3']='; '.join(combines_3)\n",
    "    test.loc[ind,'combination2']='; '.join(combines_2)\n",
    "    test.loc[ind,'combination1']='; '.join(combines_1)\n",
    "    test.loc[ind,'combination0']='; '.join(combines_0)\n",
    "\n",
    "test['combination_all']= test['combination0'] + test['combination1'] + test['combination2'] + test['combination3']\n",
    "test['combination_23']= test['combination2'] + test['combination3']\n",
    "test['combination_01']= test['combination0'] + test['combination1'] \n",
    "\n",
    "test.to_csv('database/biohub author combination.csv',index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return old_name, format_name,possible percent\n",
    "def authormatch_sim(name):\n",
    "    test=pd.read_csv('database/biohub author combination.csv', encoding='utf-8-sig')\n",
    "    degree='no'\n",
    "    format_name=re.sub(r'[^\\w]', ' ', name.strip('#'))\n",
    "    format_name=format_name.replace('  ',' ').strip(' ').replace('-','').lower()\n",
    "    \n",
    "    for ind2 in test.index:\n",
    "        if format_name in test.loc[ind2,'combination_01']:\n",
    "            format_name=test.loc[ind2,'MatchName']\n",
    "            degree='maybe'\n",
    "        \n",
    "        elif format_name in test.loc[ind2,'combination_23']:\n",
    "            format_name=test.loc[ind2,'MatchName']\n",
    "            degree='yes'\n",
    "\n",
    "    return name,format_name,degree\n",
    "\n",
    "\n",
    "authormatch_sim('yosef n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind,i in enumerate(df.loc[pre_ind,'authors']):\n",
    "    try:\n",
    "        i=i.split(';')\n",
    "        \n",
    "    except:\n",
    "        i=i.split(',')\n",
    "    \n",
    "    for j in i:  \n",
    "        j=re.sub(r'[^\\w]', ' ', j.strip('#'))\n",
    "        j=j.replace('  ',' ').strip(' ').replace('-','').lower()\n",
    "        \n",
    "yosef nir; nir yosef\tn yosef; yosef n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "af.authormatch(name)\n",
    "  for ind in authors['pmid'].index:\n",
    "        if authors.loc[ind,'TrustMatch']=='Yes':\n",
    "            #if authors.loc[ind,'MatchName'] not in df.loc[df[df['pmid']==authors.loc[ind,'pmid']].index,'biohub author'].str.lower().replace(', ',' '):\n",
    "            df.loc[df[df['pmid']==authors.loc[ind,'pmid']].index,'format biohub author'] += '; '+authors.loc[ind,'MatchName']\n",
    "        if authors.loc[ind,'TrustMatch']=='Maybe':\n",
    "            df.loc[df[df['pmid']==authors.loc[ind,'pmid']].index,'possible biohub author'] += '; '+authors.loc[ind,'MatchName']\n",
    "\n",
    "    df['format biohub author']=df['format biohub author'].str.strip('; ')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import all_function as af #import the module here, so that it can be reloaded.\n",
    "importlib.reload(af)\n",
    "#import all_function as af\n",
    "\n",
    "af.authormatch('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name=''\n",
    "test=pd.read_csv('database/biohub author combination.csv', encoding='utf-8-sig')\n",
    "degree='no'\n",
    "format_name=re.sub(r'[^\\w]', ' ', name.strip('#'))\n",
    "format_name=format_name.replace('  ',' ').strip(' ').replace('-','').lower()\n",
    "\n",
    "for ind2 in test.index:\n",
    "    if format_name in test.loc[ind2,'combination_01']:\n",
    "        format_name=test.loc[ind2,'MatchName']\n",
    "        print(format_name,ind2)\n",
    "        degree='maybe'\n",
    "    \n",
    "    elif format_name in test.loc[ind2,'combination_23']:\n",
    "        format_name=test.loc[ind2,'MatchName']\n",
    "        print(format_name,1)\n",
    "        degree='yes'\n",
    "\n",
    "print(format_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name='Nir Yosef'\n",
    "biohub_authors_variations_full = np.load('database/biohub_authors_variations_full.npy',allow_pickle=True).item()\n",
    "name=re.sub(r'[^\\w]', ' ', name.strip('#'))\n",
    "name=name.replace('  ',' ').strip(' ').replace('-','').replace(' ',', ').lower()  # \n",
    "Name = strip_accents(name)\n",
    "print(Name,0)\n",
    "\n",
    "MatchName = \"\"\n",
    "MatchType = \"\"\n",
    "if Name in biohub_authors_variations_full:\n",
    "    print(0)\n",
    "    MatchName = biohub_authors_variations_full[Name][0]\n",
    "    MatchType = biohub_authors_variations_full[Name][1] \n",
    "else:\n",
    "    compress = Name.replace(\" \",\"\").replace(\"-\",\"\").replace(\"\\'\",\"\").replace(\",\",\", \")\n",
    "    if Name[-2:-1] == \" \" and Name[-3:-2] != \",\":\n",
    "        compress = compress[:-1]+Name[-2:] # restore the penultimate space if there is one\n",
    "    if compress != Name and compress in biohub_authors_variations_full:\n",
    "        MatchName = biohub_authors_variations_full[compress][0]\n",
    "        MatchType = biohub_authors_variations_full[compress][1] \n",
    "\n",
    "print(name,MatchName,MatchType)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "af.authormatch(name='Ada S Y Poon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'sarnow, peter'.title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.load('database/biohub_authors_variations_full.npy',allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BiohubAuthors_df = pd.read_excel('database/Biohub authors.xlsx',\n",
    "                                dtype = { \n",
    "                                    'Middle' : str,\n",
    "                                    'ORCID' : str,\n",
    "                                    'Cohort' : str,\n",
    "                                    'Email-Preferred' : str,\n",
    "                                    'Email 2' : str},\n",
    "                                converters = { \n",
    "                                    'Ambiguous initials' : lambda x: np.where(x == True, True, False),\n",
    "                                    'Ambiguous incomplete full' : lambda x: np.where(x == True, True, False),\n",
    "                                })\n",
    "\n",
    "BiohubAuthors_df['MatchName'] = BiohubAuthors_df['MatchName'].apply(strip_accents)\n",
    "BiohubAuthors_df['Last Name'] = BiohubAuthors_df['Last Name'].apply(strip_accents)\n",
    "BiohubAuthors_df['First Name'] = BiohubAuthors_df['First Name'].apply(strip_accents)\n",
    "BiohubAuthors_df['Nickname'] = BiohubAuthors_df['Nickname'].apply(strip_accents)\n",
    "\n",
    "BiohubAuthors_list = BiohubAuthors_df.values.tolist()\n",
    "BiohubAuthors_columns = BiohubAuthors_df.columns.tolist()\n",
    "\n",
    "biohub_authors = {} # column names are indexed in BiohubAuthors_columns.index(\"column name\")\n",
    "biohub_authors_variations = {} \n",
    "biohub_authors_awarddates = {}\n",
    "\n",
    "name_match_weight = {\n",
    "    \"FN-NMI\" : 3, # \"FN-NMI\" : first name-no middle initial\n",
    "    \"NN-NMI\" : 3, # \"NN-NMI\" : Nickname-no middle initial\n",
    "    \"FN-MN\" : 3,  # \"FN-MN\" : first name-middle name\n",
    "    \"FI-MN\" : 3,  # \"FI-MN\" : first initial-middle name-(when preferred)\n",
    "    \"FN-MI\" : 3,  # \"FN-MI\" : first name-middle initial\n",
    "    \"FN\" : 2,     # \"FN\" : first name, omitting middle initial\n",
    "    \"NN\" : 2,     # \"NN\" : Nickname-omitting middle initial\n",
    "    \"FI-MI\" : 1,  # \"FI-MI\" : first initial-middle initial\n",
    "    \"FI-NMI\" : 1, # \"FI-NMI\" : first initial-no middle initial\n",
    "    \"FI\" : 0      # \"FI\" : first initial-omitting middle initial\n",
    "}\n",
    "\n",
    "for row in BiohubAuthors_list:\n",
    "    MatchName = row[BiohubAuthors_columns.index(\"MatchName\")]\n",
    "    biohub_authors[MatchName] = row\n",
    "        \n",
    "    LastName = row[BiohubAuthors_columns.index(\"Last Name\")].lower()\n",
    "    FirstName = row[BiohubAuthors_columns.index(\"First Name\")].lower()\n",
    "    find_bracket = FirstName.find(\"[\") # brackets used to indicate use of first initial as alternate to first name: \"J[ames]\"\n",
    "    if find_bracket != -1:\n",
    "        FirstName = FirstName.replace(\"[\",\"\").replace(\"]\",\"\")\n",
    "    Nickname = row[BiohubAuthors_columns.index(\"Nickname\")].lower()\n",
    "    Middle = row[BiohubAuthors_columns.index(\"Middle\")].lower()\n",
    "\n",
    "    EntryName = LastName+\", \"+FirstName\n",
    "    EntryFI = LastName+\", \"+FirstName[0:1]\n",
    "    if Middle == \"nmi\":\n",
    "        biohub_authors_variations[EntryName] = [MatchName, \"FN-NMI\"] # first name-no middle initial\n",
    "        biohub_authors_variations[EntryFI] = [MatchName, \"FI-NMI\"] # first initial-no middle initial\n",
    "        if Nickname != FirstName:\n",
    "            EntryName = LastName+\", \"+Nickname\n",
    "            biohub_authors_variations[EntryName] = [MatchName, \"NN-NMI\"] # Nickname-no middle initial\n",
    "    else:\n",
    "        biohub_authors_variations[EntryName] = [MatchName, \"FN\"] # first name, omitting middle initial\n",
    "        biohub_authors_variations[EntryFI] = [MatchName, \"FI\"] # first initial-omitting middle initial\n",
    "        if Nickname != FirstName:\n",
    "            EntryName = LastName+\", \"+Nickname\n",
    "            biohub_authors_variations[EntryName] = [MatchName, \"NN\"] # Nickname-omitting middle initial\n",
    "        if len(Middle) > 1:\n",
    "            EntryName = LastName+\", \"+FirstName+\" \"+Middle\n",
    "            biohub_authors_variations[EntryName] = [MatchName, \"FN-MN\"] # first name-middle name\n",
    "        if find_bracket != -1: \n",
    "            EntryName = EntryFI+\" \"+Middle\n",
    "            biohub_authors_variations[EntryName] = [MatchName, \"FI-MN\"] # first initial-middle name-preferred\n",
    "        EntryName = LastName+\", \"+FirstName+\" \"+Middle[0:1]\n",
    "        biohub_authors_variations[EntryName] = [MatchName, \"FN-MI\"] # first name-middle initial\n",
    "        EntryName = EntryFI+\" \"+Middle[0:1]\n",
    "        if EntryName not in biohub_authors_variations:\n",
    "            biohub_authors_variations[EntryName] = [MatchName, \"FI-MI\"] # first initial-middle initial\n",
    "\n",
    "biohub_authors_variations_full = {} # includes compressed versions of names, omitting spaces, dashes, apostrophes, etc   \n",
    "\n",
    "for key,value in biohub_authors_variations.items():\n",
    "    biohub_authors_variations_full[key] = value\n",
    "    compress = key.replace(\" \",\"\").replace(\"-\",\"\").replace(\"\\'\",\"\").replace(\",\",\", \")\n",
    "    if key[-2:-1] == \" \" and key[-3:-2] != \",\":\n",
    "        compress = compress[:-1]+key[-2:] # restore the penultimate space if there is one\n",
    "    if compress != key:\n",
    "        biohub_authors_variations_full[compress] = value\n",
    "\n",
    "np.save('database/biohub_authors_variations_full.npy', biohub_authors_variations_full) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11.27 debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('database/basedb.csv', encoding='utf-8-sig')\n",
    "print(df.shape)\n",
    "\n",
    "start=\"2022-06-01\"\n",
    "end=\"2022-11-27\"\n",
    "df['epost date'] = pd.to_datetime(df['epost date'])  \n",
    "condition = (df['epost date'] >= start) & (df['epost date'] <= end) \n",
    "df=df.loc[condition]\n",
    "df.fillna('', inplace=True)\n",
    "print(df.shape)\n",
    "\n",
    "for ind,i in enumerate(df['corresponding author']):\n",
    "    if i == '':\n",
    "        break\n",
    "    i=i.split(';')\n",
    "    res=[]\n",
    "    for j in i:  \n",
    "        j=re.sub(r'[^\\w]', ' ', j.strip('#'))\n",
    "        j=j.replace('  ',' ').strip(' ').replace('-','').lower()\n",
    "        old_name,new_name,prob=af.authormatch(j)\n",
    "        res.append(new_name)\n",
    "    df.loc[ind,'corresponding author'] = '; '.join(m for m in res)\n",
    "df.fillna('', inplace=True)\n",
    "\n",
    "print(df.shape)\n",
    "df.loc[df['journal']=='',:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total articles as corresponding author</th>\n",
       "      <th>Qualifying articles as corresponding author</th>\n",
       "      <th>Qualifying articles as corresponding author with preprints</th>\n",
       "      <th>Compliance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Banfield, Jill</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fordyce, Polly</th>\n",
       "      <td>16.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>228.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Frost, Adam</th>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1200.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Huber, Greg</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>133.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Marson, Alex</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>200.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pollard, Katie</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Poon, Ada</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Total articles as corresponding author  \\\n",
       "Banfield, Jill                                     2.0   \n",
       "Fordyce, Polly                                    16.0   \n",
       "Frost, Adam                                       12.0   \n",
       "Huber, Greg                                        4.0   \n",
       "Marson, Alex                                       4.0   \n",
       "Pollard, Katie                                     1.0   \n",
       "Poon, Ada                                          4.0   \n",
       "\n",
       "                Qualifying articles as corresponding author  \\\n",
       "Banfield, Jill                                          2.0   \n",
       "Fordyce, Polly                                         16.0   \n",
       "Frost, Adam                                            12.0   \n",
       "Huber, Greg                                             4.0   \n",
       "Marson, Alex                                            4.0   \n",
       "Pollard, Katie                                          1.0   \n",
       "Poon, Ada                                               4.0   \n",
       "\n",
       "                Qualifying articles as corresponding author with preprints  \\\n",
       "Banfield, Jill                                                2.0            \n",
       "Fordyce, Polly                                                7.0            \n",
       "Frost, Adam                                                   1.0            \n",
       "Huber, Greg                                                   3.0            \n",
       "Marson, Alex                                                  2.0            \n",
       "Pollard, Katie                                                1.0            \n",
       "Poon, Ada                                                     4.0            \n",
       "\n",
       "                 Compliance  \n",
       "Banfield, Jill   100.000000  \n",
       "Fordyce, Polly   228.571429  \n",
       "Frost, Adam     1200.000000  \n",
       "Huber, Greg      133.333333  \n",
       "Marson, Alex     200.000000  \n",
       "Pollard, Katie   100.000000  \n",
       "Poon, Ada        100.000000  "
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.fillna('', inplace=True)\n",
    "p3[p3['Qualifying articles as corresponding author with preprints'].notna()]\n",
    "#\n",
    "#df.loc[ (df['corresponding author']!='') & ( df['possible match result']!='' ),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total articles as corresponding author</th>\n",
       "      <th>Qualifying articles as corresponding author</th>\n",
       "      <th>Qualifying articles as corresponding author with preprints</th>\n",
       "      <th>Compliance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Abate, Adam</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Altman, Russ</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Arnaout, Rima</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ashley, Euan</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Banfield, Jill</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Yosef, Nir</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Yu, Bin</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Zhang, Wenjun</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Zou, James</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>de la Zerda, Adam</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>143 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Total articles as corresponding author  \\\n",
       "Abate, Adam                                           NaN   \n",
       "Altman, Russ                                          NaN   \n",
       "Arnaout, Rima                                         NaN   \n",
       "Ashley, Euan                                          NaN   \n",
       "Banfield, Jill                                        2.0   \n",
       "...                                                   ...   \n",
       "Yosef, Nir                                            NaN   \n",
       "Yu, Bin                                               NaN   \n",
       "Zhang, Wenjun                                         NaN   \n",
       "Zou, James                                            NaN   \n",
       "de la Zerda, Adam                                     NaN   \n",
       "\n",
       "                   Qualifying articles as corresponding author  \\\n",
       "Abate, Adam                                                NaN   \n",
       "Altman, Russ                                               NaN   \n",
       "Arnaout, Rima                                              NaN   \n",
       "Ashley, Euan                                               NaN   \n",
       "Banfield, Jill                                             2.0   \n",
       "...                                                        ...   \n",
       "Yosef, Nir                                                 NaN   \n",
       "Yu, Bin                                                    NaN   \n",
       "Zhang, Wenjun                                              NaN   \n",
       "Zou, James                                                 NaN   \n",
       "de la Zerda, Adam                                          NaN   \n",
       "\n",
       "                   Qualifying articles as corresponding author with preprints  \\\n",
       "Abate, Adam                                                      NaN            \n",
       "Altman, Russ                                                     NaN            \n",
       "Arnaout, Rima                                                    NaN            \n",
       "Ashley, Euan                                                     NaN            \n",
       "Banfield, Jill                                                   2.0            \n",
       "...                                                              ...            \n",
       "Yosef, Nir                                                       NaN            \n",
       "Yu, Bin                                                          NaN            \n",
       "Zhang, Wenjun                                                    NaN            \n",
       "Zou, James                                                       NaN            \n",
       "de la Zerda, Adam                                                NaN            \n",
       "\n",
       "                   Compliance  \n",
       "Abate, Adam               NaN  \n",
       "Altman, Russ              NaN  \n",
       "Arnaout, Rima             NaN  \n",
       "Ashley, Euan              NaN  \n",
       "Banfield, Jill          100.0  \n",
       "...                       ...  \n",
       "Yosef, Nir                NaN  \n",
       "Yu, Bin                   NaN  \n",
       "Zhang, Wenjun             NaN  \n",
       "Zou, James                NaN  \n",
       "de la Zerda, Adam         NaN  \n",
       "\n",
       "[143 rows x 4 columns]"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "st.write(df['corresponding author'])\n",
    "\n",
    "感觉possible biohub author','format biohub author 可以删掉。。。\n",
    "format biohub author  如果按名字搜，会出现一些他们发表论文时不属于biohub的情况，不该纳入考虑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        # st.image(\"https://static.streamlit.io/examples/dog.jpg\", width=200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ab8f8ae80fafc6ac129e7fe646a0be18ed2869456ff4c93941a37a28edfebd3f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
