{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import all_function as af\n",
    "#pd.read_excel('database/Biohub authors.xlsx')\n",
    "\n",
    "import re\n",
    "import os\n",
    "import unicodedata\n",
    "# from datetime import date,time\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import datacompy\n",
    "from os.path import exists as file_exists\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q: how to drop duplicates then combine 'biohub author column?'  df3&df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "m=list(df.columns)\n",
    "m.remove('biohub author')\n",
    "\n",
    "\n",
    "df=df.groupby(m)['biohub author'].apply('; '.join).reset_index()      \n",
    "#df2.groupby('pmid', as_index=False).agg(sum)\n",
    "#df=df.drop_duplicates(subset='pmid', keep=\"last\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "start=(datetime.date.today() - datetime.timedelta(days=5)).strftime('%Y-%m-%d')\n",
    "end=(datetime.date.today() -\n",
    "    datetime.timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "Keyword='biohub'\n",
    "df5=af.Bibliometrics_Collect(start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 用来重置base dataset的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start='2022-09-01'\n",
    "# end='2022-09-10'\n",
    "# af.Pubmed_search2(start,end)\n",
    "\n",
    "start_date='2022-9-1'\n",
    "end_date='2022-11-10'\n",
    "\n",
    "import importlib\n",
    "import all_function as af #import the module here, so that it can be reloaded.\n",
    "importlib.reload(af)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# start_date='2022-11-1'\n",
    "# end_date='2022-11-18'\n",
    "\n",
    "import importlib\n",
    "import all_function as af #import the module here, so that it can be reloaded.\n",
    "importlib.reload(af)\n",
    "\n",
    "\n",
    "start='2022-9-01'\n",
    "end='2022-11-10'\n",
    "base=af.Bibliometrics_Collect(start,end)\n",
    "base.fillna('', inplace=True)\n",
    "base.to_csv('database/basedb.csv', encoding='utf-8-sig',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#for day in range(1,5):\n",
    "for day in [7,6,5,4,3,2,1]:\n",
    "    start=(datetime.date.today() - datetime.timedelta(day+4)).strftime('%Y-%m-%d')\n",
    "    end=(datetime.date.today() - datetime.timedelta(day)).strftime('%Y-%m-%d') \n",
    "    df=af.Bibliometrics_Collect(start,end)\n",
    "    df.fillna('', inplace=True)\n",
    "    df.to_csv('database/basedb.csv', mode='a', index=False,header=False, encoding='utf-8-sig')\n",
    "    print('done.',start,':',end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 试一下arxiv的api\n",
    "看了。感觉还是不行。。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. 用bill的来match author\n",
    "\n",
    "nickname middle lastname  #3 NN-NMI \n",
    " 加一下如果有两个middle name且比较长"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 版本1. 只适用于pubmed。 -> 可以改成从指定的index开始循环"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def authormatch_pub(df):\n",
    "    authors=pd.read_csv('database/pubmed api author.csv', encoding='utf-8-sig')\n",
    " \n",
    "    # get Biohub author names/data\n",
    "\n",
    "    def ORCID_format (orcid):\n",
    "        try:\n",
    "            find_orcid = re.search (r\"([0-9]{4})-?([0-9]{4})-?([0-9]{4})-?([0-9]{3}[0-9X])\", orcid)\n",
    "        except:\n",
    "            find_orcid = False\n",
    "        if find_orcid:\n",
    "            formatted_orcid = \"https://orcid.org/\"+find_orcid.group(1)+\"-\"+find_orcid.group(2)+\"-\"+find_orcid.group(3)+\"-\"+find_orcid.group(4)\n",
    "        else:\n",
    "            formatted_orcid = \"nan\"\n",
    "        return (formatted_orcid)\n",
    "\n",
    "    def strip_accents(text):\n",
    "        try:\n",
    "            text = unicode(text, 'utf-8')\n",
    "        except NameError: # unicode is a default on python 3 \n",
    "            pass\n",
    "\n",
    "        text = unicodedata.normalize('NFD', text)\\\n",
    "            .encode('ascii', 'ignore')\\\n",
    "            .decode(\"utf-8\")\n",
    "\n",
    "        return (str(text))\n",
    "\n",
    "    def text_field_set_null_to_blank (text): \n",
    "        if isinstance(text,str):\n",
    "            return (text)\n",
    "        else:\n",
    "            return (\"\")\n",
    "        \n",
    "\n",
    "    condition = set() # for scoring Biohub authors\n",
    "\n",
    "    Email_address_found = set()\n",
    "\n",
    "    BiohubAuthors_df = pd.read_excel('database/Biohub authors.xlsx',\n",
    "                                    dtype = { \n",
    "                                        'Middle' : str,\n",
    "                                        'ORCID' : str,\n",
    "                                        'Cohort' : str,\n",
    "                                        'Email-Preferred' : str,\n",
    "                                        'Email 2' : str},\n",
    "                                    converters = { \n",
    "                                        'Ambiguous initials' : lambda x: np.where(x == True, True, False),\n",
    "                                        'Ambiguous incomplete full' : lambda x: np.where(x == True, True, False),\n",
    "                                    })\n",
    "\n",
    "    BiohubAuthors_df['ORCID'] = BiohubAuthors_df['ORCID'].apply(ORCID_format)\n",
    "\n",
    "    BiohubAuthors_df['MatchName'] = BiohubAuthors_df['MatchName'].apply(strip_accents)\n",
    "    BiohubAuthors_df['Last Name'] = BiohubAuthors_df['Last Name'].apply(strip_accents)\n",
    "    BiohubAuthors_df['First Name'] = BiohubAuthors_df['First Name'].apply(strip_accents)\n",
    "    BiohubAuthors_df['Nickname'] = BiohubAuthors_df['Nickname'].apply(strip_accents)\n",
    "\n",
    "    BiohubAuthors_df['Email 2'] = BiohubAuthors_df['Email 2'].apply(text_field_set_null_to_blank)\n",
    "\n",
    "    BiohubAuthors_df['Length of award'] = BiohubAuthors_df['Length of award'].fillna(0).astype(int)\n",
    "\n",
    "    BiohubAuthors_df['Award start date'] = BiohubAuthors_df['Award start date'].dt.date\n",
    "    BiohubAuthors_df['Award end date'] = BiohubAuthors_df['Award end date'].dt.date\n",
    "\n",
    "    BiohubAuthors_list = BiohubAuthors_df.values.tolist()\n",
    "    BiohubAuthors_columns = BiohubAuthors_df.columns.tolist()\n",
    "\n",
    "\n",
    "    List_MatchNames = BiohubAuthors_df['MatchName'].unique().tolist()\n",
    "    biohub_authors = {} # column names are indexed in BiohubAuthors_columns.index(\"column name\")\n",
    "    biohub_authors_ORCID = {}\n",
    "    biohub_authors_email = {}\n",
    "    biohub_authors_variations = {} \n",
    "    biohub_authors_awarddates = {}\n",
    "\n",
    "    name_match_weight = {\n",
    "        \"FN-NMI\" : 3, # \"FN-NMI\" : first name-no middle initial\n",
    "        \"NN-NMI\" : 3, # \"NN-NMI\" : Nickname-no middle initial\n",
    "        \"FN-MN\" : 3,  # \"FN-MN\" : first name-middle name\n",
    "        \"FI-MN\" : 3,  # \"FI-MN\" : first initial-middle name-(when preferred)\n",
    "        \"FN-MI\" : 3,  # \"FN-MI\" : first name-middle initial\n",
    "        \"FN\" : 2,     # \"FN\" : first name, omitting middle initial\n",
    "        \"NN\" : 2,     # \"NN\" : Nickname-omitting middle initial\n",
    "        \"FI-MI\" : 1,  # \"FI-MI\" : first initial-middle initial\n",
    "        \"FI-NMI\" : 1, # \"FI-NMI\" : first initial-no middle initial\n",
    "        \"FI\" : 0      # \"FI\" : first initial-omitting middle initial\n",
    "    }\n",
    "\n",
    "    for i in range (len(BiohubAuthors_df)):\n",
    "        if not BiohubAuthors_df.loc[i]['MatchName'] in biohub_authors_awarddates:\n",
    "            biohub_authors_awarddates[BiohubAuthors_df.loc[i]['MatchName']] = []\n",
    "            if  BiohubAuthors_df.loc[i]['Award start date'] ==  BiohubAuthors_df.loc[i]['Award start date']: # check for null\n",
    "                biohub_authors_awarddates[BiohubAuthors_df.loc[i]['MatchName']].append(BiohubAuthors_df.loc[i]['Award start date'])\n",
    "                biohub_authors_awarddates[BiohubAuthors_df.loc[i]['MatchName']].append(BiohubAuthors_df.loc[i]['Award end date'])\n",
    "            else:\n",
    "                biohub_authors_awarddates[BiohubAuthors_df.loc[i]['MatchName']].append(datetime.date(2000, 1, 1))\n",
    "                biohub_authors_awarddates[BiohubAuthors_df.loc[i]['MatchName']].append(datetime.date(3000, 1, 1))\n",
    "\n",
    "\n",
    "    for row in BiohubAuthors_list:\n",
    "        MatchName = row[BiohubAuthors_columns.index(\"MatchName\")]\n",
    "        biohub_authors[MatchName] = row\n",
    "            \n",
    "        LastName = row[BiohubAuthors_columns.index(\"Last Name\")].lower()\n",
    "        FirstName = row[BiohubAuthors_columns.index(\"First Name\")].lower()\n",
    "        find_bracket = FirstName.find(\"[\") # brackets used to indicate use of first initial as alternate to first name: \"J[ames]\"\n",
    "        if find_bracket != -1:\n",
    "            FirstName = FirstName.replace(\"[\",\"\").replace(\"]\",\"\")\n",
    "        Nickname = row[BiohubAuthors_columns.index(\"Nickname\")].lower()\n",
    "        Middle = row[BiohubAuthors_columns.index(\"Middle\")].lower()\n",
    "\n",
    "        EntryName = LastName+\", \"+FirstName\n",
    "        EntryFI = LastName+\", \"+FirstName[0:1]\n",
    "        if Middle == \"nmi\":\n",
    "            biohub_authors_variations[EntryName] = [MatchName, \"FN-NMI\"] # first name-no middle initial\n",
    "            biohub_authors_variations[EntryFI] = [MatchName, \"FI-NMI\"] # first initial-no middle initial\n",
    "            if Nickname != FirstName:\n",
    "                EntryName = LastName+\", \"+Nickname\n",
    "                biohub_authors_variations[EntryName] = [MatchName, \"NN-NMI\"] # Nickname-no middle initial\n",
    "        else:\n",
    "            biohub_authors_variations[EntryName] = [MatchName, \"FN\"] # first name, omitting middle initial\n",
    "            biohub_authors_variations[EntryFI] = [MatchName, \"FI\"] # first initial-omitting middle initial\n",
    "            if Nickname != FirstName:\n",
    "                EntryName = LastName+\", \"+Nickname\n",
    "                biohub_authors_variations[EntryName] = [MatchName, \"NN\"] # Nickname-omitting middle initial\n",
    "            if len(Middle) > 1:\n",
    "                EntryName = LastName+\", \"+FirstName+\" \"+Middle\n",
    "                biohub_authors_variations[EntryName] = [MatchName, \"FN-MN\"] # first name-middle name\n",
    "            if find_bracket != -1: \n",
    "                EntryName = EntryFI+\" \"+Middle\n",
    "                biohub_authors_variations[EntryName] = [MatchName, \"FI-MN\"] # first initial-middle name-preferred\n",
    "            EntryName = LastName+\", \"+FirstName+\" \"+Middle[0:1]\n",
    "            biohub_authors_variations[EntryName] = [MatchName, \"FN-MI\"] # first name-middle initial\n",
    "            EntryName = EntryFI+\" \"+Middle[0:1]\n",
    "            if EntryName not in biohub_authors_variations:\n",
    "                biohub_authors_variations[EntryName] = [MatchName, \"FI-MI\"] # first initial-middle initial\n",
    "        \n",
    "        email_list = re.findall (r\"[a-zA-Z][a-zA-Z0-9_.-]*@[a-zA-Z][a-zA-Z0-9_.-]*\", row[BiohubAuthors_columns.index(\"Email-Preferred\")]+\" \"+row[BiohubAuthors_columns.index(\"Email 2\")])\n",
    "        for item in email_list:\n",
    "            item = item.lower()\n",
    "            biohub_authors_email[item] = row[BiohubAuthors_columns.index(\"MatchName\")]\n",
    "            \n",
    "        if row[BiohubAuthors_columns.index(\"ORCID\")] != 'nan':\n",
    "            biohub_authors_ORCID[row[BiohubAuthors_columns.index(\"ORCID\")]] = row[BiohubAuthors_columns.index(\"MatchName\")]\n",
    "\n",
    "    biohub_authors_variations_full = {} # includes compressed versions of names, omitting spaces, dashes, apostrophes, etc   \n",
    "\n",
    "    for key,value in biohub_authors_variations.items():\n",
    "        biohub_authors_variations_full[key] = value\n",
    "        compress = key.replace(\" \",\"\").replace(\"-\",\"\").replace(\"\\'\",\"\").replace(\",\",\", \")\n",
    "        if key[-2:-1] == \" \" and key[-3:-2] != \",\":\n",
    "            compress = compress[:-1]+key[-2:] # restore the penultimate space if there is one\n",
    "        if compress != key:\n",
    "            biohub_authors_variations_full[compress] = value\n",
    "\n",
    "\n",
    "    # author_fields notes:\n",
    "    #    \"ForeName\" includes middle initials\n",
    "    #    \"Initials\" includes first name initial\n",
    "    #    \"Email\" field may include email addresses for all authors with listed email addresses, not just the current author\n",
    "    #    \"BiohubAuthor\" = \"investigator\", \"intramural\" (group/platform leader, etc)\n",
    "    #    \"MatchName\" = uniform version of matched name for investigator or group leader, used for joining tables\n",
    "    #    \"MatchType\" = Abbreviations for first name, first initial, NMI, etc. in biohub_authors_variations_full\n",
    "    #    \"TrustMatch\" = Yes, Maybe, No - based on MatchType (if match based on initials) and if there are matching\n",
    "    #                   or mis-matching ORCID IDs, email addresses, or campus affiliations\n",
    "    #    \"OrcidMatch\" = True if record ORCID matches with Biohub author ORCID; False only if there are values for both that don't match\n",
    "    #    \"EmailMatch\" = True if record email matches with Biohub author email\n",
    "    #                       - ignore if there is a mis-match: Affiliation records for an author occasionally include \n",
    "    #                         email addresses for co-authors\n",
    "    #    \"AffiliationMatch\" = True if record Affiliation includes a match to Biohub author campus affiliation\n",
    "    #    \"Biohub\" -> \"Chan Zuckerberg Biohub\" or variants found in affiliation or email address\n",
    "    #    \"Biohub-Funding\" -> \"Chan Zuckerberg Biohub\" or variants found in grant list\n",
    "    authors.drop_duplicates(subset=authors.columns[0:10],inplace=True)\n",
    "    author_fields = ['AuthorNo', 'pmid', 'name', 'ORCID', 'LastName', 'ForeName', 'Initials',\n",
    "                    'affiliation', 'ISEmail', 'ISBiohub author','Suffix',\n",
    "                    'BiohubAuthor',\n",
    "                    'MatchName',\n",
    "                    'TrustMatch', \n",
    "                    'MatchType',\n",
    "                    'OrcidMatch',\n",
    "                    'EmailMatch',\n",
    "                    'AffiliationMatch',\n",
    "                    'Biohub', \n",
    "                    'Biohub-Funding', \n",
    "                    'Stanford',\n",
    "                    'UCSF',\n",
    "                    'Berkeley',\n",
    "                    'Email',  \n",
    "                    'EqualContrib']\n",
    "\n",
    "    for col in author_fields:\n",
    "        if col not in authors.columns.to_list():\n",
    "            authors[col]=''\n",
    "    authors = authors[author_fields]\n",
    "    authors.fillna('', inplace=True)\n",
    "\n",
    "\n",
    "    # score for Biohub authorship\n",
    "\n",
    "    for i in range(len(authors)):\n",
    "        Name = strip_accents(authors.loc[i,\"LastName\"].lower()+\", \"+authors.loc[i,\"ForeName\"].lower())\n",
    "        MatchName = \"\"\n",
    "        MatchType = \"\"\n",
    "        if Name in biohub_authors_variations_full:\n",
    "            MatchName = biohub_authors_variations_full[Name][0]\n",
    "            MatchType = biohub_authors_variations_full[Name][1] \n",
    "        else:\n",
    "            compress = Name.replace(\" \",\"\").replace(\"-\",\"\").replace(\"\\'\",\"\").replace(\",\",\", \")\n",
    "            if Name[-2:-1] == \" \" and Name[-3:-2] != \",\":\n",
    "                compress = compress[:-1]+Name[-2:] # restore the penultimate space if there is one\n",
    "            if compress != Name and compress in biohub_authors_variations_full:\n",
    "                MatchName = biohub_authors_variations_full[compress][0]\n",
    "                MatchType = biohub_authors_variations_full[compress][1] \n",
    "            elif len(authors.loc[i,'ForeName'])>=2:\n",
    "                if authors.loc[i,'ForeName'][-2] != \" \" and authors.loc[i,'ForeName'].find(\" \") != -1:\n",
    "                    # if the full middle name is given in the Pubmed record, which may not be present in the Biohub author record\n",
    "                    forename = authors.loc[i,'ForeName'][:authors.loc[i,'ForeName'].find(\" \")+2]\n",
    "                    Name2 = strip_accents(authors.loc[i,'LastName'].lower()+\", \"+forename.lower())\n",
    "                    if Name2 in biohub_authors_variations_full:\n",
    "                        MatchName = biohub_authors_variations_full[Name2][0]\n",
    "                        MatchType = biohub_authors_variations_full[Name2][1] \n",
    "                        print (\"Found full middle name for Biohub author\",Name,\"PMID\",authors.loc[i,'pmid'])\n",
    "                    else:\n",
    "                        compress = Name2.replace(\" \",\"\").replace(\"-\",\"\").replace(\"\\'\",\"\").replace(\",\",\", \")\n",
    "                        if Name2[-2:-1] == \" \" and Name[-3:-2] != \",\":\n",
    "                            compress = compress[:-1]+Name[-2:] # restore the penultimate space if there is one\n",
    "                        if compress != Name2:\n",
    "                            if compress in biohub_authors_variations_full:\n",
    "                                MatchName = biohub_authors_variations_full[compress][0]\n",
    "                                MatchType = biohub_authors_variations_full[compress][1] \n",
    "                                print (\"Found full middle name for Biohub author\",Name,\"PMID\",authors.loc[i,'pmid'])\n",
    "                    \n",
    "                # ToDo - backup if there wasn't a match of the author name: \n",
    "                # else: \n",
    "                #       if the author record has an ORCID ID, try searching for it in the Biohub Author ORCID ID dictionary\n",
    "                #       and alert user if there is a match; ditto if there is an email address\n",
    "                \n",
    "        if MatchName != \"\":\n",
    "            authors.loc[i,'MatchName'] = MatchName\n",
    "            authors.loc[i,\"MatchType\"] = MatchType\n",
    "            authors.loc[i,\"BiohubAuthor\"] = biohub_authors[MatchName][BiohubAuthors_columns.index(\"Role\")]\n",
    "            authors.loc[i,\"AffiliationMatch\"] =  authors.iloc[i,author_fields.index(biohub_authors[MatchName][BiohubAuthors_columns.index(\"Campus (simple)\")])]\n",
    "            if authors.loc[i,\"AffiliationMatch\"] != True:\n",
    "                if len(authors.loc[i,\"BiohubAuthor\"]) > 0:\n",
    "                    if authors.loc[i,\"Biohub\"] == True:\n",
    "                        authors.loc[i,\"AffiliationMatch\"] = True \n",
    "            orcid_from_record = authors.loc[i,\"ORCID\"]\n",
    "            orcid_from_biohub_authors = biohub_authors[MatchName][BiohubAuthors_columns.index(\"ORCID\")]\n",
    "            if orcid_from_record == orcid_from_biohub_authors:\n",
    "                authors.loc[i,\"OrcidMatch\"] = True\n",
    "            elif len(orcid_from_record) > 0:\n",
    "                if orcid_from_biohub_authors == \"nan\":\n",
    "                    print (\"ORCID ID found for \\\"\"+MatchName+\"\\\" in Pubmed\",authors.loc[i,'pmid'],\":\\n\",orcid_from_record,\"\\n\")\n",
    "                else:\n",
    "                    authors.loc[i,\"OrcidMatch\"] = False\n",
    "                    print (\"Mismatched ORCID ID found for \\\"\"+MatchName+\"\\\" in Pubmed\",authors.loc[i,'pmid'],\"\\n  ORCID ID in publication:\",orcid_from_record,\"\\n  ORCID ID from \\\"Biohub authors.xlsx\\\" file:\",orcid_from_biohub_authors,\"\\n\")\n",
    "            if str(authors.loc[i,'pmid'])+\" \"+authors.loc[i,'MatchName'] in Email_address_found:\n",
    "                authors.loc[i,'EmailMatch'] = True\n",
    "            \n",
    "            # set TrustMatch:\n",
    "            if authors.loc[i,\"OrcidMatch\"] or authors.loc[i,'EmailMatch'] or authors.loc[i,\"Biohub\"]:\n",
    "                authors.loc[i,\"TrustMatch\"] = \"Yes\"\n",
    "                condition.add(1)\n",
    "            elif authors.loc[i,\"AffiliationMatch\"] and name_match_weight[MatchType] >= 2:\n",
    "                authors.loc[i,\"TrustMatch\"] = \"Yes\"\n",
    "                condition.add(2)\n",
    "            elif authors.loc[i,\"AffiliationMatch\"] != True and name_match_weight[MatchType] >= 2:\n",
    "                if biohub_authors[MatchName][BiohubAuthors_columns.index(\"Ambiguous incomplete full\")] == True and name_match_weight[MatchType] == 2:\n",
    "                    authors.loc[i,'TrustMatch'] = \"Maybe\"\n",
    "                    condition.add(3)\n",
    "                else:\n",
    "                    authors.loc[i,'TrustMatch'] = \"Yes\"\n",
    "                    condition.add(4)\n",
    "            elif authors.loc[i,'AffiliationMatch'] and name_match_weight[MatchType] == 1 and biohub_authors[MatchName][BiohubAuthors_columns.index(\"Ambiguous initials\")] == False:\n",
    "                authors.loc[i,'TrustMatch'] = \"Yes\"\n",
    "                condition.add(5)\n",
    "            elif authors.loc[i,'AffiliationMatch'] and name_match_weight[MatchType] == 0 and biohub_authors[MatchName][BiohubAuthors_columns.index(\"Ambiguous initials\")] == False:\n",
    "                authors.loc[i,'TrustMatch'] = \"Maybe\"\n",
    "                condition.add(6)\n",
    "                \n",
    "            elif authors.loc[i,'AffiliationMatch'] and name_match_weight[MatchType] == 1 and biohub_authors[MatchName][BiohubAuthors_columns.index(\"Ambiguous initials\")] == True:\n",
    "                authors.loc[i,'TrustMatch'] = \"Maybe\"\n",
    "                condition.add(7)\n",
    "            elif authors.loc[i,'AffiliationMatch'] and name_match_weight[MatchType] == 0 and biohub_authors[MatchName][BiohubAuthors_columns.index(\"Ambiguous initials\")] == True:\n",
    "                authors.loc[i,'TrustMatch'] = \"No\"\n",
    "                condition.add(8)\n",
    "                \n",
    "            elif authors.loc[i,'AffiliationMatch'] != True and name_match_weight[MatchType] == 1 and biohub_authors[MatchName][BiohubAuthors_columns.index(\"Ambiguous initials\")] == False:\n",
    "                authors.loc[i,'TrustMatch'] = \"Maybe\"\n",
    "                condition.add(9)\n",
    "            elif authors.loc[i,'AffiliationMatch'] != True and name_match_weight[MatchType] == 0 and biohub_authors[MatchName][BiohubAuthors_columns.index(\"Ambiguous initials\")] == False:\n",
    "                authors.loc[i,'TrustMatch'] = \"No\"\n",
    "                condition.add(10)\n",
    "            elif authors.loc[i,'AffiliationMatch'] != True and name_match_weight[MatchType] <= 1 and biohub_authors[MatchName][BiohubAuthors_columns.index(\"Ambiguous initials\")] == True:\n",
    "                authors.loc[i,'TrustMatch'] = \"No\"\n",
    "                condition.add(11)\n",
    "            else:\n",
    "                print (\"TrustMatch error - didn't match any conditions, Author-PMID\",Name,authors[i][author_fields.index(\"PMID\")])\n",
    "                print (authors.loc[i,'AffiliationMatch'],name_match_weight[MatchType], biohub_authors[MatchName][BiohubAuthors_columns.index(\"Ambiguous initials\")])\n",
    "\n",
    "            # Suffix match - \n",
    "            # for now, no Biohub authors have a suffix; when one does, we'll have to create a field for it; for now:\n",
    "            \n",
    "            if len(authors.loc[i,'Suffix'])>0:\n",
    "                authors.loc[i,'TrustMatch'] = \"No\"\n",
    "\n",
    "    authors.to_csv('database/pubmed api author.csv',index=False, encoding='utf-8-sig')\t\t    \n",
    "    \n",
    "    \n",
    "    # save them to dataframe.\n",
    "    for ind,i in enumerate(authors['pmid']):\n",
    "        if authors.loc[ind,'TrustMatch']=='Yes':\n",
    "            if authors.loc[ind,'MatchName'] not in df.loc[df[df['pmid']==authors.loc[ind,'pmid']].index,'biohub author'].str.lower().replace(', ',' '):\n",
    "                df.loc[df[df['pmid']==authors.loc[ind,'pmid']].index,'biohub author'] += authors.loc[ind,'MatchName']\n",
    "        elif authors.loc[ind,'TrustMatch']=='Maybe':\n",
    "            df.loc[df[df['pmid']==authors.loc[ind,'pmid']].index,'possible biohub author'] += authors.loc[ind,'MatchName']\n",
    "    \n",
    "    return df\n",
    "\n",
    "authormatch_pub(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.版本2.用于preprint的\n",
    "\n",
    "除了分号以外的都去掉。比如. , -\n",
    "\n",
    "if the author record has an ORCID ID, try searching for it in the Biohub Author ORCID ID dictionary\n",
    "and alert user if there is a match; ditto if there is an email address\n",
    "\n",
    "\n",
    "经测试。没有last name=nickname的. middle也没有空的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def authormatch_pre(df):\n",
    "\n",
    "    def strip_accents(text):\n",
    "        try:\n",
    "            text = unicode(text, 'utf-8')\n",
    "        except NameError: # unicode is a default on python 3 \n",
    "            pass\n",
    "\n",
    "        text = unicodedata.normalize('NFD', text)\\\n",
    "            .encode('ascii', 'ignore')\\\n",
    "            .decode(\"utf-8\")\n",
    "\n",
    "        return (str(text))\n",
    "\n",
    "    standard=pd.read_excel('database/Biohub authors.xlsx',\n",
    "                        dtype = { \n",
    "                            'Middle' : str,\n",
    "                            'ORCID' : str,\n",
    "                            'Cohort' : str,\n",
    "                            'Email-Preferred' : str,\n",
    "                            'Email 2' : str},\n",
    "                        converters = { \n",
    "                            'Ambiguous initials' : lambda x: np.where(x == True, True, False),\n",
    "                            'Ambiguous incomplete full' : lambda x: np.where(x == True, True, False),\n",
    "                        })\n",
    "    standard.dropna(how='all', axis=1,inplace=True)\t\n",
    "    for i in standard.columns[2:7]:\n",
    "        standard[i]=standard[i].apply(strip_accents).str.lower().replace(\"[\",\"\").replace(\"]\",\"\")\n",
    "\n",
    "    #use a new df to store\n",
    "    test=standard.iloc[:,2:7]  #后面可以加上2:9, & email相等的话\n",
    "\n",
    "    from itertools import permutations \n",
    "\n",
    "    for ind,i in enumerate(test['Middle']):\n",
    "        combines_3=[]\n",
    "        combines_2=[]\n",
    "        combines_1=[]\n",
    "        combines_0=[]\n",
    "        \n",
    "        if i=='nmi': # no middle name\n",
    "            # \"FN-NMI\" : 3  # firstname lastname  & Lastname firstname\n",
    "            for result in permutations(test.iloc[ind,1:3], 2):\n",
    "                combines_3.append(\" \".join(result))\n",
    "            \n",
    "            # FI-NMI  # firstname[0] lastname  & lastname firstname[0]\n",
    "            for result in permutations([test.loc[ind,'First Name'][0],test.loc[ind,'Last Name']], 2):\n",
    "                combines_1.append(\" \".join(result))\n",
    "            \n",
    "            # #   ？      # firstname lastname[0]  & Lastname[0] firstname\n",
    "            # for result in permutations([test.loc[ind,'First Name'],test.loc[ind,'Last Name'][0]], 2):\n",
    "            #     combines.append(\" \".join(result))\n",
    "            \n",
    "            if (test.loc[ind,'Nickname'] != test.loc[ind,'First Name']):\n",
    "                # NN-NMI\n",
    "                for result in permutations([test.loc[ind,'Nickname'],test.loc[ind,'Last Name']], 2):\n",
    "                    combines_3.append(\" \".join(result))\n",
    "                    \n",
    "                # for result in permutations([test.loc[ind,'Nickname'],test.loc[ind,'Last Name'][0]], 2):\n",
    "                #     combines.append(\" \".join(result))\n",
    "                        \n",
    "        else:\n",
    "            # FN \n",
    "            for result in permutations(test.iloc[ind,1:3], 2):\n",
    "                combines_2.append(\" \".join(result))\n",
    "                \n",
    "            # FI-NMI  # firstname[0] lastname  & lastname firstname[0]\n",
    "            for result in permutations([test.loc[ind,'First Name'][0],test.loc[ind,'Last Name']], 2):\n",
    "                combines_0.append(\" \".join(result))\n",
    "            \n",
    "            # FN-MN\"            \n",
    "            combines_3.append(test.loc[ind,'First Name']+' '+test.loc[ind,'Middle'][0]+' '+test.loc[ind,'Last Name'])\n",
    "            combines_3.append(test.loc[ind,'Last Name']+' '+test.loc[ind,'Middle'][0]+' '+test.loc[ind,'First Name'])\n",
    "            \n",
    "            # FN-MI\n",
    "            combines_3.append(test.loc[ind,'Last Name']+' '+test.loc[ind,'First Name']+' '+test.loc[ind,'Middle'][0])\n",
    "            \n",
    "            # FI-MI\n",
    "            combines_1.append(test.loc[ind,'Last Name']+' '+test.loc[ind,'First Name'][0]+' '+test.loc[ind,'Middle'][0])\n",
    "            \n",
    "            if (test.loc[ind,'Nickname'] != test.loc[ind,'First Name']):\n",
    "                # NN\n",
    "                for result in permutations([test.loc[ind,'Nickname'],test.loc[ind,'Last Name']], 2):\n",
    "                    combines_2.append(\" \".join(result))\n",
    "                \n",
    "            if len(test['Middle'])>1:\n",
    "                # FN-MN\"\n",
    "                combines_3.append(test.loc[ind,'First Name']+' '+test.loc[ind,'Middle']+' '+test.loc[ind,'Last Name'])\n",
    "                combines_3.append(test.loc[ind,'Last Name']+' '+test.loc[ind,'Middle']+' '+test.loc[ind,'First Name'])\n",
    "                \n",
    "                # FI-MN\n",
    "                combines_3.append(test.loc[ind,'Last Name']+' '+test.loc[ind,'First Name'][0]+' '+test.loc[ind,'Middle'])\n",
    "                \n",
    "        test.loc[ind,'combination3']='; '.join(combines_3)\n",
    "        test.loc[ind,'combination2']='; '.join(combines_2)\n",
    "        test.loc[ind,'combination1']='; '.join(combines_1)\n",
    "        test.loc[ind,'combination0']='; '.join(combines_0)\n",
    "\n",
    "    test['combination_all']= test['combination0'] + test['combination1'] + test['combination2'] + test['combination3']\n",
    "    test['combination_23']= test['combination2'] + test['combination3']\n",
    "    test['combination_01']= test['combination0'] + test['combination1'] \n",
    "\n",
    "\n",
    "    df.fillna('', inplace=True)\n",
    "    preprint_list=['biorxiv','bioRxiv','medrxiv','medRxiv','arxiv','arXiv']\n",
    "    pre_ind=df[df['journal'].isin(preprint_list)].index\n",
    "    for ind,i in enumerate(df.loc[pre_ind,'authors']):\n",
    "        try:\n",
    "            i=i.split(';')\n",
    "        except:\n",
    "            i=i.split(',')\n",
    "\n",
    "        yes_name_list=list()\n",
    "        maybe_name_list=list()\n",
    "        \n",
    "        for j in i:  # j is every single author name\n",
    "            j=re.sub(r'[^\\w]', ' ', j.strip('#'))\n",
    "            j=j.replace('  ',' ').strip(' ').replace('-','').lower()\n",
    "\n",
    "            for ind2 in test.index:\n",
    "                if j in test.loc[ind2,'combination_01']:\n",
    "                    x=standard.loc[ind2,'MatchName']\n",
    "                    maybe_name_list.append(x)\n",
    "                \n",
    "                if j in test.loc[ind2,'combination_23']:\n",
    "                    x=standard.loc[ind2,'MatchName']\n",
    "                    yes_name_list.append(x)\n",
    "                    \n",
    "        df.loc[ind,'possible biohub author']='; '.join(maybe_name_list)\n",
    "        df.loc[ind,'biohub author']='; '.join(yes_name_list)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_accents(text):\n",
    "    try:\n",
    "        text = unicode(text, 'utf-8')\n",
    "    except NameError: # unicode is a default on python 3 \n",
    "        pass\n",
    "\n",
    "    text = unicodedata.normalize('NFD', text)\\\n",
    "           .encode('ascii', 'ignore')\\\n",
    "           .decode(\"utf-8\")\n",
    "\n",
    "    return (str(text))\n",
    "\n",
    "standard=pd.read_excel('database/Biohub authors.xlsx',\n",
    "                    dtype = { \n",
    "                        'Middle' : str,\n",
    "                        'ORCID' : str,\n",
    "                        'Cohort' : str,\n",
    "                        'Email-Preferred' : str,\n",
    "                        'Email 2' : str},\n",
    "                    converters = { \n",
    "                        'Ambiguous initials' : lambda x: np.where(x == True, True, False),\n",
    "                        'Ambiguous incomplete full' : lambda x: np.where(x == True, True, False),\n",
    "                    })\n",
    "standard.dropna(how='all', axis=1,inplace=True)\t\n",
    "for i in standard.columns[2:7]:\n",
    "    standard[i]=standard[i].apply(strip_accents).str.lower().replace(\"[\",\"\").replace(\"]\",\"\")\n",
    "\n",
    "\n",
    "#储存biohub author所有可能的match\n",
    "test=standard.iloc[:,2:7]  #后面可以加上2:9, & email相等的话\n",
    "\n",
    "\n",
    "#经测试。没有last name=nickname的. middle也没有空的\n",
    "\n",
    "from itertools import permutations \n",
    "\n",
    "for ind,i in enumerate(test['Middle']):\n",
    "    combines_3=[]\n",
    "    combines_2=[]\n",
    "    combines_1=[]\n",
    "    combines_0=[]\n",
    "    \n",
    "    if i=='nmi': # no middle name\n",
    "        # \"FN-NMI\" : 3  # firstname lastname  & Lastname firstname\n",
    "        for result in permutations(test.iloc[ind,1:3], 2):\n",
    "            combines_3.append(\" \".join(result))\n",
    "        \n",
    "        # FI-NMI  # firstname[0] lastname  & lastname firstname[0]\n",
    "        for result in permutations([test.loc[ind,'First Name'][0],test.loc[ind,'Last Name']], 2):\n",
    "            combines_1.append(\" \".join(result))\n",
    "        \n",
    "        # #   ？      # firstname lastname[0]  & Lastname[0] firstname\n",
    "        # for result in permutations([test.loc[ind,'First Name'],test.loc[ind,'Last Name'][0]], 2):\n",
    "        #     combines.append(\" \".join(result))\n",
    "        \n",
    "        if (test.loc[ind,'Nickname'] != test.loc[ind,'First Name']):\n",
    "            # NN-NMI\n",
    "            for result in permutations([test.loc[ind,'Nickname'],test.loc[ind,'Last Name']], 2):\n",
    "                combines_3.append(\" \".join(result))\n",
    "                \n",
    "            # for result in permutations([test.loc[ind,'Nickname'],test.loc[ind,'Last Name'][0]], 2):\n",
    "            #     combines.append(\" \".join(result))\n",
    "                       \n",
    "    else:\n",
    "        # FN \n",
    "        for result in permutations(test.iloc[ind,1:3], 2):\n",
    "            combines_2.append(\" \".join(result))\n",
    "            \n",
    "        # FI-NMI  # firstname[0] lastname  & lastname firstname[0]\n",
    "        for result in permutations([test.loc[ind,'First Name'][0],test.loc[ind,'Last Name']], 2):\n",
    "            combines_0.append(\" \".join(result))\n",
    "        \n",
    "        # FN-MN\"            \n",
    "        combines_3.append(test.loc[ind,'First Name']+' '+test.loc[ind,'Middle'][0]+' '+test.loc[ind,'Last Name'])\n",
    "        combines_3.append(test.loc[ind,'Last Name']+' '+test.loc[ind,'Middle'][0]+' '+test.loc[ind,'First Name'])\n",
    "        \n",
    "        # FN-MI\n",
    "        combines_3.append(test.loc[ind,'Last Name']+' '+test.loc[ind,'First Name']+' '+test.loc[ind,'Middle'][0])\n",
    "        \n",
    "        # FI-MI\n",
    "        combines_1.append(test.loc[ind,'Last Name']+' '+test.loc[ind,'First Name'][0]+' '+test.loc[ind,'Middle'][0])\n",
    "        \n",
    "        if (test.loc[ind,'Nickname'] != test.loc[ind,'First Name']):\n",
    "            # NN\n",
    "            for result in permutations([test.loc[ind,'Nickname'],test.loc[ind,'Last Name']], 2):\n",
    "                combines_2.append(\" \".join(result))\n",
    "             \n",
    "        if len(test['Middle'])>1:\n",
    "            # FN-MN\"\n",
    "            combines_3.append(test.loc[ind,'First Name']+' '+test.loc[ind,'Middle']+' '+test.loc[ind,'Last Name'])\n",
    "            combines_3.append(test.loc[ind,'Last Name']+' '+test.loc[ind,'Middle']+' '+test.loc[ind,'First Name'])\n",
    "            \n",
    "            # FI-MN\n",
    "            combines_3.append(test.loc[ind,'Last Name']+' '+test.loc[ind,'First Name'][0]+' '+test.loc[ind,'Middle'])\n",
    "            \n",
    "    test.loc[ind,'combination3']='; '.join(combines_3)\n",
    "    test.loc[ind,'combination2']='; '.join(combines_2)\n",
    "    test.loc[ind,'combination1']='; '.join(combines_1)\n",
    "    test.loc[ind,'combination0']='; '.join(combines_0)\n",
    "\n",
    "test['combination_all']= test['combination0'] + test['combination1'] + test['combination2'] + test['combination3']\n",
    "test['combination_23']= test['combination2'] + test['combination3']\n",
    "test['combination_01']= test['combination0'] + test['combination1'] \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('database/basedb.csv', encoding='utf-8-sig')\n",
    "df.fillna('', inplace=True)\n",
    "\n",
    "preprint_list=['biorxiv','bioRxiv','medrxiv','medRxiv','arxiv','arXiv']\n",
    "pre_ind=df[df['journal'].isin(preprint_list)].index\n",
    "for ind,i in enumerate(df.loc[pre_ind,'authors']):\n",
    "    try:\n",
    "        i=i.split(';')\n",
    "    except:\n",
    "        i=i.split(',')\n",
    "\n",
    "    yes_name_list=list()\n",
    "    maybe_name_list=list()\n",
    "    \n",
    "    for j in i:  # j is every single author name\n",
    "        j=re.sub(r'[^\\w]', ' ', j.strip('#'))\n",
    "        j=j.replace('  ',' ').strip(' ').replace('-','').lower()\n",
    "\n",
    "        #mat_col=['combination3','combination2','combination1','combination0']\n",
    "        for ind2 in test.index:\n",
    "            if j in test.loc[ind2,'combination_01']:\n",
    "                x=standard.loc[ind2,'MatchName']\n",
    "                maybe_name_list.append(x)\n",
    "            \n",
    "            if j in test.loc[ind2,'combination_23']:\n",
    "                x=standard.loc[ind2,'MatchName']\n",
    "                yes_name_list.append(x)\n",
    "                \n",
    "    df.loc[ind,'possible biohub author']='; '.join(maybe_name_list)\n",
    "    df.loc[ind,'biohub author']='; '.join(yes_name_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Pubmed_search_author(start_date,end_date):\n",
    "    # start_date=end_date\n",
    "    author=pd.read_excel('database/Biohub authors.xlsx')\n",
    "    author_list=author['Last Name']+', '+author['First Name']+ '[FAU]'\n",
    "    df=pd.DataFrame()\n",
    "\n",
    "    for term in author_list:\n",
    "        try:\n",
    "            dt=af.Pubmed_search2(start_date, end_date,TERM=term,save_AuthorInfo=False)\n",
    "            dt['biohub author']=term.replace('[FAU]','')\n",
    "            if isinstance(dt, pd.DataFrame):\n",
    "                df=pd.concat([df,dt],ignore_index=True)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    m=list(df.columns)\n",
    "    m.remove('biohub author')\n",
    "    df=df.groupby(m)['biohub author'].apply('; '.join).reset_index()      #df2.groupby('pmid', as_index=False).agg(sum)\n",
    "    df=df.drop_duplicates(subset='pmid', keep=\"last\") # Q: how to drop duplicates then combine 'biohub author column?'\n",
    "    return df\n",
    "\n",
    "end=(datetime.date.today() - datetime.timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "df4 = Pubmed_search_author(start_date=end,end_date=end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "改进一下 \n",
    "1. middle name search\n",
    "2. 作者的对应机构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10.report\n",
    "put this in a new page. add side bar.\n",
    "create a iterative process.  \n",
    "\n",
    "\n",
    "+ reference\n",
    "\n",
    "## 1. condition: time period\n",
    "- check recent report example\n",
    "just list publication are by intramural author (only from biohub_author.csv. people only from campus simple = biohub)\n",
    "\n",
    "list publication & summary statics\n",
    "\n",
    "## 2. preprint compliance report  (around 28min'\n",
    "number + percentage\n",
    "only for inverstagtor. organized by inverstagtor MatchName.\n",
    "\n",
    "statistic is :\n",
    "    how many publication paper they have , \n",
    "    how many publication paper they have they are coressbding author \n",
    "    how many publication paper's publication type -> preprint policy apply ( they are research article not a review)\n",
    "    how many they have preprint & how many they don't\n",
    "    how many preprint they have \\ percentage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprint policy\n",
    "1. 出版类型。\n",
    "   1. pubmed种有记录 eg  review article 有review这一术语\n",
    "   2. 有部分的journal只发表review或者别的我们的政策 not apply to的 （check txt file。 这个文件包括全名和缩写。如果一些文章在这些期刊上发表，那么我们的政策不适用\n",
    "       (~df['journal'].isin(review_list))\n",
    "\n",
    "2. 当我们运行并生成报告时，我们需要的不仅是\n",
    "   1. 摘要统计数据 + 一份详细的报告，     【【一个是出版物列表，另一个是摘要统计】】\n",
    "   2. 可以逐个作者查看第一作者，然后逐个记录，\n",
    "   3. 对于在首选策略中被称为违规的已发表论文，只需手动管理它们。然后他们可能会说，你知道，当我看记录时，我会说，哦，好吧，出于某种原因，这项政策不适用于此。然后勾选一个字段，表示存在异常。这是个例外。\n",
    "      1. 对于报告作者栏：只展示前三个作者，其余改成et al  title (year) 但是不重要！后面在搞。先留个函数的框\n",
    "\n",
    "\n",
    "## 详细报告：\n",
    "第一行： 加粗的 通讯作者\n",
    "\n",
    "Author (Year) Title. Journal| Volume|: Pages|. DOI|. PMID: PMID/Accession Number. History: Original Publication|.\n",
    "\n",
    "List of authors (up to three): LastName, FirstName, Middle Initial ... if there are more than N, than first author only followed by et al. (Year) Title. Journal| Volume|: Pages|. DOI|. PMID: PMID/Accession Number. History: Original Publication|.\n",
    "\n",
    "Journal = abbreviated version of journal name\n",
    "\n",
    "\n",
    "# \n",
    "\n",
    "preprint compliance report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import all_function as af\n",
    "import pandas as pd\n",
    "\n",
    "start='2022-7-01'\n",
    "end='2022-10-30'\n",
    "\n",
    "# 1. Select author list \n",
    "author=pd.read_excel('database/Biohub authors.xlsx')  #  biohub author\n",
    "\n",
    "# 需要确定下\n",
    "intramural=author[author['Campus (simple)'] == 'Biohub']\n",
    "investigator=author.loc[author['Role'] == 'Investigator']\n",
    "\n",
    "#author.loc[((author['Campus (simple)'] == 'Biohub') & (author['Role'] == 'Investigator')]\n",
    "df_a=author.loc[(author['Campus (simple)'] == 'Biohub') | (author['Role'] == 'Investigator') ]\n",
    "\n",
    "\n",
    "# 2. Choose publication & prerpint within specific time period  # 指定时间内的pub & pre\n",
    "df = pd.read_csv('database/basedb.csv', encoding='utf-8-sig')\n",
    "df.fillna('', inplace=True)\n",
    "#df['date'] = df['date'].str.strip('/6/0').str.split(' ')[0]\n",
    "#df=af.transfer_date_format(df)\n",
    "\n",
    "\n",
    "# 2.1 Exclude 'review list'\n",
    "with open('database/list of review journals.txt') as file:\n",
    "    review_list = [line.rstrip() for line in file]\n",
    "\n",
    "df=df[~df['journal'].isin(review_list)]\n",
    "\n",
    "# 2.2 divided into two categories: publication and preprint\n",
    "#得把date格式改一下然后。换成date\n",
    "preprint_list=['biorxiv','bioRxiv','medrxiv','medRxiv','arxiv','arXiv']\n",
    "preprint = df[df['journal'].isin(preprint_list)]\n",
    "publication = df[~df['journal'].isin(preprint_list)]\n",
    "\n",
    "df['epost date'] = pd.to_datetime(df['epost date'])  \n",
    "\n",
    "pre_condition = (df['epost date'] >= start) & (df['epost date'] <= end) & (df['journal'].isin(preprint_list))\n",
    "op1_pre=df.loc[pre_condition]\n",
    "\n",
    "pub_condition = (df['epost date'] >= start) & (df['epost date'] <= end) & (~df['journal'].isin(preprint_list))\n",
    "op1_pub=df.loc[pub_condition]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "op1=\"\"\"\n",
    "Biohub intramural research program – Papers published and preprints first-deposited\n",
    "{start} ~ {end}:\n",
    "Includes papers, conference proceedings, and preprints published or first-deposited since the last Biohub All-Hands meeting that cite Biohub affiliation or funding and that include a Biohub employee or close, non-investigator affiliate as a co-author (we may easily have missed something, so please feel free to send Bill Burkholder any additions or corrections)\n",
    "Papers (Research articles, methods papers, reviews, etc.) and conference proceedings:\\n\n",
    "\"\"\"\n",
    "\n",
    "print(sub(op1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "\n",
    "d = Document()\n",
    "d.add_heading('Biohub intramural research program – Papers published and preprints first-deposited\\n ', level=1)\n",
    "\n",
    "\n",
    "paragraph = d.add_paragraph()\n",
    "paragraph.add_run('dolor').bold\n",
    "paragraph.add_run(' and some ')\n",
    "\n",
    "run = paragraph.add_run('dolor22')\n",
    "run.bold = True\n",
    "\n",
    "d.save('docx_file.docx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "run.bold = True\n",
    "document.add_heading('Biohub intramural research program – Papers published and preprints first-deposited\\n ', level=1)\n",
    "\n",
    "\\033[1mBiohub intramural research program – Papers published and preprints first-deposited\n",
    "\\033[1m{start} ~ {end}:\n",
    "\\033[0mIncludes papers, conference proceedings, and preprints published or first-deposited since the last Biohub All-Hands meeting that cite Biohub affiliation or funding and that include a Biohub employee or close, non-investigator affiliate as a co-author (we may easily have missed something, so please feel free to send Bill Burkholder any additions or corrections)\n",
    "\\n\\033[1mPapers (Research articles, methods papers, reviews, etc.) and conference proceedings:\\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 Create detail report\n",
    "# 输出word\n",
    "\n",
    "\n",
    "\n",
    "## '\\033[1m' bold following text \n",
    "## '\\033[0m' turn to normal\n",
    "import sys\n",
    "class safesub(dict):\n",
    "    def __missing__(self, key):\n",
    "        return '{' + key + '}'\n",
    "def sub(text):\n",
    "    return text.format_map(safesub(sys._getframe(1).f_locals))\n",
    "\n",
    "# 3.1.0 heading\n",
    "op1=\"\"\"\n",
    "\\033[1mBiohub intramural research program – Papers published and preprints first-deposited\n",
    "\\033[1m{start} ~ {end}:\n",
    "\\033[0mIncludes papers, conference proceedings, and preprints published or first-deposited since the last Biohub All-Hands meeting that cite Biohub affiliation or funding and that include a Biohub employee or close, non-investigator affiliate as a co-author (we may easily have missed something, so please feel free to send Bill Burkholder any additions or corrections)\n",
    "\\n\\033[1mPapers (Research articles, methods papers, reviews, etc.) and conference proceedings:\\n\n",
    "\"\"\"\n",
    "\n",
    "# 3.1.1 publication part\n",
    "for ind,row in op1_pub.iterrows():\n",
    "    op1 +='\\033[1m'+row['biohub author']+row['possible biohub author']+'\\n'+'\\033[0m   '+row['title']+str(row['pmid'])+'\\n\\n'\n",
    "\n",
    "# 3.1.2 preprint part\n",
    "op1+='\\033[1m'+\"Preprints:\\n\"\n",
    "\n",
    "for ind,row in op1_pre.iterrows():\n",
    "    op1+='\\033[1m'+row['biohub author']+row['possible biohub author']+'\\n'+'\\033[0m   '+row['title']+str(row['pmid'])+'\\n\\n'\n",
    "\n",
    "print(sub(op1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CREATE DF report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "             Biohub staff authors    All authors\n",
    "Papers (Research articles, methods papers, reviews, etc.) and conference proceedings\n",
    "Preprints\n",
    "Total\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_au=df_a['MatchName'].str.replace(',','').values.tolist()\n",
    "\n",
    "df[df['biohub author'].isin(b_au)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=pd.DataFrame(columns =['Biohub staff authors', 'All authors'],index = ['Papers (Research articles, methods papers, reviews, etc.) and conference proceedings', 'Preprints', 'Total'])\n",
    "\n",
    "df2.iloc[0,1]=op1_pub.shape[0]\n",
    "df2.iloc[1,1]=op1_pre.shape[0]\n",
    "df2.iloc[2,1]=df2.iloc[0,1]+df2.iloc[1,1]\n",
    "\n",
    "df2.iloc[0,0]=op1_pub[(op1_pub['biohub author']!='') | (op1_pub['possible biohub author']!='')].shape[0]\n",
    "df2.iloc[1,0]=op1_pre[(op1_pre['biohub author']!='') | (op1_pre['possible biohub author']!='')].shape[0]\n",
    "df2.iloc[2,0]=df2.iloc[0,0]+df2.iloc[1,0]\n",
    "\n",
    "\n",
    "print(\"Papers published and preprints first-deposited from\",start,'to',end)\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 表3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3=df.iloc[df[df['b_au']!=''].index,:]\n",
    "df3['b_au'] = df3['b_au'].map(lambda x:x.split(', '))  # Q: split by '; '\n",
    "df3=df3.explode('b_au')\n",
    "t=pd.DataFrame(df3.groupby('b_au')['record id'].count()).rename(columns={'record id':'Compliance'}) \n",
    "t.index.name = None\n",
    "#t=t[t.index.isin(author['MatchName'])]\n",
    "\n",
    "p3=pd.DataFrame(columns =['Total articles as corresponding author','Qualifying articles as corresponding author','Qualifying articles as corresponding author with preprints','Compliance'],index = author['MatchName'])\n",
    "p3.index.name = None\n",
    "p3['Compliance'].fillna(t['Compliance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preprint policy 标准:\n",
    "    publication type\n",
    "        1 pubmed: review article\n",
    "            /PublicationTypeList\n",
    "            if publication belong to this journal（看bill发的文件）, we can say the preprint policy is not apply\n",
    "        2 personal curation\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Markdown2docx import Markdown2docx\n",
    "project = Markdown2docx('Report')\n",
    "project.eat_soup()\n",
    "project.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "author file里的格式是 last_name, first_name\n",
    "\n",
    "首先。需要是每一行中存在就继续、所以不能用isin 得用循环\n",
    "\n",
    "其次。match name的大小写、中间是否有逗号得统一\n",
    "'biohub author','possible biohub author','corresponding author'\n",
    "\n",
    "现在是全 matchname格式只是小写了而已"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=af.authormatch_pre(df)\n",
    "df=af.authormatch_pub(df)\n",
    "df.to_csv('database/basedb.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start='2022-11-1'\n",
    "end='2022-11-10'\n",
    "keyword='biohub'\n",
    "df3 = af.Pubmed_search2(start_date=start, end_date=end,TERM='(zuckerb* AND biohub) OR \"cz biohub\" OR \"czi biohub\"',save_AuthorInfo=True)\n",
    "df4 = af.Pubmed_search_author(start_date=end,end_date=end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors=pd.read_csv('database/pubmed api author.csv',encoding='utf-8-sig')\t\t\n",
    "df = pd.read_csv('database/basedb.csv', encoding='utf-8-sig')      \n",
    "authors\n",
    "\n",
    "\n",
    "# save them to dataframe.\n",
    "df['format biohub author']=''\n",
    "for ind,i in enumerate(authors['pmid']):\n",
    "    if authors.loc[ind,'TrustMatch']=='Yes':\n",
    "        #if authors.loc[ind,'MatchName'] not in df.loc[df[df['pmid']==authors.loc[ind,'pmid']].index,'biohub author'].str.lower().replace(', ',' '):\n",
    "        df.loc[df[df['pmid']==authors.loc[ind,'pmid']].index,'format biohub author'] += '; '+authors.loc[ind,'MatchName']\n",
    "    if authors.loc[ind,'TrustMatch']=='Maybe':\n",
    "        df.loc[df[df['pmid']==authors.loc[ind,'pmid']].index,'possible biohub author'] += '; '+authors.loc[ind,'MatchName']\n",
    "df['format biohub author']=df['format biohub author'].str.strip('; ')\n",
    "set(df['format biohub author'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors.loc[ind,'pmid']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind=3\n",
    "authors.loc[ind,'MatchName']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import all_function as af #import the module here, so that it can be reloaded.\n",
    "importlib.reload(af)\n",
    "df6,dfa=af.Pubmed_search2(start_date='2022-11-1',\n",
    "                 end_date='2022-11-12',TERM='biohub',save_AuthorInfo=False)\n",
    "\n",
    "# df1=af.BioMedrxiv_Search2(start_date='2022-11-1',\n",
    "#                 end_date='2022-11-10',\n",
    "#                 keyword='biohub')\n",
    "\n",
    "set(af.authormatch_pub(df6)['format biohub author'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "report 增加filter。根据选择可以返回不同的内容\n",
    "    Campus (simple)\tCampus Role\tTeam\tStatus\n",
    "    \n",
    "    \n",
    "Tab2:\n",
    "biohub staff author: Campus (simple)=='Biohub'\n",
    "    \n",
    "all author: biohub csv里的\n",
    "\n",
    "Tab3:\n",
    "    review article is not Qualifying \n",
    "\n",
    "1. Qualifying articles as corresponding author: \tnot review paper\n",
    "2. Qualifying articles as corresponding author with preprints:\t\n",
    "3. Compliance: percentage %.\n",
    "   \n",
    "    8 paper = corresponding author & qualifying\n",
    "        none perprint  Compliance=0\n",
    "        all have preprint Compliance=100\n",
    "        4 have preprint Compliance=50\n",
    "\n",
    "\n",
    "#['Campus (simple)']=='Biohub'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('database/basedb.csv', encoding='utf-8-sig')\n",
    "df.fillna('', inplace=True)\n",
    "author=pd.read_csv('database/Biohub authors.csv', encoding='utf-8-sig')  #  biohub author\n",
    "\n",
    "filter_author=author['MatchName'].str.replace(',','').to_list()\n",
    "\n",
    "\n",
    "# 2.1 Exclude 'review list'\n",
    "with open('database/list of review journals.txt') as file:\n",
    "    review_list = [line.rstrip() for line in file]\n",
    "\n",
    "df=df[~df['journal'].isin(review_list)]\n",
    "df['b_au']=df[['biohub author','possible biohub author','corresponding author','format biohub author']].agg('; '.join, axis=1).str.replace('; ; ','; ').str.strip('; ')\n",
    "\n",
    "ind_list=[]\n",
    "for ind,i in enumerate(df['b_au']):\n",
    "    for j in i.split(';'): \n",
    "        print(j)\n",
    "        if j.strip(' ').replace(', ',' ') in filter_author:\n",
    "            print(j)\n",
    "            ind_list.append(ind)\n",
    "\n",
    "print(ind_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['b_au']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Charles R Langelier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. 加两个按钮来输出报告 & report\n",
    "\n",
    "start="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 8. 加登录界面。只有特定的人才能修改。\n",
    "\teg。 只能用biohub的邮箱注册"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. author的csv也得保证更新是那种的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "把 award date放进author csv\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加citation - 失败。bill说先搁置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# author match找到missing的之后可以把他replace一下呢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# debug： 只有delete db部分有问题。他变成全delete了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11.24 FILTER [Done]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11.26 report数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 加一个方法。输入名字，输出改好的名字\n",
    "\n",
    "如何把方法改成：\n",
    "def (input=名字)\n",
    "    return format名字，可能等级Yes/No\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "简单版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard=pd.read_excel('database/Biohub authors.xlsx',\n",
    "                    dtype = { \n",
    "                        'Middle' : str,\n",
    "                        'ORCID' : str,\n",
    "                        'Cohort' : str,\n",
    "                        'Email-Preferred' : str,\n",
    "                        'Email 2' : str},\n",
    "                    converters = { \n",
    "                        'Ambiguous initials' : lambda x: np.where(x == True, True, False),\n",
    "                        'Ambiguous incomplete full' : lambda x: np.where(x == True, True, False),\n",
    "                    })\n",
    "standard.dropna(how='all', axis=1,inplace=True)\t\n",
    "for i in standard.columns[3:7]:\n",
    "    standard[i]=standard[i].apply(strip_accents).str.lower().str.replace(\"[\",\"\").str.replace(\"]\",\"\")\n",
    "\n",
    "#use a new df to store\n",
    "test=standard.iloc[:,2:7]  \n",
    "\n",
    "for ind,i in enumerate(test['Middle']):\n",
    "    combines_3=[]\n",
    "    combines_2=[]\n",
    "    combines_1=[]\n",
    "    combines_0=[]\n",
    "    \n",
    "    if i=='nmi': # no middle name\n",
    "        # \"FN-NMI\" : 3  # firstname lastname  & Lastname firstname\n",
    "        for result in permutations(test.iloc[ind,1:3], 2):\n",
    "            combines_3.append(\" \".join(result))\n",
    "        \n",
    "        # FI-NMI  # firstname[0] lastname  & lastname firstname[0]\n",
    "        for result in permutations([test.loc[ind,'First Name'][0],test.loc[ind,'Last Name']], 2):\n",
    "            combines_1.append(\" \".join(result))\n",
    "\n",
    "        if (test.loc[ind,'Nickname'] != test.loc[ind,'First Name']):\n",
    "            # NN-NMI\n",
    "            for result in permutations([test.loc[ind,'Nickname'],test.loc[ind,'Last Name']], 2):\n",
    "                combines_3.append(\" \".join(result))\n",
    "                \n",
    "            # for result in permutations([test.loc[ind,'Nickname'],test.loc[ind,'Last Name'][0]], 2):\n",
    "            #     combines.append(\" \".join(result))\n",
    "        \n",
    "    else:\n",
    "        # FN \n",
    "        for result in permutations(test.iloc[ind,1:3], 2):\n",
    "            combines_2.append(\" \".join(result))\n",
    "            \n",
    "        # FI-NMI  # firstname[0] lastname  & lastname firstname[0]\n",
    "        for result in permutations([test.loc[ind,'First Name'][0],test.loc[ind,'Last Name']], 2):\n",
    "            combines_0.append(\" \".join(result))\n",
    "        \n",
    "        # FN-MN\"            \n",
    "        combines_3.append(test.loc[ind,'First Name']+' '+test.loc[ind,'Middle'][0]+' '+test.loc[ind,'Last Name'])\n",
    "        combines_3.append(test.loc[ind,'Last Name']+' '+test.loc[ind,'Middle'][0]+' '+test.loc[ind,'First Name'])\n",
    "        \n",
    "        # FN-MI\n",
    "        combines_3.append(test.loc[ind,'Last Name']+' '+test.loc[ind,'First Name']+' '+test.loc[ind,'Middle'][0])\n",
    "        \n",
    "        # FI-MI\n",
    "        combines_1.append(test.loc[ind,'Last Name']+' '+test.loc[ind,'First Name'][0]+' '+test.loc[ind,'Middle'][0])\n",
    "        \n",
    "        if (test.loc[ind,'Nickname'] != test.loc[ind,'First Name']):\n",
    "            # NN\n",
    "            for result in permutations([test.loc[ind,'Nickname'],test.loc[ind,'Last Name']], 2):\n",
    "                combines_2.append(\" \".join(result))\n",
    "            \n",
    "        if len(test['Middle'])>1:\n",
    "            # FN-MN\"\n",
    "            combines_3.append(test.loc[ind,'First Name']+' '+test.loc[ind,'Middle']+' '+test.loc[ind,'Last Name'])\n",
    "            combines_3.append(test.loc[ind,'Last Name']+' '+test.loc[ind,'Middle']+' '+test.loc[ind,'First Name'])\n",
    "            \n",
    "            # FI-MN\n",
    "            combines_3.append(test.loc[ind,'Last Name']+' '+test.loc[ind,'First Name'][0]+' '+test.loc[ind,'Middle'])\n",
    "            \n",
    "    test.loc[ind,'combination3']='; '.join(combines_3)\n",
    "    test.loc[ind,'combination2']='; '.join(combines_2)\n",
    "    test.loc[ind,'combination1']='; '.join(combines_1)\n",
    "    test.loc[ind,'combination0']='; '.join(combines_0)\n",
    "\n",
    "test['combination_all']= test['combination0'] + test['combination1'] + test['combination2'] + test['combination3']\n",
    "test['combination_23']= test['combination2'] + test['combination3']\n",
    "test['combination_01']= test['combination0'] + test['combination1'] \n",
    "\n",
    "test.to_csv('database/biohub author combination.csv',index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return old_name, format_name,possible percent\n",
    "def authormatch_sim(name):\n",
    "    test=pd.read_csv('database/biohub author combination.csv', encoding='utf-8-sig')\n",
    "    degree='no'\n",
    "    format_name=re.sub(r'[^\\w]', ' ', name.strip('#'))\n",
    "    format_name=format_name.replace('  ',' ').strip(' ').replace('-','').lower()\n",
    "    \n",
    "    for ind2 in test.index:\n",
    "        if format_name in test.loc[ind2,'combination_01']:\n",
    "            format_name=test.loc[ind2,'MatchName']\n",
    "            degree='maybe'\n",
    "        \n",
    "        elif format_name in test.loc[ind2,'combination_23']:\n",
    "            format_name=test.loc[ind2,'MatchName']\n",
    "            degree='yes'\n",
    "\n",
    "    return name,format_name,degree\n",
    "\n",
    "\n",
    "authormatch_sim('yosef n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind,i in enumerate(df.loc[pre_ind,'authors']):\n",
    "    try:\n",
    "        i=i.split(';')\n",
    "        \n",
    "    except:\n",
    "        i=i.split(',')\n",
    "    \n",
    "    for j in i:  \n",
    "        j=re.sub(r'[^\\w]', ' ', j.strip('#'))\n",
    "        j=j.replace('  ',' ').strip(' ').replace('-','').lower()\n",
    "        \n",
    "yosef nir; nir yosef\tn yosef; yosef n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "af.authormatch(name)\n",
    "  for ind in authors['pmid'].index:\n",
    "        if authors.loc[ind,'TrustMatch']=='Yes':\n",
    "            #if authors.loc[ind,'MatchName'] not in df.loc[df[df['pmid']==authors.loc[ind,'pmid']].index,'biohub author'].str.lower().replace(', ',' '):\n",
    "            df.loc[df[df['pmid']==authors.loc[ind,'pmid']].index,'format biohub author'] += '; '+authors.loc[ind,'MatchName']\n",
    "        if authors.loc[ind,'TrustMatch']=='Maybe':\n",
    "            df.loc[df[df['pmid']==authors.loc[ind,'pmid']].index,'possible biohub author'] += '; '+authors.loc[ind,'MatchName']\n",
    "\n",
    "    df['format biohub author']=df['format biohub author'].str.strip('; ')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import all_function as af #import the module here, so that it can be reloaded.\n",
    "importlib.reload(af)\n",
    "#import all_function as af\n",
    "\n",
    "af.authormatch('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name=''\n",
    "test=pd.read_csv('database/biohub author combination.csv', encoding='utf-8-sig')\n",
    "degree='no'\n",
    "format_name=re.sub(r'[^\\w]', ' ', name.strip('#'))\n",
    "format_name=format_name.replace('  ',' ').strip(' ').replace('-','').lower()\n",
    "\n",
    "for ind2 in test.index:\n",
    "    if format_name in test.loc[ind2,'combination_01']:\n",
    "        format_name=test.loc[ind2,'MatchName']\n",
    "        print(format_name,ind2)\n",
    "        degree='maybe'\n",
    "    \n",
    "    elif format_name in test.loc[ind2,'combination_23']:\n",
    "        format_name=test.loc[ind2,'MatchName']\n",
    "        print(format_name,1)\n",
    "        degree='yes'\n",
    "\n",
    "print(format_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name='Nir Yosef'\n",
    "biohub_authors_variations_full = np.load('database/biohub_authors_variations_full.npy',allow_pickle=True).item()\n",
    "name=re.sub(r'[^\\w]', ' ', name.strip('#'))\n",
    "name=name.replace('  ',' ').strip(' ').replace('-','').replace(' ',', ').lower()  # \n",
    "Name = strip_accents(name)\n",
    "print(Name,0)\n",
    "\n",
    "MatchName = \"\"\n",
    "MatchType = \"\"\n",
    "if Name in biohub_authors_variations_full:\n",
    "    print(0)\n",
    "    MatchName = biohub_authors_variations_full[Name][0]\n",
    "    MatchType = biohub_authors_variations_full[Name][1] \n",
    "else:\n",
    "    compress = Name.replace(\" \",\"\").replace(\"-\",\"\").replace(\"\\'\",\"\").replace(\",\",\", \")\n",
    "    if Name[-2:-1] == \" \" and Name[-3:-2] != \",\":\n",
    "        compress = compress[:-1]+Name[-2:] # restore the penultimate space if there is one\n",
    "    if compress != Name and compress in biohub_authors_variations_full:\n",
    "        MatchName = biohub_authors_variations_full[compress][0]\n",
    "        MatchType = biohub_authors_variations_full[compress][1] \n",
    "\n",
    "print(name,MatchName,MatchType)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "af.authormatch(name='Ada S Y Poon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'sarnow, peter'.title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.load('database/biohub_authors_variations_full.npy',allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BiohubAuthors_df = pd.read_excel('database/Biohub authors.xlsx',\n",
    "                                dtype = { \n",
    "                                    'Middle' : str,\n",
    "                                    'ORCID' : str,\n",
    "                                    'Cohort' : str,\n",
    "                                    'Email-Preferred' : str,\n",
    "                                    'Email 2' : str},\n",
    "                                converters = { \n",
    "                                    'Ambiguous initials' : lambda x: np.where(x == True, True, False),\n",
    "                                    'Ambiguous incomplete full' : lambda x: np.where(x == True, True, False),\n",
    "                                })\n",
    "\n",
    "BiohubAuthors_df['MatchName'] = BiohubAuthors_df['MatchName'].apply(strip_accents)\n",
    "BiohubAuthors_df['Last Name'] = BiohubAuthors_df['Last Name'].apply(strip_accents)\n",
    "BiohubAuthors_df['First Name'] = BiohubAuthors_df['First Name'].apply(strip_accents)\n",
    "BiohubAuthors_df['Nickname'] = BiohubAuthors_df['Nickname'].apply(strip_accents)\n",
    "\n",
    "BiohubAuthors_list = BiohubAuthors_df.values.tolist()\n",
    "BiohubAuthors_columns = BiohubAuthors_df.columns.tolist()\n",
    "\n",
    "biohub_authors = {} # column names are indexed in BiohubAuthors_columns.index(\"column name\")\n",
    "biohub_authors_variations = {} \n",
    "biohub_authors_awarddates = {}\n",
    "\n",
    "name_match_weight = {\n",
    "    \"FN-NMI\" : 3, # \"FN-NMI\" : first name-no middle initial\n",
    "    \"NN-NMI\" : 3, # \"NN-NMI\" : Nickname-no middle initial\n",
    "    \"FN-MN\" : 3,  # \"FN-MN\" : first name-middle name\n",
    "    \"FI-MN\" : 3,  # \"FI-MN\" : first initial-middle name-(when preferred)\n",
    "    \"FN-MI\" : 3,  # \"FN-MI\" : first name-middle initial\n",
    "    \"FN\" : 2,     # \"FN\" : first name, omitting middle initial\n",
    "    \"NN\" : 2,     # \"NN\" : Nickname-omitting middle initial\n",
    "    \"FI-MI\" : 1,  # \"FI-MI\" : first initial-middle initial\n",
    "    \"FI-NMI\" : 1, # \"FI-NMI\" : first initial-no middle initial\n",
    "    \"FI\" : 0      # \"FI\" : first initial-omitting middle initial\n",
    "}\n",
    "\n",
    "for row in BiohubAuthors_list:\n",
    "    MatchName = row[BiohubAuthors_columns.index(\"MatchName\")]\n",
    "    biohub_authors[MatchName] = row\n",
    "        \n",
    "    LastName = row[BiohubAuthors_columns.index(\"Last Name\")].lower()\n",
    "    FirstName = row[BiohubAuthors_columns.index(\"First Name\")].lower()\n",
    "    find_bracket = FirstName.find(\"[\") # brackets used to indicate use of first initial as alternate to first name: \"J[ames]\"\n",
    "    if find_bracket != -1:\n",
    "        FirstName = FirstName.replace(\"[\",\"\").replace(\"]\",\"\")\n",
    "    Nickname = row[BiohubAuthors_columns.index(\"Nickname\")].lower()\n",
    "    Middle = row[BiohubAuthors_columns.index(\"Middle\")].lower()\n",
    "\n",
    "    EntryName = LastName+\", \"+FirstName\n",
    "    EntryFI = LastName+\", \"+FirstName[0:1]\n",
    "    if Middle == \"nmi\":\n",
    "        biohub_authors_variations[EntryName] = [MatchName, \"FN-NMI\"] # first name-no middle initial\n",
    "        biohub_authors_variations[EntryFI] = [MatchName, \"FI-NMI\"] # first initial-no middle initial\n",
    "        if Nickname != FirstName:\n",
    "            EntryName = LastName+\", \"+Nickname\n",
    "            biohub_authors_variations[EntryName] = [MatchName, \"NN-NMI\"] # Nickname-no middle initial\n",
    "    else:\n",
    "        biohub_authors_variations[EntryName] = [MatchName, \"FN\"] # first name, omitting middle initial\n",
    "        biohub_authors_variations[EntryFI] = [MatchName, \"FI\"] # first initial-omitting middle initial\n",
    "        if Nickname != FirstName:\n",
    "            EntryName = LastName+\", \"+Nickname\n",
    "            biohub_authors_variations[EntryName] = [MatchName, \"NN\"] # Nickname-omitting middle initial\n",
    "        if len(Middle) > 1:\n",
    "            EntryName = LastName+\", \"+FirstName+\" \"+Middle\n",
    "            biohub_authors_variations[EntryName] = [MatchName, \"FN-MN\"] # first name-middle name\n",
    "        if find_bracket != -1: \n",
    "            EntryName = EntryFI+\" \"+Middle\n",
    "            biohub_authors_variations[EntryName] = [MatchName, \"FI-MN\"] # first initial-middle name-preferred\n",
    "        EntryName = LastName+\", \"+FirstName+\" \"+Middle[0:1]\n",
    "        biohub_authors_variations[EntryName] = [MatchName, \"FN-MI\"] # first name-middle initial\n",
    "        EntryName = EntryFI+\" \"+Middle[0:1]\n",
    "        if EntryName not in biohub_authors_variations:\n",
    "            biohub_authors_variations[EntryName] = [MatchName, \"FI-MI\"] # first initial-middle initial\n",
    "\n",
    "biohub_authors_variations_full = {} # includes compressed versions of names, omitting spaces, dashes, apostrophes, etc   \n",
    "\n",
    "for key,value in biohub_authors_variations.items():\n",
    "    biohub_authors_variations_full[key] = value\n",
    "    compress = key.replace(\" \",\"\").replace(\"-\",\"\").replace(\"\\'\",\"\").replace(\",\",\", \")\n",
    "    if key[-2:-1] == \" \" and key[-3:-2] != \",\":\n",
    "        compress = compress[:-1]+key[-2:] # restore the penultimate space if there is one\n",
    "    if compress != key:\n",
    "        biohub_authors_variations_full[compress] = value\n",
    "\n",
    "np.save('database/biohub_authors_variations_full.npy', biohub_authors_variations_full) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11.27 debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('database/basedb.csv', encoding='utf-8-sig')\n",
    "print(df.shape)\n",
    "\n",
    "start=\"2022-06-01\"\n",
    "end=\"2022-11-27\"\n",
    "df['epost date'] = pd.to_datetime(df['epost date'])  \n",
    "condition = (df['epost date'] >= start) & (df['epost date'] <= end) \n",
    "df=df.loc[condition]\n",
    "df.fillna('', inplace=True)\n",
    "print(df.shape)\n",
    "\n",
    "for ind,i in enumerate(df['corresponding author']):\n",
    "    if i == '':\n",
    "        break\n",
    "    i=i.split(';')\n",
    "    res=[]\n",
    "    for j in i:  \n",
    "        j=re.sub(r'[^\\w]', ' ', j.strip('#'))\n",
    "        j=j.replace('  ',' ').strip(' ').replace('-','').lower()\n",
    "        old_name,new_name,prob=af.authormatch(j)\n",
    "        res.append(new_name)\n",
    "    df.loc[ind,'corresponding author'] = '; '.join(m for m in res)\n",
    "df.fillna('', inplace=True)\n",
    "\n",
    "print(df.shape)\n",
    "df.loc[df['journal']=='',:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def authormatch(name):\n",
    "    test=pd.read_csv('database/biohub author combination.csv', encoding='utf-8-sig')\n",
    "    degree='no'\n",
    "    format_name=re.sub(r'[^\\w]', ' ', name.strip('#'))\n",
    "    format_name=format_name.replace('  ',' ').strip(' ').replace('-','').lower()\n",
    "    \n",
    "    if name=='':\n",
    "        return name,format_name,degree\n",
    "    for ind2 in test.index:\n",
    "        if format_name in test.loc[ind2,'combination_01']:\n",
    "            format_name=test.loc[ind2,'MatchName']\n",
    "            degree='maybe'\n",
    "        \n",
    "        elif format_name in test.loc[ind2,'combination_23']:\n",
    "            format_name=test.loc[ind2,'MatchName']\n",
    "            degree='yes'\n",
    "    \n",
    "    return name,format_name,degree\n",
    "\n",
    "\n",
    "for ind,i in enumerate(df['corresponding author']):\n",
    "    if i == '':\n",
    "        continue\n",
    "    i_list=i.split(';')\n",
    "    res=[]\n",
    "    for j in i_list:  \n",
    "        old_name,new_name,prob=authormatch(j)\n",
    "        res.append(new_name)\n",
    "    df.loc[ind,'corresponding author'] = '; '.join(m for m in res)\n",
    "\n",
    "\n",
    "author=pd.read_csv('database/Biohub authors.csv', encoding='utf-8-sig')  #  biohub author\n",
    "author['Team']=author['Team'].fillna('NA')\n",
    "author=author.fillna('')\n",
    "fauthor_list=author['MatchName'].str.replace(',','').to_list()\n",
    "df['b_au']=df['format biohub author']\n",
    "ind_list=[]\n",
    "biohub_staff_ind=[]\n",
    "for ind,i in enumerate(df['b_au']):\n",
    "    i='; '.join(set([j.strip() for j in i.split(';') if j]))\n",
    "    df.loc[ind,'b_au']=i\n",
    "    for j in i.split(';'): \n",
    "        if j.strip(' ').replace(', ',' ') in fauthor_list:  # only return people who is in filter list\n",
    "            # Problem: if there are some author not in 'biohub author.xlsx', it will not show their publication\n",
    "            ind_list.append(ind)   \n",
    "\n",
    "df=df.iloc[ind_list,:]\n",
    "pd.option_context('display.max_rows', None, 'display.max_columns', None)\n",
    "\n",
    "    #print(df)\n",
    "#df.loc[df['corresponding author']!='',['title','journal','corresponding author']].set_index(['title','journal'])['corresponding author'].str.split(\"; \", expand=True).stack().reset_index(drop=True, level=-1).reset_index().groupby([0]).size().rename(\"corresponding author\")\n",
    "\n",
    "# p3=pd.DataFrame(columns =['Total articles as corresponding author','Qualifying articles as corresponding author','Qualifying articles as corresponding author with preprints','Compliance'],index = author['MatchName'])\n",
    "# p3.index.name = None\n",
    "\n",
    "\n",
    "# c1=df.loc[df['corresponding author']!='',['title','journal','corresponding author']].set_index(['title','journal'])['corresponding author'].str.split(\"; \", expand=True).stack().reset_index(drop=True, level=-1).reset_index().groupby([0]).size().rename(\"corresponding author\")\n",
    "# #c1=c1.groupby(['0']).size().rename(columns={0: \"corresponding author\"})\n",
    "\n",
    "\n",
    "# #p3['Total articles as corresponding author']=\n",
    "# t1=pd.merge(p3,c1,how='left',left_index=True,right_index=True)\n",
    "# t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total articles as corresponding author</th>\n",
       "      <th>Qualifying articles as corresponding author</th>\n",
       "      <th>Qualifying articles as corresponding author with preprints</th>\n",
       "      <th>Compliance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Banfield, Jill</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fordyce, Polly</th>\n",
       "      <td>16.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>228.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Frost, Adam</th>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1200.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Huber, Greg</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>133.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Marson, Alex</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>200.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pollard, Katie</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Poon, Ada</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Total articles as corresponding author  \\\n",
       "Banfield, Jill                                     2.0   \n",
       "Fordyce, Polly                                    16.0   \n",
       "Frost, Adam                                       12.0   \n",
       "Huber, Greg                                        4.0   \n",
       "Marson, Alex                                       4.0   \n",
       "Pollard, Katie                                     1.0   \n",
       "Poon, Ada                                          4.0   \n",
       "\n",
       "                Qualifying articles as corresponding author  \\\n",
       "Banfield, Jill                                          2.0   \n",
       "Fordyce, Polly                                         16.0   \n",
       "Frost, Adam                                            12.0   \n",
       "Huber, Greg                                             4.0   \n",
       "Marson, Alex                                            4.0   \n",
       "Pollard, Katie                                          1.0   \n",
       "Poon, Ada                                               4.0   \n",
       "\n",
       "                Qualifying articles as corresponding author with preprints  \\\n",
       "Banfield, Jill                                                2.0            \n",
       "Fordyce, Polly                                                7.0            \n",
       "Frost, Adam                                                   1.0            \n",
       "Huber, Greg                                                   3.0            \n",
       "Marson, Alex                                                  2.0            \n",
       "Pollard, Katie                                                1.0            \n",
       "Poon, Ada                                                     4.0            \n",
       "\n",
       "                 Compliance  \n",
       "Banfield, Jill   100.000000  \n",
       "Fordyce, Polly   228.571429  \n",
       "Frost, Adam     1200.000000  \n",
       "Huber, Greg      133.333333  \n",
       "Marson, Alex     200.000000  \n",
       "Pollard, Katie   100.000000  \n",
       "Poon, Ada        100.000000  "
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.fillna('', inplace=True)\n",
    "p3[p3['Qualifying articles as corresponding author with preprints'].notna()]\n",
    "#\n",
    "#df.loc[ (df['corresponding author']!='') & ( df['possible match result']!='' ),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total articles as corresponding author</th>\n",
       "      <th>Qualifying articles as corresponding author</th>\n",
       "      <th>Qualifying articles as corresponding author with preprints</th>\n",
       "      <th>Compliance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Abate, Adam</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Altman, Russ</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Arnaout, Rima</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ashley, Euan</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Banfield, Jill</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Yosef, Nir</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Yu, Bin</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Zhang, Wenjun</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Zou, James</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>de la Zerda, Adam</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>143 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Total articles as corresponding author  \\\n",
       "Abate, Adam                                           NaN   \n",
       "Altman, Russ                                          NaN   \n",
       "Arnaout, Rima                                         NaN   \n",
       "Ashley, Euan                                          NaN   \n",
       "Banfield, Jill                                        2.0   \n",
       "...                                                   ...   \n",
       "Yosef, Nir                                            NaN   \n",
       "Yu, Bin                                               NaN   \n",
       "Zhang, Wenjun                                         NaN   \n",
       "Zou, James                                            NaN   \n",
       "de la Zerda, Adam                                     NaN   \n",
       "\n",
       "                   Qualifying articles as corresponding author  \\\n",
       "Abate, Adam                                                NaN   \n",
       "Altman, Russ                                               NaN   \n",
       "Arnaout, Rima                                              NaN   \n",
       "Ashley, Euan                                               NaN   \n",
       "Banfield, Jill                                             2.0   \n",
       "...                                                        ...   \n",
       "Yosef, Nir                                                 NaN   \n",
       "Yu, Bin                                                    NaN   \n",
       "Zhang, Wenjun                                              NaN   \n",
       "Zou, James                                                 NaN   \n",
       "de la Zerda, Adam                                          NaN   \n",
       "\n",
       "                   Qualifying articles as corresponding author with preprints  \\\n",
       "Abate, Adam                                                      NaN            \n",
       "Altman, Russ                                                     NaN            \n",
       "Arnaout, Rima                                                    NaN            \n",
       "Ashley, Euan                                                     NaN            \n",
       "Banfield, Jill                                                   2.0            \n",
       "...                                                              ...            \n",
       "Yosef, Nir                                                       NaN            \n",
       "Yu, Bin                                                          NaN            \n",
       "Zhang, Wenjun                                                    NaN            \n",
       "Zou, James                                                       NaN            \n",
       "de la Zerda, Adam                                                NaN            \n",
       "\n",
       "                   Compliance  \n",
       "Abate, Adam               NaN  \n",
       "Altman, Russ              NaN  \n",
       "Arnaout, Rima             NaN  \n",
       "Ashley, Euan              NaN  \n",
       "Banfield, Jill          100.0  \n",
       "...                       ...  \n",
       "Yosef, Nir                NaN  \n",
       "Yu, Bin                   NaN  \n",
       "Zhang, Wenjun             NaN  \n",
       "Zou, James                NaN  \n",
       "de la Zerda, Adam         NaN  \n",
       "\n",
       "[143 rows x 4 columns]"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind,i in enumerate(df['corresponding author']):\n",
    "        if i == '':\n",
    "            continue \n",
    "        i_list=i.split(';')\n",
    "        res=[]\n",
    "        for j in i_list:  \n",
    "            old_name,new_name,prob=af.authormatch(j)\n",
    "            res.append(new_name)\n",
    "        df.loc[ind,'corresponding author'] = '; '.join(m for m in res)\n",
    "    df.fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('david j pagliarini', 'david j pagliarini', 'no')"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "authormatch('david j pagliarini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(88, 32)\n",
      "(65, 32)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>record id</th>\n",
       "      <th>save datetime</th>\n",
       "      <th>biohub author</th>\n",
       "      <th>possible biohub author</th>\n",
       "      <th>format biohub author</th>\n",
       "      <th>corresponding author</th>\n",
       "      <th>corresponding author institution</th>\n",
       "      <th>journal</th>\n",
       "      <th>doi</th>\n",
       "      <th>pmid</th>\n",
       "      <th>...</th>\n",
       "      <th>authors2</th>\n",
       "      <th>affiliations list</th>\n",
       "      <th>author - affiliations</th>\n",
       "      <th>published or not</th>\n",
       "      <th>confirm published doi</th>\n",
       "      <th>confirm preprint doi</th>\n",
       "      <th>possible match result</th>\n",
       "      <th>match id</th>\n",
       "      <th>record change number</th>\n",
       "      <th>b_au</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>96.0</td>\n",
       "      <td>2022-11-10</td>\n",
       "      <td>Peter J Turnbaugh</td>\n",
       "      <td></td>\n",
       "      <td>Turnbaugh, Peter; Turnbaugh, Peter</td>\n",
       "      <td>Turnbaugh, Peter</td>\n",
       "      <td>Department of Microbiology and Immunology, Uni...</td>\n",
       "      <td>Nature metabolism</td>\n",
       "      <td>10.1038/s42255-022-00684-9</td>\n",
       "      <td>36333489.0</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>Department of Microbiology and Immunology, Uni...</td>\n",
       "      <td>Vaibhav Upadhyay: Department of Microbiology a...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>Turnbaugh, Peter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>97.0</td>\n",
       "      <td>2022-11-10</td>\n",
       "      <td>Markita P Landry; Aaron Streets</td>\n",
       "      <td></td>\n",
       "      <td>Landry, Markita; Streets, Aaron; Landry, Marki...</td>\n",
       "      <td>Streets, Aaron</td>\n",
       "      <td>Department of Bioengineering, University of Ca...</td>\n",
       "      <td>Scientific reports</td>\n",
       "      <td>10.1038/s41598-022-23054-7</td>\n",
       "      <td>36335145.0</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>Biophysics Graduate Group, University of Calif...</td>\n",
       "      <td>Xinyi Zhang: Department of Bioengineering, Uni...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>Landry, Markita; Streets, Aaron</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>98.0</td>\n",
       "      <td>2022-11-10</td>\n",
       "      <td>Rob Phillips; Sophie Dumont</td>\n",
       "      <td></td>\n",
       "      <td>Dumont, Sophie; Dumont, Sophie</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>eLife</td>\n",
       "      <td>10.7554/eLife.79558</td>\n",
       "      <td>36346735.0</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>Biochemistry and Molecular Biophysics Option, ...</td>\n",
       "      <td>Pooja Suresh: Biophysics Graduate Program, Uni...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>10.1101/2022.04.08.487649</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>Dumont, Sophie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>100.0</td>\n",
       "      <td>2022-11-10</td>\n",
       "      <td>Chun Jimmie Ye; Alexander Marson; Kole T Roybal</td>\n",
       "      <td></td>\n",
       "      <td>Ye, Jimmie; Marson, Alex; Roybal, Kole; Ye, Ji...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Science translational medicine</td>\n",
       "      <td>10.1126/scitranslmed.abm1463</td>\n",
       "      <td>36350984.0</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>Department of Microbiology and Immunology, Uni...</td>\n",
       "      <td>Daniel B Goodman: Department of Microbiology a...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>Ye, Jimmie; Marson, Alex; Roybal, Kole</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>104.0</td>\n",
       "      <td>2022-11-10</td>\n",
       "      <td>Kerwyn Casey Huang</td>\n",
       "      <td></td>\n",
       "      <td>Huang, KC; Huang, KC; Huang, KC</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Computational and structural biotechnology jou...</td>\n",
       "      <td>10.1016/j.csbj.2022.10.008</td>\n",
       "      <td>36382191.0</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>Biophysics Program, Stanford University, Stanf...</td>\n",
       "      <td>Benjamin D Knapp: Biophysics Program, Stanford...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>Huang, KC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    record id save datetime                                    biohub author  \\\n",
       "77       96.0    2022-11-10                                Peter J Turnbaugh   \n",
       "78       97.0    2022-11-10                  Markita P Landry; Aaron Streets   \n",
       "79       98.0    2022-11-10                      Rob Phillips; Sophie Dumont   \n",
       "81      100.0    2022-11-10  Chun Jimmie Ye; Alexander Marson; Kole T Roybal   \n",
       "83      104.0    2022-11-10                               Kerwyn Casey Huang   \n",
       "\n",
       "   possible biohub author                               format biohub author  \\\n",
       "77                                        Turnbaugh, Peter; Turnbaugh, Peter   \n",
       "78                         Landry, Markita; Streets, Aaron; Landry, Marki...   \n",
       "79                                            Dumont, Sophie; Dumont, Sophie   \n",
       "81                         Ye, Jimmie; Marson, Alex; Roybal, Kole; Ye, Ji...   \n",
       "83                                           Huang, KC; Huang, KC; Huang, KC   \n",
       "\n",
       "   corresponding author                   corresponding author institution  \\\n",
       "77     Turnbaugh, Peter  Department of Microbiology and Immunology, Uni...   \n",
       "78       Streets, Aaron  Department of Bioengineering, University of Ca...   \n",
       "79                                                                           \n",
       "81                                                                           \n",
       "83                                                                           \n",
       "\n",
       "                                              journal  \\\n",
       "77                                  Nature metabolism   \n",
       "78                                 Scientific reports   \n",
       "79                                              eLife   \n",
       "81                     Science translational medicine   \n",
       "83  Computational and structural biotechnology jou...   \n",
       "\n",
       "                             doi        pmid  ... authors2  \\\n",
       "77    10.1038/s42255-022-00684-9  36333489.0  ...            \n",
       "78    10.1038/s41598-022-23054-7  36335145.0  ...            \n",
       "79           10.7554/eLife.79558  36346735.0  ...            \n",
       "81  10.1126/scitranslmed.abm1463  36350984.0  ...            \n",
       "83    10.1016/j.csbj.2022.10.008  36382191.0  ...            \n",
       "\n",
       "                                    affiliations list  \\\n",
       "77  Department of Microbiology and Immunology, Uni...   \n",
       "78  Biophysics Graduate Group, University of Calif...   \n",
       "79  Biochemistry and Molecular Biophysics Option, ...   \n",
       "81  Department of Microbiology and Immunology, Uni...   \n",
       "83  Biophysics Program, Stanford University, Stanf...   \n",
       "\n",
       "                                author - affiliations published or not  \\\n",
       "77  Vaibhav Upadhyay: Department of Microbiology a...                    \n",
       "78  Xinyi Zhang: Department of Bioengineering, Uni...                    \n",
       "79  Pooja Suresh: Biophysics Graduate Program, Uni...                    \n",
       "81  Daniel B Goodman: Department of Microbiology a...                    \n",
       "83  Benjamin D Knapp: Biophysics Program, Stanford...                    \n",
       "\n",
       "   confirm published doi       confirm preprint doi possible match result  \\\n",
       "77                                                                          \n",
       "78                                                                          \n",
       "79                        10.1101/2022.04.08.487649                         \n",
       "81                                                                          \n",
       "83                                                                          \n",
       "\n",
       "   match id record change number                                    b_au  \n",
       "77                             0                        Turnbaugh, Peter  \n",
       "78                             0         Landry, Markita; Streets, Aaron  \n",
       "79                             0                          Dumont, Sophie  \n",
       "81                             0  Ye, Jimmie; Marson, Alex; Roybal, Kole  \n",
       "83                             0                               Huang, KC  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import importlib\n",
    "import all_function as af #import the module here, so that it can be reloaded.\n",
    "importlib.reload(af)\n",
    "\n",
    "biohub_staff_author=author[author['Campus (simple)'] == 'Biohub']['MatchName'].str.replace(',','').to_list()\n",
    "\n",
    "# 2. Choose publication & prerpint within specific time period \n",
    "df = pd.read_csv('database/basedb.csv', encoding='utf-8-sig')\n",
    "\n",
    "df['epost date'] = pd.to_datetime(df['epost date'])  \n",
    "condition = (df['epost date'] >= start) & (df['epost date'] <= end) \n",
    "df=df.loc[condition]\n",
    "df=df.fillna('').reset_index(drop=True)\n",
    "\n",
    "# format 'corresponding author' column and replace original\n",
    "for ind,i in enumerate(df['corresponding author']):\n",
    "    if i == '':\n",
    "        continue \n",
    "    i_list=i.split(';')\n",
    "    res=[]\n",
    "    for j in i_list:  \n",
    "        old_name,new_name,prob=af.authormatch(j)\n",
    "        res.append(new_name)\n",
    "    df.loc[ind,'corresponding author'] = '; '.join(m for m in res)\n",
    "df.fillna('', inplace=True)\n",
    "#print(df['corresponding author'])\n",
    "\n",
    "#df['b_au']=df[['biohub author','possible biohub author','format biohub author']].agg('; '.join, axis=1)#.str.replace('; ; ; ','; ').str.replace('; ; ','; ').str.strip('; ')\n",
    "#df['b_au']=df[['possible biohub author','possible biohub author','format biohub author']].agg('; '.join, axis=1)\n",
    "df['b_au']=df['format biohub author']\n",
    "\n",
    "print(df.shape)  \n",
    "#display(df.tail(5))  \n",
    "\n",
    "ind_list=[]\n",
    "biohub_staff_ind=[]\n",
    "for ind,i in enumerate(df['b_au']):\n",
    "    i='; '.join(set([j.strip() for j in i.split(';') if j]))\n",
    "    #print(ind,i)\n",
    "    df.loc[ind,'b_au']=i\n",
    "    for j in i.split(';'): \n",
    "        if j.strip(' ').replace(', ',' ') in fauthor_list:  # only return people who is in filter list\n",
    "            # Problem: if there are some author not in 'biohub author.xlsx', it will not show their publication\n",
    "            ind_list.append(ind)   \n",
    "        if j.strip(' ').replace(', ',' ') in biohub_staff_author:\n",
    "            biohub_staff_ind.append(ind)\n",
    "            \n",
    "df=df.iloc[list(set(ind_list)),:]\n",
    "\n",
    "print(df.shape) \n",
    "display(df.tail(5))  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind_list=[]\n",
    "biohub_staff_ind=[]\n",
    "for ind,i in enumerate(df['b_au']):\n",
    "    i='; '.join(set([j.strip() for j in i.split(';') if j]))\n",
    "    df.loc[ind,'b_au']=i\n",
    "    for j in i.split(';'): \n",
    "        if j.strip(' ').replace(', ',' ') in fauthor_list:  # only return people who is in filter list\n",
    "            # Problem: if there are some author not in 'biohub author.xlsx', it will not show their publication\n",
    "            ind_list.append(ind)\n",
    "            break\n",
    "             \n",
    "        if j.strip(' ').replace(', ',' ') in biohub_staff_author:\n",
    "            biohub_staff_ind.append(ind)\n",
    "            break\n",
    "print(len([*set(ind_list)]))\n",
    "len(ind_list)\n",
    "len(biohub_staff_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "st.write(df['corresponding author'])\n",
    "\n",
    "感觉possible biohub author','format biohub author 可以删掉。。。\n",
    "format biohub author  如果按名字搜，会出现一些他们发表论文时不属于biohub的情况，不该纳入考虑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement aspose-words (from versions: none)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for aspose-words\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install aspose-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from docx import Document\n",
    "\n",
    "doc = Document()\n",
    "\n",
    "title = doc.add_heading('this is title', 1)  \n",
    "doc.save('test.docx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 ='##### Papers (Research articles, methods papers, reviews, etc.) and conference proceedings:\\n'\n",
    "for ind,row in p1_pub.iterrows():\n",
    "    if row['corresponding author'] !='':\n",
    "        p1 +='- **'+row['b_au']+\" of \"+row['corresponding author']+\"’s lab at \"+', '.join(row['corresponding author institution'].split(',')[0:2])+'**\\n\\n  '+row['title']+' PMID: '+str(row['pmid'])+'\\n'\n",
    "    else:\n",
    "        p1 +='- **'+row['b_au']+'**\\n\\n  '+row['title']+' PMID: '+str(row['pmid'])+'\\n'\n",
    "\n",
    "# Record: Prerprint\n",
    "p1 += \"\\n\\n##### Preprints\\n\"\n",
    "for ind,row in p1_pre.iterrows():\n",
    "    if row['corresponding author'] !='':\n",
    "        p1 +='- **'+row['b_au']+\" of \"+row['corresponding author']+\"’s lab at \"+', '.join(row['corresponding author institution'].split(',')[0:2])+'**\\n\\n  '+row['title']+' doi: '+str(row['doi'])+'\\n'\n",
    "    else:\n",
    "        p1 +='- **'+row['b_au']+'**\\n\\n  '+row['title']+' doi: '+str(row['doi'])+'\\n'\n",
    "\n",
    "p1=p1.replace('; ; ','; ')\n",
    "\n",
    "st.markdown(\"### Biohub intramural research program – Papers published and preprints first-deposited \\n\\nIncludes papers, conference proceedings, and preprints published or first-deposited since the last Biohub All-Hands meeting that cite Biohub affiliation or funding and that include a Biohub employee as a co-author (we may easily have missed something, so please feel free to send Bill Burkholder any additions or corrections)\\n\")\n",
    "st.download_button(label='Download Report',data=p1,file_name='Report.md')\n",
    "st.markdown(p1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ab8f8ae80fafc6ac129e7fe646a0be18ed2869456ff4c93941a37a28edfebd3f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
